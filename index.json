[{"categories":["docker"],"content":"CGroups与Namespaces 了解下容器背后的两大核心技术：CGroups 和 Namespace。 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:0:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"CGroups 概述 CGroups 全称为 Linux Control Group，其作用是限制一组进程使用的资源（CPU、内存等）上限，CGroups 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 CGroups 的基本概念： Task: 在 cgroup 中，task 可以理解为一个进程，但这里的进程和一般意义上的操作系统进程不太一样，实际上是进程 ID 和线程 ID 列表。 CGroup: 即控制组，一个控制组就是一组按照某种标准划分的 Tasks，可以理解为资源限制是以进程组为单位实现的，一个进程加入到某个控制组后，就会受到相应配置的资源限制。 Hierarchy: cgroup 的层级组织关系，cgroup 以树形层级组织，每个 cgroup 子节点默认继承其父 cgroup 节点的配置属性，这样每个 Hierarchy 在初始化会有 root cgroup。 Subsystem: 即子系统，子系统表示具体的资源配置，如 CPU 使用，内存占用等，Subsystem 附加到 Hierarchy 上后可用 CGroups 支持的子系统包含以下几类，即为每种可以控制的资源定义了一个子系统: cpuset: 为 cgroup 中的进程分配单独的 CPU 节点，即可以绑定到特定的 CPU cpu: 限制 cgroup 中进程的 CPU 使用份额 cpuacct: 统计 cgroup 中进程的 CPU 使用情况 memory: 限制 cgroup 中进程的内存使用,并能报告内存使用情况 devices: 控制 cgroup 中进程能访问哪些文件设备(设备文件的创建、读写) freezer: 挂起或恢复 cgroup 中的 task net_cls: 可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块(traffic contro)对数据包进行控制 blkio: 限制 cgroup 中进程的块设备 IO perf_event: 监控 cgroup 中进程的 perf 时间，可用于性能调优 hugetlb: hugetlb 的资源控制功能 pids: 限制 cgroup 中可以创建的进程数 net_prio: 允许管理员动态的通过各种应用程序设置网络传输的优先级 通过上面的各个子系统，可以看出使用 CGroups 可以控制的资源有: CPU、内存、网络、IO、文件设备等。CGroups 具有以下几个特点： CGroups 的 API 以一个伪文件系统（/sys/fs/cgroup/）的实现方式，用户的程序可以通过文件系统实现 CGroups 的组件管理 CGroups 的组件管理操作单元可以细粒度到线程级别，用户可以创建和销毁 CGroups，从而实现资源载分配和再利用 所有资源管理的功能都以子系统（cpu、cpuset 这些）的方式实现，接口统一子任务创建之初与其父任务处于同一个 CGroups 的控制组 我们可以通过查看 /proc/cgroups 文件来查找当前系统支持的 CGroups 子系统: $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 5 153 1 cpu 8 478 1 cpuacct 8 478 1 memory 3 478 1 devices 9 478 1 freezer 4 153 1 net_cls 6 153 1 blkio 10 478 1 perf_event 2 153 1 hugetlb 7 153 1 pids 11 478 1 net_prio 6 153 在使用 CGroups 时需要先挂载，我们可以使用 df -h | grep cgroup 命令进行查看: $ df -h | grep cgroup tmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup 可以看到被挂载到了 /sys/fs/cgroup，cgroup 其实是一种文件系统类型，所有的操作都是通过文件来完成的，我们可以使用 mount –type cgroup命令查看当前系统挂载了哪些 cgroup： $ mount --type cgroup cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) /sys/fs/cgroup 目录下的每个子目录就对应着一个子系统，cgroup 是以目录形式组织的，/ 是 cgroup 的根目录，但是这个根目录可以被挂载到任意目录，例如 CGroups 的 memory 子系统的挂载点是 /sys/fs/cgroup/memory，那么 /sys/fs/cgroup/memory/ 对应 memory 子系统的根目录，我们可以列出该目录下面的文件： $ ll /sys/fs/cgroup/memory/ total 0 -rw-r--r-- 1 root root 0 Aug 8 2022 cgroup.clone_children --w--w--w- 1 root root 0 Aug 8 2022 cgroup.event_control -rw-r--r-- 1 root root 0 Aug 8 2022 cgroup.procs -r--r--r-- 1 root root 0 Aug 8 2022 cgroup.sane_behavior drwxr-xr-x 5 root root 0 Aug 28 14:38 docker drwxr-xr-x 5 root root 0 Sep 6 14:01 kubepods -rw-r--r-- 1 root root 0 Aug 8 2022 memory.failcnt --w------- 1 root root 0 Aug 8 2022 memory.force_empty -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.failcnt -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.limit_in_bytes -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.max_usage_in_bytes -r--r--r-- 1 root root 0 Aug 8 2022 memory.kmem.slabinfo -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.failcnt -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.limit_in_bytes -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.max_usage_in_bytes -r--r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.usage_in_bytes -r--r--r-- 1 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:1:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"CGroup 测试 接下来我们来尝试手动设置下 cgroup，以 CPU 这个子系统为例进行说明，首先我们在 /sys/fs/cgroup/cpu 目录下面创建一个名为 ydzs.test 的目录： ➜ ~ mkdir -p /sys/fs/cgroup/cpu/ydzs.test ➜ ~ ls /sys/fs/cgroup/cpu/ydzs.test/ cgroup.clone_children cpuacct.stat cpu.cfs_period_us cpu.rt_runtime_us notify_on_release cgroup.event_control cpuacct.usage cpu.cfs_quota_us cpu.shares tasks cgroup.procs cpuacct.usage_percpu cpu.rt_period_us cpu.stat 我们可以看到目录创建完成后，下面就会已经自动创建 cgroup 的相关文件，这里我们重点关注 cpu.cfs_period_us 和 cpu.cfs_quota_us 这两个文件，前面一个是用来配置 CPU 时间周期长度的，默认为 100000us，后者用来设置在此时间周期长度内所能使用的 CPU 时间数，默认值为-1，表示不受时间限制。 ➜ ~ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_period_us 100000 ➜ ~ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us -1 现在我们写一个简单的 Python 脚本来消耗 CPU： # cgroup.py while True: pass 直接执行这个死循环脚本即可： ➜ ~ python cgroup.py \u0026 [1] 2113 使用 top 命令可以看到进程号 2113 的 CPU 使用率达到了 100% 现在我们将这个进程 ID 写入到 /sys/fs/cgroup/cpu/ydzs.test/tasks 文件下面去，然后设置 /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us 为 10000us，因为 cpu.cfs_period_us 默认值为 100000us，所以这表示我们要限制 CPU 使用率为 10%： ➜ ~ echo 2113 \u003e /sys/fs/cgroup/cpu/ydzs.test/tasks ➜ ~ echo 10000 \u003e /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us 设置完过后上面我们的测试进程 CPU 就会被限制在 10% 左右了，再次使用 top 命令查看该进程可以验证。 如果要限制内存等其他资源的话，同样去对应的子系统下面设置资源，并将进程 ID 加入 tasks 中即可。如果要删除这个 cgroup，直接删除文件夹是不行的，需要使用 libcgroup 工具： ➜ ~ yum install libcgroup libcgroup-tools ➜ ~ cgdelete cpu:ydzs.test ➜ ~ ls /sys/fs/cgroup/cpu/ydzs.test ls: cannot access /sys/fs/cgroup/cpu/ydzs.test: No such file or directory ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:2:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"在容器中使用 CGroups 上面我们测试了一个普通应用如何配置 cgroup，接下来我们在 Containerd 的容器中来使用 cgroup，比如使用 nerdctl 启动一个 nginx 容器，并限制其使用内存为 50M: ➜ ~ nerdctl run -d -m 50m --name nginx nginx:alpine 8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c ➜ ~ nerdctl ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8690c7dba4ff docker.io/library/nginx:alpine \"/docker-entrypoint.…\" 53 seconds ago Up nginx 在使用 nerdctl run 启动容器的时候可以使用 -m 或 –memory 参数来限制内存，启动完成后该容器的 cgroup 会出现在 名为 default 的目录下面，比如查看内存子系统的目录： ➜ ~ ll /sys/fs/cgroup/memory/default/ total 0 drwxr-xr-x 2 root root 0 Oct 21 15:01 8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children --w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs ...... 上面我们启动的 nginx 容器 ID 的目录会出现在 /sys/fs/cgroup/memory/default/ 下面，该文件夹下面有很多和内存相关的 cgroup 配置文件，要进行相关的配置就需要在该目录下对应的文件中去操作： ➜ ~ ll /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c total 0 -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children --w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs -rw-r--r-- 1 root root 0 Oct 21 15:01 memory.failcnt --w------- 1 root root 0 Oct 21 15:01 memory.force_empty -rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.failcnt 我们这里需要关心的是 memory.limit_in_bytes 文件，该文件就是用来设置内存大小的，正常应该是 50M 的内存限制： ➜ ~ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/memory.limit_in_bytes 52428800 同样我们的 nginx 容器进程 ID 也会出现在上面的 tasks 文件中： ➜ ~ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/tasks 2686 2815 2816 2817 2818 我们可以通过如下命令过滤该进程号，可以看出第一行的 2686 就是 nginx 进程在主机上的进程 ID，下面几个是这个进程下的线程： ➜ ~ ps -ef | grep 2686 root 2686 2656 0 15:01 ? 00:00:00 nginx: master process nginx -g daemon off; 101 2815 2686 0 15:01 ? 00:00:00 nginx: worker process 101 2816 2686 0 15:01 ? 00:00:00 nginx: worker process 101 2817 2686 0 15:01 ? 00:00:00 nginx: worker process 101 2818 2686 0 15:01 ? 00:00:00 nginx: worker process root 2950 1976 0 15:36 pts/0 00:00:00 grep --color=auto 2686 我们删除这个容器后，/sys/fs/cgroup/memory/default/ 目录下的容器 ID 文件夹也会自动删除。 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:3:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"Namespaces namespace 也称命名空间，是 Linux 为我们提供的用于隔离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用个人 PC 时，我们并没有运行多个完全分离的服务器的需求，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这是我们不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。而我们这里的容器其实就通过 Linux 的 Namespaces 技术来实现的对不同的容器进行隔离。 linux 共有 6(7)种命名空间: ipc namespace: 管理对 IPC 资源（进程间通信（信号量、消息队列和共享内存）的访问 net namespace: 网络设备、网络栈、端口等隔离 mnt namespace: 文件系统挂载点隔离 pid namespace: 用于进程隔离 user namespace: 用户和用户组隔离（3.8 以后的内核才支持） uts namespace: 主机和域名隔离 cgroup namespace：用于 cgroup 根目录隔离（4.6 以后版本的内核才支持） 我们可以通过 lsns 命令查看当前系统已经创建的命名空间： # lsns|more NS TYPE NPROCS PID USER COMMAND 4026531836 pid 600 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531837 user 946 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531838 uts 629 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531839 ipc 600 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531840 mnt 591 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531856 mnt 1 88 root kdevtmpfs 4026531956 net 643 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026532450 mnt 1 6635 chrony /usr/sbin/chronyd 4026532451 mnt 1 6660 root /usr/sbin/NetworkManager --no-daemon 要查看一个进程所属的命名空间信息，可以到 /proc//ns 目录下查看： # ps -ef|grep chronyd chrony 6635 1 0 May11 ? 00:00:50 /usr/sbin/chronyd root 17096 25474 0 17:38 pts/1 00:00:00 grep --color=auto chronyd # ll /proc/6635/ns total 0 lrwxrwxrwx 1 root root 0 Sep 11 17:23 ipc -\u003e ipc:[4026531839] lrwxrwxrwx 1 root root 0 Sep 11 17:23 mnt -\u003e mnt:[4026532450] lrwxrwxrwx 1 root root 0 Sep 11 17:23 net -\u003e net:[4026531956] lrwxrwxrwx 1 root root 0 Sep 11 17:23 pid -\u003e pid:[4026531836] lrwxrwxrwx 1 root root 0 Sep 11 17:23 user -\u003e user:[4026531837] lrwxrwxrwx 1 root root 0 Sep 11 17:23 uts -\u003e uts:[4026531838] 这些 namespace 都是链接文件, 格式为 namespaceType:[inode number]，inode number 用来标识一个 namespace，可以理解为 namespace id，如果两个进程的某个命名空间的链接文件指向同一个，那么其相关资源在同一个命名空间中，也就没有隔离了。比如同样针对上面运行的 nginx 容器，我们查看其命名空间： # lsns |grep nginx 4026533147 mnt 17 25233 10000 nginx: master process nginx -g daemon off 4026533148 uts 17 25233 10000 nginx: master process nginx -g daemon off 4026533149 ipc 17 25233 10000 nginx: master process nginx -g daemon off 4026533150 pid 17 25233 10000 nginx: master process nginx -g daemon off 4026533152 net 17 25233 10000 nginx: master process nginx -g daemon off 4026533313 mnt 17 25604 10000 nginx: master process nginx -g daemon off 4026533314 uts 17 25604 10000 nginx: master process nginx -g daemon off 4026533315 ipc 17 25604 10000 nginx: master process nginx -g daemon off 4026533316 pid 17 25604 10000 nginx: master process nginx -g daemon off 4026533318 net 17 25604 10000 nginx: master process nginx -g daemon off 可以看出 nginx 容器启动后，已经为该容器自动创建了单独的 mtn、uts、ipc、pid、net 命名空间，也就是这个容器在这些方面是独立隔离的，其他容器想要和该容器共享某一个命名空间，那么就需要指向同一个命名空间。 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:4:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"参考 https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/ https://www.infoq.cn/article/docker-resource-management-cgroups/ ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:5:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"介绍 Dockerfile 是一个文本文件，其内包含了一条条的 指令(Instruction)，每一条指令 构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 一个简单Dockerfile文件 # VERSION 0.0.1 FROM ubuntu MAINTAINER James Turnbull \"james@example.com\" RUN echo \"deb http://archive.ubuntu.com/ubuntu precise main ↩ universe\" \u003e /etc/apt/sources.list RUN apt-get update RUN apt-get install -y openssh-server RUN mkdir /var/run/sshd RUN echo \"root:password\" | chpasswd EXPOSE 22 可以看的出来Dockerfile 包含一系列命令并附上参数 每个命令都是大写开头，后面是跟着参数 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:0:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"Dockerfile 命令 命令 解释 FROM 指定基础镜像 RUN Dockerfile中的每个指令都会创建一个新的镜像层 CMD 在容器中执行的命令 EXPOSE 暴露端口 ENV 设置环境变量 COPY 拷贝本地文件和目录至镜像 ADD 功能更丰富的添加拷贝指令，COPY优先于ADD ENTRYPOINT ENTRYPOINT指令并不是必须的，ENTRYPOINT是一个脚本 VOLUME 定义镜像中的某个目录为容器卷，会随机生成一个容器卷名 WORKDIR 指定工作目录 ARG 定义了可以通过docker build –build-arg命令传递并在Dockerfile中使用的变量。 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:1:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"Dockerfile 小建议 要使用 tag，但不要 latest。 Debian 挺好的，不要总 Ubuntu。 apt-get update 在前，rm -rf /var/lib/apt/lists/* 在后。 yum install，不忘 yum clean。 多 RUN 要合并，来减少层数。 无用的软件，不要乱安装。 COPY 放最后，缓存很开心。 善用 dockerignore，不浪费传输。 不忘 MAINTAINER，这都是我的。 容器只运行一个应用 小结：（FROM）选择合适的基础镜像(alpine版本最好)，(RUN)在安装更新时勿忘删除缓存和安装包， 多个命令聚合在一起减少构建时的层数，用不到的软件不要安装，(ADD和COPY)选COPY,（LABEL）添加镜像元数据， (MAINTAINER)让我知道是谁维护我，(ENV)设置默认的环境变量，（EXPOSE）说一下暴露的映射端口 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:2:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"示例 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:3:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"nginx FROM centos MAINTAINER xianchao RUN yum install wget -y RUN yum install nginx -y # COPY index.html /usr/share/nginx/html/ EXPOSE 80 ENTRYPOINT [\"/usr/sbin/nginx\",\"-g\",\"daemon off;\"] ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:3:1","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"tomcat FROM centos MAINTAINER xianchao RUN yum install wget -y ADD jdk-8u45-linux-x64.rpm /usr/local/ ADD apache-tomcat-8.0.26.tar.gz /usr/local/ RUN cd /usr/local \u0026\u0026 rpm -ivh jdk-8u45-linux-x64.rpm RUN mv /usr/local/apache-tomcat-8.0.26 /usr/local/tomcat8 ENTRYPOINT /usr/local/tomcat8/bin/startup.sh \u0026\u0026 tail -F /usr/local/tomcat8/logs/catalina.out EXPOSE 8080 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:3:2","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"平时较多使用的是docker项目将容器运行时迁移至了containerd，这里整理下相关的操作命令 ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:0:0","tags":["docker"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["docker"],"content":"常用指令对比 ctr是containerd自带的工具，有命名空间的概念 crictl是k8s社区的专用CLI工具，所有操作都在命名空间k8s.io 所以使用ctr操作时注意指定-n k8s.io ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:1:0","tags":["docker"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["docker"],"content":"清理未被使用的镜像，一般用于释放本地空间 docker 场景 docker image prune -a containerd场景（需要高版本crictl） crictl rmi --prune ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:2:0","tags":["docker"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["docker"],"content":"本地文件和容器内文件间拷贝 docker 场景 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH 使用kubectl cp Copy files and directories to and from containers. Examples: # !!!Important Note!!! # Requires that the 'tar' binary is present in your container # image. If 'tar' is not present, 'kubectl cp' will fail. # Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace kubectl cp /tmp/foo_dir \u003csome-pod\u003e:/tmp/bar_dir # Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container kubectl cp /tmp/foo \u003csome-pod\u003e:/tmp/bar -c \u003cspecific-container\u003e # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace \u003csome-namespace\u003e kubectl cp /tmp/foo \u003csome-namespace\u003e/\u003csome-pod\u003e:/tmp/bar # Copy /tmp/foo from a remote pod to /tmp/bar locally kubectl cp \u003csome-namespace\u003e/\u003csome-pod\u003e:/tmp/foo /tmp/bar Options: -c, --container='': Container name. If omitted, the first container in the pod will be chosen --no-preserve=false: The copied file/directory's ownership and permissions will not be preserved in the container Usage: kubectl cp \u003cfile-spec-src\u003e \u003cfile-spec-dest\u003e [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:3:0","tags":["docker"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["Kubernetes"],"content":"k8s namespace 无法删除记录 名字空间yaml 文件 kind: Namespace apiVersion: v1 metadata: name: kube-logging 删除卡住 $ sudo kubectl delete -f kube-logging.yaml --wait=true namespace \"kube-logging\" deleted # 长久的等待 强制删除（无效果） sudo kubectl delete ns kube-logging --grace-period=0 --force 排查思路 查看命名空间下的所有资源 kubectl api-resources -o name --verbs=list --namespaced | xargs -n 1 kubectl get --show-kind --ignore-not-found -n kube-logging 输出结果该命名空间下无关联的相关资源 ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:0","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"解决方式 一行命令解决，注意替换两处待删命名空间字样 kubectl get namespace \"待删命名空间\" -o json \\ | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" \\ | kubectl replace --raw /api/v1/namespaces/待删命名空间/finalize -f - ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:1","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"总结 创建namesapce时注入finalizers会导致namespace无法 可以通过edit ns 的方式将finalizers 置空或者上面的一行命令解决的方式，本质是一致的 ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:2","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"补充 K8s Finalizers Finalizers 字段属于 Kubernetes GC 垃圾收集器，是一种删除拦截机制，能够让控制器实现异步的删除前（Pre-delete）回调。其存在于任何一个资源对象的 Meta[1] 中，在 k8s 源码中声明为 []string，该 Slice 的内容为需要执行的拦截器名称。 对带有 Finalizer 的对象的第一个删除请求会为其 metadata.deletionTimestamp 设置一个值，但不会真的删除对象。一旦此值被设置，finalizers 列表中的值就只能被移除。 当 metadata.deletionTimestamp 字段被设置时，负责监测该对象的各个控制器会通过轮询对该对象的更新请求来执行它们所要处理的所有 Finalizer。当所有 Finalizer 都被执行过，资源被删除。 metadata.deletionGracePeriodSeconds 的取值控制对更新的轮询周期 当 finalizers 字段为空时，k8s 认为删除已完成。 ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:3","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"参考 k8s 如何让 ns 无法删除 kubernetes_Namespace无法删除的解决方法 K8S从懵圈到熟练 - 我们为什么会删除不了集群的命名空间？ k8s问题解决 - 删除命名空间长时间处于terminating状态 熟悉又陌生的 k8s 字段：finalizers ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:1:0","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"故障描述 PVC 显示创建不成功：kubectl get pvc -n efk 显示 Pending，这是由于版本太高导致的。k8sv1.20 以上版本默认禁止使用 selfLink。(selfLink：通过 API 访问资源自身的 URL，例如一个 Pod 的 link 可能是 /api/v1/namespaces/ns36aa8455/pods/sc-cluster-test-1-6bc58d44d6-r8hld)。 ","date":"2023-05-26","objectID":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes v1.20创建PVC报错","uri":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/"},{"categories":["Kubernetes"],"content":"故障解决 $ vi /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 ··· - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --feature-gates=RemoveSelfLink=false # 添加这个配置 重启下kube-apiserver.yaml # 如果是二进制安装的 k8s，执行 systemctl restart kube-apiserver # 如果是 kubeadm 安装的 k8s $ ps aux|grep kube-apiserver $ kill -9 [Pid] $ kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml ... $ kubectl get pvc # 查看 pvc 显示 Bound NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-pvc Bound pvc-ae9f6d4b-fc4c-4e19-8854-7bfa259a3a04 1Gi RWX example-nfs 13m ","date":"2023-05-26","objectID":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes v1.20创建PVC报错","uri":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/"},{"categories":["Kubernetes"],"content":"背景 k8s 容器运行时是docker和containerd结合使用，且生产环境是离线环境将需要的containerd和docker版本相关的rpm包通过yum 进行升级，升级过程涉及到docker的重启，docker容器将会导致所有的容器进行重启，故升级前需要将k8s该节点先设置成维护状态，待升级结束再解除，本文对离线运行时升级进行一个记录，至于为什么需要将Docker和containerd进行结合使用参考下面补充介绍 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:1:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级前提和影响 docker 版本低于 19.03.xx containerd 版本低于 1.3.9 升级后的版本 $ sudo docker version Client: Docker Engine - Community Version: 19.03.11 API version: 1.40 Go version: go1.13.10 Git commit: 42e35e61f3 Built: Mon Jun 1 09:13:48 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.11 API version: 1.40 (minimum version 1.12) Go version: go1.13.10 Git commit: 42e35e61f3 Built: Mon Jun 1 09:12:26 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.9 GitCommit: ea765aba0d05254012b0b9e595e995c09186427f runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 $ containerd --version containerd containerd.io 1.3.9 ea765aba0d05254012b0b9e595e995c09186427f ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:2:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级操作 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"获取升级前集群所有节点状态 $ kubectl get nodes \u003e before-update-node-status.txt ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:1","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级过程 升级原则，升级 1 个，检查 OK 后，再升级另外 1 个节点 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:2","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级前操作 将准备的升级介质上传至yum源 # rpm 包指定文件夹 /repos/zcm-custom/centos/7/x86_64/ mv docker-ce-19.03.11.tar.gz /repos/zcm-custom/centos/7/x86_64/ cd /repos/zcm-custom/centos/7/x86_64/ tar zxvf docker-ce-19.03.11.tar.gz cd /repos/zcm-custom/centos/7/ createrepo --update x86_64/ # 更新yum源仓库 yum makecache # #重新生成缓存 yum list docker-ce 检查 $ sudo yum list docker-ce Loaded plugins: product-id, search-disabled-repos Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast Installed Packages docker-ce.x86_64 3:19.03.11-3.el7 @zcm-custom 能看到 19.03.11 说明 yum 源更新成功 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:3","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级步骤 将被升级节点置为非调度状态；对被升级节点执行驱逐操作，将该节点上的业务容器飘到其它节点 $ /usr/local/bin/kubectl cordon 10.45.80.44 $ /usr/local/bin/kubectl drain 10.45.80.44 --ignore-daemonsets 停止 kubelet 服务、停止 kube-proxy 服务 $ sudo systemctl stop kube-proxy $ sudo systemctl stop kubelet 停止 docker/containerd 服务 $ sudo systemctl stop docker $ sudo systemctl stop containerd $ sudo ps aux | grep docker # 检查该机器上所有与 docker 相关的进程 # 如果存在，则 kill 掉 $ sudo ps -ef|grep docker|grep -v dockerd|awk '{print $2}'|xargs kill -9 containerd 升级 $ yum install -y containerd 启动升级后的 docker/containerd 服务 $ sudo systemctl start containerd $ sudo systemctl start docker $ sudo systemctl enable docker $ sudo systemctl enable containerd 启动 kubelet 服务、启动 kube-proxy 服务 $ sudo systemctl start kube-proxy $ sudo systemctl start kubelet 登录 master 节点将升级节点状态设置为允许调度状态 $ sudo kubectl get nodes $ sudo kubectl uncordon 10.45.80.44 被升级节点的状态检查 检查被升级节点的状态 在 k8s master 节点上，执行 kubectl get nodes |grep 被升级节点的 IP 看看是否处于 Ready 状态。 检查被升级节点的容器网络，在升级完后的节点上 ping 一下其它机器上的容器 IP，ping 通的话，容器网络正常 该节点升级完成 依次升级其它节点，按前面的升级步骤位次升级后续的节点，在升级后续节点的过程中，业务容器会在前面升级完成后的节点上调度 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:4","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级后的节点状态检查 $ kubectl get nodes ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:4:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"补充-为什么要将Docker和containerd进行结合使用？ 更轻量级：containerd是一个更轻量级且专注于容器操作的运行时工具，相比于Docker具有更小的内存占用和更快的启动时间。 更稳定和可靠：containerd是由Docker开源社区开发和维护的，因此可以受益于Docker社区的经验和反馈，从而获得更稳定和可靠的容器技术。 更灵活的集成：containerd被设计为模块化的容器运行时，可以与其他容器生态系统中的工具和服务集成，例如Kubernetes、CRI-O等。 更高性能：由于containerd比Docker更轻量级，因此可以获得更高的性能。它可以快速创建和销毁容器，并提供更低的延迟和更高的吞吐量。 兼容性：通过使用Docker作为容器镜像格式，containerd能够与Docker生态系统中的工具和服务无缝集成，而无需进行任何修改。 综上所述，使用Docker和containerd结合可以提供更轻量级、更稳定可靠、更灵活集成、更高性能和更好的兼容性。这使得它成为一个强大的容器运行时组合，适用于各种不同的容器化场景。 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:5:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"参考 Docker 与 Containerd 并用配置 手把手教你搭建Linux离线YUM源环境 ","date":"2023-04-26","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:6:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"现象 k8s 某台主机的/var/lib/kubelet目录存储使用率超过80% 使用了40多G存储 ","date":"2023-04-26","objectID":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:1:0","tags":["Kubernetes"],"title":"kubelet目录满问题记录","uri":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes"],"content":"分析 申请root权限 cd /var/lib/kubelet 执行du -h -d 1 发现 pod目录占据大量存储 cd pod 继续执行du -h -d 1 定位到pod 2753dcf9-4113-4e65-a944-278bcbd8d6d8 按上面方法继续执行 发现最终路径为 /var/lib/kubelet/pods/2753dcf9-4113-4e65-a944-278bcbd8d6d8/volumes/kubernetes.io~empty-dir/storage-volume docker ps|grep 2753dcf9-4113-4e65-a944-278bcbd8d6d8 定位容器为prometheus docker inspect 122ec2fbbd1e 该容器使用了empty-dir挂载内部的/data ","date":"2023-04-26","objectID":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:2:0","tags":["Kubernetes"],"title":"kubelet目录满问题记录","uri":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes"],"content":"问题描述 在高可用的k8s集群中，当Node节点挂掉，kubelet无法提供工作的时候，pod将会自动调度到其他的节点上去，而调度到节点上的时间需要我们慎重考量，因为它决定了生产的稳定性、可靠性，更快的迁移可以减少我们业务的影响性，但是有可能会对集群造成一定的压力，从而造成集群崩溃。 ","date":"2023-04-26","objectID":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/:1:0","tags":["Kubernetes"],"title":"调整节点pod驱逐时间","uri":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/"},{"categories":["Kubernetes"],"content":"Kubelet 状态更新的基本流程 1.kubelet 自身会定期更新状态到 apiserver，通过参数–node-status-update-frequency指定上报频率，默认是 10s 上报一次。 2.kube-controller-manager 会每隔–node-monitor-period时间去检查 kubelet 的状态，默认是 5s。 3.当 node 失联一段时间后，kubernetes 判定 node 为 notready 状态，这段时长通过–node-monitor-grace-period参数配置，默认 40s。 4.当 node 失联一段时间后，kubernetes 判定 node 为 unhealthy 状态，这段时长通过–node-startup-grace-period参数配置，默认 1m0s。 5.当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过–pod-eviction-timeout参数配置，默认 5m0s。 kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果–node-status-update-frequency设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。 如果业务生产认为默认的驱逐时间不能满足业务的及时性，可以手动修改驱逐时间。 在master节点操作： 修改/etc/systemd/system/kube-controller-manager.service文件 增加一行–pod-eviction-timeout=1m （这里是调整为1分钟，具体数值可以根据业务重要实际情况修改） ","date":"2023-04-26","objectID":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/:2:0","tags":["Kubernetes"],"title":"调整节点pod驱逐时间","uri":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/"},{"categories":["Kubernetes"],"content":"kube-dns切换为coredns k8s版本及其dns适配版本 k8s版本 coredns版本 镜像 镜像yaml 1.17 1.6.7 coredns-1.6.7.tar.gz coredns-1.6.7.yaml 1.22 1.8.4 coredns-1.8.4.tar.gz coredns-1.8.4.yaml coredns/coredns:1.6.7/1.8.4 ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:1:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"操作 根据版本选择对应的dns镜像以及yaml文件上传到其中一台master主机，以1.8.4 版本为例 如果是内网则将coredns-1.6.7/1.8.4.tar.gz 压缩包进行解压并上传至现场的harbor仓库，如果可以连接外网则直接修改 coredns-1.8.4.yaml 文件，将站位符{{ image_address }} 进行修改 ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:2:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"将原有的 kube-dns 停止 $ kubectl scale deploy kube-dns --replicas=0 -n kube-system $ kubectl scale deploy kube-dns-autoscaler --replicas=0 -n kube-system ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:3:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"部署coredns $ kubectl apply -f coredns.yaml ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:4:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"补充: kube-dns vs CoreDNS CoreDNS和kube-dns都是用于Kubernetes集群中的DNS解析服务，它们在不同方面有一些区别，适用于不同的场景。 另外还有一个方面是早期版本的 kube-dns 在 Kubernetes 中主要支持 IPv4。IPv6 的支持不如 CoreDNS 完善，并且在实际应用中可能会遇到限制。 CoreDNS： CoreDNS是一个开源的、轻量级的、灵活的DNS服务器，它是一个通用的DNS服务器，可以用于各种不同的环境和场景，不仅限于Kubernetes集群。 CoreDNS是一个插件驱动的DNS服务器，可以通过插件的方式支持各种功能，例如反向代理、负载均衡、缓存、DNSSEC等。 CoreDNS在Kubernetes集群中可以作为集群的默认DNS插件，用于提供内部服务发现和外部域名解析。它可以动态地解析Kubernetes集群内部的服务和Pod，并提供DNS记录。 kube-dns： kube-dns是Kubernetes集群中的默认DNS解析服务，是一种基于SkyDNS的插件，与Kubernetes紧密集成。 kube-dns使用了一个集群内部的DNS服务器和一个外部的DNS服务器配合工作。集群内部的DNS服务器负责解析集群内的服务和Pod，并提供DNS记录。外部的DNS服务器负责解析集群外的域名，并提供DNS记录。 kube-dns的优点是它是Kubernetes的默认解析器，易于配置和使用，可以满足大多数基本的服务发现和域名解析需求。 综上所述，CoreDNS适用于更通用的环境和场景，可以满足更多的功能需求，而kube-dns是Kubernetes集群中的默认解析器，适用于基本的服务发现和域名解析需求。具体使用哪个取决于特定的需求和场景。 ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:5:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"参考 【kubernetes】部署-CoreDNS-服务 ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:6:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"coredns-1.6.7.yaml apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } template ANY AAAA { rcode NXDOMAIN } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\" spec: # replicas: not specified here: # 1. Default is 1. # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" - key: node.kubernetes.io/unschedulable effect: NoSchedule nodeSelector: kubernetes.io/role: master affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: [\"kube-dns\"] topologyKey: kubernetes.io/hostname containers: - name: coredns image: {{ image_address }}/coredns/coredns:1.6.7 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP ","date":"2023-03-26","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:7:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":" 当Kubernetes创建Pod时发生了什么-全景图 https://github.com/jamiehannaford/what-happens-when-k8s/tree/master/zh-cn ","date":"2023-03-26","objectID":"/%E5%BD%93-kubernetes-%E5%88%9B%E5%BB%BA-pod-%E6%97%B6%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:0:0","tags":["Kubernetes"],"title":"当 Kubernetes 创建 Pod 时发生了什么","uri":"/%E5%BD%93-kubernetes-%E5%88%9B%E5%BB%BA-pod-%E6%97%B6%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["Kubernetes"],"content":"记录一下kuberntes 遇到的问题场景及排查方向 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:0:0","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"DNS 解析异常 5 秒延时 如果DNS查询经常延时5秒才返回，通常是遇到内核 conntrack 冲突导致的丢包 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"解析超时 如果容器内报 DNS 解析超时，先检查下集群 DNS 服务 (kube-dns/coredns) 的 Pod 是否 Ready，如果不是，查看日志信息。如果运行正常，再具体看下超时现象。 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:1:1","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"Service 无法解析 集群 DNS 没有正常运行(kube-dns或CoreDNS) 检查集群 DNS 是否运行正常: kubelet 启动参数 –cluster-dns 可以看到 dns 服务的 cluster ip: $ ps -ef | grep kubelet ... /usr/bin/kubelet --cluster-dns=172.16.14.217 ... 或者放置配置文件中 $ cat /var/lib/kubelet/config.yaml|grep clusterDNS -A 2 clusterDNS: - 172.16.0.10 clusterDomain: cluster.local 找到 dns 的 service: $ kubectl get svc -n kube-system | grep 172.16.14.217 kube-dns ClusterIP 172.16.14.217 \u003cnone\u003e 53/TCP,53/UDP 47d 看是否存在 endpoint: $ kubectl -n kube-system describe svc kube-dns | grep -i endpoints Endpoints: 172.16.0.156:53,172.16.0.167:53 Endpoints: 172.16.0.156:53,172.16.0.167:53 检查 endpoint 的 对应 pod 是否正常: $ kubectl -n kube-system get pod -o wide | grep 172.16.0.156 kube-dns-898dbbfc6-hvwlr 3/3 Running 0 8d 172.16.0.156 10.0.0.3 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"Pod 与 DNS 服务之间网络不通 检查下 pod 是否连不上 dns 服务，可以在 pod 里 telnet 一下 dns 的 53 端口: # 连 dns service 的 cluster ip $ telnet 172.16.14.217 53 如果检查到是网络不通，就需要排查下网络设置: 检查节点的安全组设置，需要放开集群的容器网段 检查是否还有防火墙规则，检查 iptables ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:2:1","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["linux"],"content":"监控系统资源和进程工具 top 命令 快捷键及使用场景 1：显示每个 CPU 核心的详细信息。当你需要查看系统中每个 CPU 核心的负载和使用情况时，可以按下 1 键。 E：切换累计模式（Cumulative Mode）。在累计模式下，CPU 使用率和内存占用等指标会显示所有进程的累计值而不是单个进程的值。 M：按内存使用排序。按下 M 键后，top 命令会按照内存使用量的大小对进程进行排序，并显示最高的内存使用进程。 P：按 CPU 使用排序。按下 P 键后，top 命令会按照 CPU 使用率的大小对进程进行排序，并显示最高的 CPU 使用进程。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:0:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"内存 内存相关名词释义 1）Working Set Size(WSS)是指一个app保持正常运行所须的内存。比如一个应用在初始阶段申请了100G主存，在实际正常运行时每秒只需要50M，那么这里的50M就是一个WSS 2）RES：resident memory usage。常驻内存 3）RSS Resident Set Size 实际使用物理内存，（包含共享库占用的内存） 4）CACHE，是一种特殊的内存，因为主内存速度不够快，用少量的特别快的但特别昂贵的内存来做缓存加速,就是cache。简单来说，buffer是即将要被写入磁盘的，而cache是被从磁盘中读出来的。 5）SWAP 交换区间，内存不够用使用磁盘充当内存 查看占用内存较高的10个进程 ps -aux | sort -k4nr | head 查看进程占用的内存大小 # 通过端口获取进程号 netstat -anp|grep 9100 tcp6 0 0 :::9100 :::* LISTEN 5505/node_exporter # 通过进程id获取进程占用内存大小 cat /proc/5505/status|grep VmRSS VmRSS: 31500 kB 注意：VmRSS对应的值就是物理内存占用 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:1:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"IO 查看磁盘io及传递状态 iostat -x 1 3 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:2:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"如何判断磁盘 io 性能达到瓶颈？ iostat 是一个用于监控系统磁盘IO使用情况的工具，可以显示每个磁盘的IO操作、IO延迟和数据吞吐量等信息。当磁盘IO达到瓶颈时，可以通过 iostat 来判断。 有以下几个指标可以帮助你判断磁盘IO是否达到瓶颈： %util：这是磁盘IO利用率的百分比，表示磁盘正在处理IO请求所占用的时间比例。当 %util 高于 80% 或 90% 时，磁盘IO可能已经达到瓶颈状态。 await：这是平均每个IO请求在队列中等待被处理的时间，表示系统平均IO响应时间。当 await 较高(通常大于 10ms)时，说明磁盘IO负载过重，也可能意味着磁盘IO已达到瓶颈。 svctm：这是平均每个IO请求的服务时间，表示磁盘处理IO请求的平均时间。当 svctm 较高时，系统处理IO请求的速度较慢，可能代表磁盘IO已经达到瓶颈状态。 总之，当磁盘IO利用率高、IO等待时间长或服务时间长时，磁盘IO可能已达到瓶颈状态。通常还需要综合考虑诸如系统性能需求、磁盘类型和配置等因素，才能确定是否对系统进行优化或升级。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:2:1","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看磁盘io占用较高的几个进程 可以使用 iotop 命令来查看磁盘IO占用较高的几个进程 $ iotop -oP Total DISK READ : 577.97 K/s | Total DISK WRITE : 115.06 K/s Actual DISK READ: 64.22 K/s | Actual DISK WRITE: 1934.59 K/s PID PRIO USER DISK READ DISK WRITE SWAPIN IO\u003e COMMAND 574 be/4 root 577.97 K/s 0.00 B/s 0.00 % 8.68 % [xfsaild/dm-0] 73282 be/4 root 0.00 B/s 45.49 K/s 0.00 % 0.47 % influxd 53910 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.02 % [kworker/7:0] 55873 be/4 systemd- 0.00 B/s 2.68 K/s 0.00 % 0.02 % mysqld ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:2:2","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"监控进程使用情况 pidstat 是一个用于监控进程资源使用情况的工具，可以提供有关 CPU、内存、磁盘IO、网络和上下文切换等方面的统计信息。以下是 pidstat 常用命令及其使用场景： 1.查看特定进程的资源使用情况： pidstat -p 该命令可用于监控指定 PID 的进程的资源使用情况，包括 CPU 占用率、内存使用量、上下文切换次数、磁盘IO等。 2.实时监控整个系统中进程的资源使用情况： pidstat -r 该命令可实时显示所有进程的内存使用情况，包括虚拟内存大小、物理内存大小、共享内存大小等。 3.监控进程的CPU使用情况： pidstat -u 通过设置 和 参数，该命令可定期显示进程的 CPU 使用情况，包括用户空间和内核空间的 CPU 使用时间、CPU 使用百分比等。 pidstat 命令常用于性能分析和资源监控，可以帮助定位和解决系统性能问题。具体使用场景包括但不限于： 查找 CPU 密集型进程、监测内存泄漏、检测磁盘IO瓶颈、分析网络传输情况等。 最佳实践 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:3:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"手动清理缓存buffer/cache 1.清理pagecache(页面缓存) echo 1 \u003e /proc/sys/vm/drop_caches #或者 sysctl -w vm.drop_caches=1 2.清理目录缓存 echo 2 \u003e /proc/sys/vm/drop_caches #或者 sysctl -w vm.drop_caches=2 3.清理pagecache、dentries和inodes echo 3 \u003e /proc/sys/vm/drop_caches #或者 sysctl -w vm.drop_caches=3 上面三种方式都是临时释放缓存的方法，想要永久释放缓存，需要在/etc/sysctl.conf文件中配置： vm.drop_caches=1/2/3,然后sysctl -p生效即可 温馨提示： 上面操作在大多数情况下都不会对系统造成伤害，只会有助于释放不用的内存 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:4:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看僵尸进程并kill掉 查看 ps -ef | grep defunct kill kill -9 父进程号 在linux系统中，进程有如下几种状态，它们随时可能处于以上状态中的一种： D = 不可中断的休眠 I = 空闲 R = 运行中 S = 休眠 T = 被调度信号终止 t = 被调试器终止 Z = 僵尸状态 我们可以在命令终端中通过top命令来查看系统进程和它的当前状态。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:4:1","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看进程占用的的物理内存大小指令 cat /proc/`ps -ef|grep grafana|grep -v grep|awk '{print $2}'`/status |grep VmRSS 指令分解 # 通过关键词xxx确认进程id，进程关键词如grafana $ ps -ef|grep grafana|grep -v grep|awk '{print $2}' 118581 # 通过进程标识查看服务器物理内存占用 $ cat /proc/118581/status |grep VmRSS VmRSS: 100788 kB # 动态查看进程资源占用 top -p 118581 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:5:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看TCP连接数 netstat -n | awk '/^tcp/ {++state[$NF]} END {for(key in state) print key,\"\\t\",state[key]}' 总结 Linux 服务器性能资源排查思路 在排查 Linux 服务器性能问题时，可以按照以下思路进行操作： 1.监测系统资源使用情况： 使用命令 top 或 htop 实时监视 CPU、内存和交换空间的使用情况。 使用命令 df 查看磁盘空间使用情况。 使用命令 free 查看内存使用情况。 使用命令 iostat 查看磁盘IO使用情况。 使用命令 sar 查看系统整体的资源利用率。 2.检查进程和服务： 使用命令 ps 或 top 查看正在运行的进程和它们的资源占用情况。 使用命令 systemctl status 检查服务的状态。 3.定位高负载进程： 使用命令 top 或 htop 查看 CPU 使用率最高的进程，并观察其 PID 和资源消耗情况。 使用命令 pidstat 监测指定 PID 的进程的资源使用情况。 4.分析日志信息： 查看系统日志（如 /var/log/messages）以了解系统运行期间发生的错误或异常情况。 查看应用程序日志，特别是与性能相关的日志，以寻找可能的瓶颈或错误。 5.检查网络连接和流量： 使用命令 netstat 或 ss 查看网络连接状态。 使用命令 iftop 或 nethogs 实时监测网络流量和使用情况。 6.进行性能调优： 调整系统内核参数，如文件句柄数、内存分配等，以提高性能。 优化关键应用程序的配置，如数据库、Web 服务器等。 7.使用性能分析工具： 使用工具如 perf、strace、tcpdump 等进行更深入的性能分析和故障排查。 这些步骤可以帮助你定位服务器的性能问题，并找出潜在的瓶颈或异常。根据具体的问题，你可能需要进一步深入研究和分析，以找到解决方案。同时，及时备份数据是必要的，以免在调优过程中发生数据丢失或其他意外。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:6:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["Kubernetes"],"content":"背景 项目现场通过 kubeadm 部署一套 k8s 临时环境 版本是v1.22, 因为是临时环境，部署方式是一主两从的模式， 部署后十天客户直接将主节点的内存进行了扩容，导致现场的应用服务无法访问。 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"现象 打开访问应用门户，提示 nginx 网关访问报错 503 Service Temporarily Unavailable ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"定位 通过 kubectl 查看 pod 状态发现很多应用是 CrashLoopBackOff,猜测是系统基础组件出现异常,查看应用日志提示 jdbc 连接 MySQL 异常，查看MySQL 日志发现端口52306被占用。 排查为 apiserver和etcd连接的随机端口占用了52306 # sudo netstat -nap|grep 52306 tcp 0 0 127.0.0.1:52306 127.0.0.1:2379 ESTABLISHED 30166/kube-apiserve tcp 0 0 127.0.0.1:2379 127.0.0.1:52306 ESTABLISHED 23788/etcd ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"问题处理 确认端口占用情况：执行 sudo netstat -lnp | grep 命令查看指定端口是否被占用 释放端口：如果该端口已被占用，可以通过 sudo lsof -i: 命令查找占用端口的进程 临时处理，pod 重建 kill 掉 etcd 的静态 pod，端口可能还是会重新被占用 永久处理，主机 k8s.conf 预留端口 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:4:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"保留预留端口 $ cat /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 kernel.core_pattern=/tmp/zcore/core.%h~%e fs.inotify.max_user_watches = 1048576 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_local_reserved_ports = 52000-52999 net.ipv4.ip_local_reserved_ports新增预留端口配置，保留端口范围不被占用，告诉内核保留端口范围从 52000 到 52999，以便这些端口不会被普通应用程序占用 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:4:1","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"/etc/sysctl.d/k8s.conf 配置项说明 在 Kubernetes 1.22 版本的 /etc/sysctl.d/k8s.conf 配置文件中，可能包含以下一些配置项： 1、 net.bridge.bridge-nf-call-ip6tables： 含义：控制是否将 IPv6 数据包传递给 iptables 的 netfilter 框架进行处理。如果设置为 1，则表示启用；如果设置为 0，则表示禁用。 默认值：1 2、 net.bridge.bridge-nf-call-iptables： 含义：控制是否将数据包传递给 iptables 的 netfilter 框架进行处理。如果设置为 1，则表示启用；如果设置为 0，则表示禁用。 默认值：1 3、 net.ipv4.ip_forward： 含义：控制 Linux 内核是否允许 IP 数据包转发。如果设置为 1，则表示启用 IP 转发；如果设置为 0，则表示禁用 IP 转发。 默认值：0 4、 net.ipv4.conf.all.forwarding： 含义：控制所有网络接口的 IP 转发功能。如果设置为 1，则表示启用 IP 转发；如果设置为 0，则表示禁用 IP 转发。 默认值：0 5、 net.ipv4.conf.default.forwarding： 含义：控制默认网络接口的 IP 转发功能。如果设置为 1，则表示启用 IP 转发；如果设置为 0，则表示禁用 IP 转发。 默认值：0 以上是常见的示例配置项和默认值，但实际的配置项和默认值可能会根据操作系统和 Kubernetes 版本而有所不同。在实际使用中，请根据文档和操作系统的要求进行正确配置。 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:5:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"问题现象 生产环境无法访问，这里主要是梳理遇到问题应该有的一个排查思路 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次项目生产环境故障分析","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes"],"content":"问题排查 登录节点查看namespace 下各pod状态 kubectl get pod -o wide -n prod 发现portal cmdb application等均处于异常状态 由于portal启动会依赖cmdb 优先查看cmdb的日志，发现报错连接redis异常 查看redis状态 处于正常状态 kubectl get pod -o wide -n prod|grep redis 进入cmdb 容器进行redis 连接测试 telnet redis 6379 发现解析域名 redis 有问题 因此怀疑coredns存在问题 查看 coredns 状态 kubectl get pod -o wide -n kube-system|grep coredns 发现pod处于terminating以及pending 由于coredns配置有节点选择器，只会调度到k8s master节点 此外对master做了taint ,—-防止其它的各种系统组件向Master调度，导致master资源受压缩。（此污点对已经调度在该节点的pod不会产生驱逐，但是新建pod的将无法调度） $ kubectl describe node 10.10.xxx Name: 10.10.xxx Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=10.10.xxx kubernetes.io/os=linux kubernetes.io/role=master zcm.role=k8s Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 25 Mar 2023 10:08:15 +0800 Taints: scheduler=custom:NoSchedule Unschedulable: false 导致coredns pending 临时处理,将节点选择器移除 coredns调度成功，启动完成 portal cmdb等依赖redis的服务自行恢复 生产环境恢复访问 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次项目生产环境故障分析","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes"],"content":"问题分析 问题发生前，集成对10.10.xxx等k8s master机器进行了迁移操作，主机发生了重启。 因此coredns发生重新调度，此时由于节点选择器以及taint的缘故，coredns无法成功调度启动， 进而影响了容器内对redis的解析 ，导致依赖redis的容器不断重启，生产环境无法访问。 后续对各个k8s集群的coredns配置了对污点的容忍，避免类似问题的再次发生。 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次项目生产环境故障分析","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes"],"content":"背景 statefulset 通过nfs 动态创建pv，发现无法创建成功 ","date":"2022-05-18","objectID":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 通过 nfs 动态创建 pv 异常","uri":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/"},{"categories":["Kubernetes"],"content":"报错日志 Warning FailedMount 2m50s (x6 over 6m4s) kubelet Unable to attach or mount volumes: unmounted volumes=[data], unattached volumes=[kube-api-access-8p8wf data]: error processing PVC kube-logging/data-es-cluster-0: PVC is not bound 通过报错日志可以看的出来pvc 无法绑定pv,通过命令查看pvc创建成功，pv 则没有进行创建 StatefulSet 挂载 NFS 存储卷时出现了问题,导致 PVC 没有被成功绑定到 PV的原因 常见原因有: PVC 和 PV 不匹配 PVC 的 storage class、访问模式、大小资源请求等需要和 PV 定义一致,否则不会被匹配到合适的 PV。 NFS 服务器配置问题 NFS 服务器需要正常启动、导出共享目录,并且 Kubernetes 节点能够访问到 NFS 服务器。 RBAC 鉴权问题 Kubernetes 节点上的 kubelet 需要有获取、挂载 PV 的权限。 NFS Client 配置问题 Kubernetes 节点上需要安装 NFS Client,并且配置可以访问 NFS 服务器。 StatefulSet 配置错误 StatefulSet 的 volumeClaimTemplates 需要与 PVC 的定义相匹配。 存储卷读写权限问题 存储卷需要有读写权限,避免因权限问题无法挂载。 ","date":"2022-05-18","objectID":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 通过 nfs 动态创建 pv 异常","uri":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/"},{"categories":["Kubernetes"],"content":"nfs 在kubernetes中动态创建pv和版本的关系 在早期的 Kubernetes 版本中(如 v1.11 之前),NFS provisioner 并不包含在默认部署中,这会导致通过 NFS 存储类无法动态创建 PV。 从 Kubernetes v1.11 开始,NFS 动态供应功能成为了默认部署的一部分,但也需要进行额外的配置,主要步骤包括: 安装 NFS 客户端组件 部署 NFS 动态供应器 external-provisioner 创建 StorageClass,指定 nfs 作为 provisioner 创建 PersistentVolumeClaim, 引用该 StorageClass 此时,NFS 供应器就可以根据 PVC 的请求动态创建 PV 来绑定 PVC。 所以简单来说,Kubernetes 低版本中需要手动安装和配置 NFS 动态供应器; 高版本中已内置但也需要显式配置才能启用该功能。正确配置后就可以通过 NFS StorageClass 来动态创建 PV。 这里的高版本指的是v1.20 之后，在 kube-apiserver 的启动参数中不移除 API 对象中的 selfLink 字段。 selfLink 是 Kubernetes 中每个 API 对象的一个字段,用于表示对象自身的 URL 地址 从 Kubernetes 1.20 版本开始,该字段默认被移除了。但可以通过设置 RemoveSelfLink=false 来保留该字段 原因是 selfLink 字段已很少被用到,但删去后可能会影响部分老版本的客户端。所以提供了这个特性开关来向后兼容 一般来说,除非确实需要兼容老版本客户端,否则不建议保留 selfLink 字段 显式开启方式： $ vi /etc/kubernetes/manifests/kube-apiserver.yaml .... spec: containers: - command: - kube-apiserver - --feature-gates=RemoveSelfLink=false ","date":"2022-05-18","objectID":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes 通过 nfs 动态创建 pv 异常","uri":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/"},{"categories":["Kubernetes"],"content":"使用 kubernetes 经常会用到命令行，这里对命令行进行分类整理 集群管理 kubectl cluster-info kubectl config kubectl version kubectl api-versions 查看当前集群支持的api版本 kubectl get node kubectl get pod kubectl get service kubectl cordon 命令：用于标记某个节点不可调度 kubectl uncordon 命令：用于标签节点可以调度 kubectl drain 命令： 用于在维护期间排除节点。 kubectl taint 命令：用于给某个Node节点设置污点 Pod 管理 kubectl create pod kubectl get pod kubectl get pod pod_name –show-labels kubectl describe pod kubectl logs kubectl exec kubectl delete pod 资源监测 kubectl top node kubectl top pods kubectl get quota kubectl describe Service管理 kubectl create service kubectl get service kubectl expose kubectl describe service kubectl delete service kubectl port-forwad 配置和加密 kubectl create configmap kubectl get configmap kubectl create secret kubectl get secret kubectl describe configmap kubectl describe secret Deployment管理 kubectl create deployment kubectl get deployment kubectl scale deployment sudo kubectl scale deployment deploy-name –replicas=0 不删除应用暂停服务 kubectl autoscale deployment kubectl rollout status kubectl rollout history kubectl delete deployment 名字空间管理 kubectl create namespace kubectl get namespace kubectl describe namespace kubectl delete namespace kubectl apply -n kubectl switch -n kubeadm管理 kubeadm init 启动引导一个 Kubernetes 主节点 kubeadm join 启动引导一个 Kubernetes 工作节点并且将其加入到集群 kubeadm upgrade 更新 Kubernetes 集群到新版本 kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadm upgrade 命令 kubeadm token 使用 kubeadm join 来管理令牌 kubeadm reset 还原之前使用 kubeadm init 或者 kubeadm join 对节点所作改变 kubeadm version 打印出 kubeadm 版本 kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈 kubeadm token create –print-join-command 创建node加入口令 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:0:0","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"命令说明 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:0","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"1. kubectl expose 和 kubectl port-forwad 的区别 kubectl expose 和 kubectl port-forward 是两个不同的命令，它们在 Kubernetes 中有不同的作用和用法。 kubectl expose 命令用于创建一个新的 Service 对象来暴露 Deployment、ReplicationController 或 ReplicaSet 等其他资源。它会为指定的资源创建一个新的 ClusterIP 类型的 Service，在集群内部通过 Service IP 和端口号来访问暴露的资源。这个 Service 对象可以是临时的，也可以是永久的，它根据指定的参数设置来控制其行为。 kubectl port-forward 命令用于在本地机器和 Kubernetes 集群之间建立一个临时的协议转发，将本地的端口与 Kubernetes Pod 或 Service 的端口进行绑定。这样可以直接通过本地机器上的端口来访问 Kubernetes 集群中的服务或应用。该命令通常用于开发和调试，比如在本地机器上直接访问运行在集群中的应用程序或服务的日志。 总结来说，kubectl expose 用于创建一个新的 Service 对象来暴露 Kubernetes 资源，而 kubectl port-forward 则用于建立本地与集群之间的临时端口转发，方便本地机器访问集群中的服务或应用程序。 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:1","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"2. kubectl apply 和 kubectl switch 的区别 kubectl apply 用于在 Kubernetes 集群上部署或更新资源对象，可以通过文件或目录进行声明。它会读取文件中的资源定义，并在集群中创建或更新相应的资源对象。kubectl switch 是一个非官方的插件，用于切换当前上下文的集群。Kubernetes 集群允许存在多个上下文，每个上下文对应一个集群。 kubectl switch 可以方便地在多个集群之间切换，目的是为了方便用户在不同的集群之间进行操作。因此，kubectl apply 用于部署和更新资源对象，而 kubectl switch 用于切换当前操作的集群。它们是两个不同的命令，用途和功能不同。 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:2","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"3. kubectl scale 和 kubectl autoscale的区别 kubectl scale和kubectl autoscale是用于扩容或缩容Kubernetes Deployment或ReplicaSet的命令，但它们有一些区别。 kubectl scale命令允许通过手动指定副本数量来更新Deployment或ReplicaSet的副本数量。例如，可以使用以下命令将副本数量设置为3： kubectl scale deployment/my-deployment --replicas=3 kubectl autoscale命令则是根据指定的CPU使用率来自动扩容或缩容Deployment。它会创建一个HorizontalPodAutoscaler(HPA)对象，该对象会根据CPU的使用情况动态地增加或减少Pod的副本数量以满足指定的CPU目标使用率。例如，可以使用以下命令将Pod的副本数量自动调整为至少2个，当CPU使用率超过50%时： kubectl autoscale deployment/my-deployment --min=2 --max=10 --cpu-percent=50 总结起来，kubectl scale用于手动指定副本数量来更新Deployment或ReplicaSet的副本数量，而kubectl autoscale用于根据CPU使用率自动扩容或缩容Pod的副本数量。 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:3","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"4. kubectl exec和kubectl attach 的区别 kubectl exec和kubectl attach是用于与正在运行的Pod进行交互的命令，但它们在使用方式和功能上有一些区别。 kubectl exec命令用于在正在运行的Pod中执行命令。它可以连接到正在运行的容器，并在容器中执行指定的命令。例如，可以使用以下命令在名为my-pod的Pod中执行echo命令： kubectl exec -it my-pod -- echo \"Hello, World!\" kubectl attach命令用于将本地终端连接到正在运行的Pod中的某个容器的标准输入、输出和错误流。它可以与容器建立交互式会话。例如，可以使用以下命令将本地终端连接到名为my-pod的Pod中的第一个容器： kubectl attach -it my-pod 总结起来，kubectl attach命令类似于kubectl exec，但它附加到容器中运行的主进程，而不是运行额外的进程。 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:4","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"5. kubectl patch和kubectl replace的区别 kubectl patch和kubectl replace是用于部分更新和替换更新Kubernetes资源的命令，但它们在更新方式和行为上有一些区别。 kubectl patch命令用于部分更新Kubernetes资源的特定字段。它可以通过提供一个JSON或YAML文件、一个JSON或YAML字符串，或者使用特定的操作类型（如add、remove、replace等）来更新资源的字段。例如，可以使用以下命令将名为my-deployment的Deployment资源的labels字段添加一个新的标签： kubectl patch deployment/my-deployment -p '{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"new-label\":\"value\"}}}}}' kubectl replace命令用于替换更新整个Kubernetes资源。它会基于提供的配置文件完全替换现有资源的内容。例如，可以使用以下命令将名为my-deployment的Deployment资源替换为一个新的配置文件： kubectl replace -f new-deployment.yaml 总结起来，kubectl patch用于部分更新Kubernetes资源的特定字段，而kubectl replace用于完全替换更新整个Kubernetes资源。 ","date":"2022-04-26","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:5","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["linux"],"content":"netstat netstat是一个常用的命令行网络工具，用于显示与网络连接、路由表和网络接口相关的信息。以下是一些常用的netstat命令： netstat -tunlp：显示所有TCP和UDP端口的监听情况，以及对应的进程信息。 netstat -tuln：显示所有TCP端口的监听情况，包括本地地址、外部地址和状态。 netstat -a：显示所有活动的网络连接，包括监听和非监听状态。 netstat -r：显示路由表，包括目标地址、网关、子网掩码和接口。 netstat -s：显示网络统计信息，包括传输的数据包数量、错误数量和丢失数量等。 netstat -i：显示网络接口信息，包括接口名称、MAC地址、IP地址和状态等。 netstat -c：持续输出网络连接的信息，每隔一段时间刷新一次。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:1:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"sar sar（System Activity Reporter）是一个性能监控工具，它可以提供系统资源使用情况的历史数据。以下是一些常用的sar命令和相应的使用场景： sar -u：显示CPU使用情况。 使用场景：了解系统CPU的平均负载、用户空间CPU使用率、系统空间CPU使用率，以及等待I/O的CPU使用率。 sar -r：显示内存使用情况。 使用场景：查看主要的内存指标，如总内存、可用内存、内存利用率、缓存和缓冲区。 sar -n DEV：显示网络接口的数据传输情况。 使用场景：监测网络流量、带宽使用情况，识别网络瓶颈。 sar -q：显示系统负载情况。 使用场景：通过观察平均负载、运行队列长度和就绪进程数，了解系统性能和负载情况。 sar -b：显示系统的I/O使用情况。 使用场景：监测磁盘IO的情况，包括块读写次数、块传输速率，查找磁盘性能问题。 sar -W：显示系统交换空间的使用情况。 使用场景：了解交换空间的使用情况，识别系统内存不足导致的交换瓶颈。 sar -d：显示块设备的I/O统计信息。 使用场景：查看块设备（硬盘）的读写操作、传输速率和请求队列长度。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:2:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"Example 统计sockets连接新 sar -n SOCK $ sar -n SOCK 1 1 Linux 4.4.115-1.el7.elrepo.x86_64 (chm) 2022年05月20日 _x86_64_ (12 CPU) 14时05分40秒 totsck tcpsck udpsck rawsck ip-frag tcp-tw 14时05分41秒 7725 76 5 0 0 642 平均时间: 7725 76 5 0 0 642 totsck 当前被使用的socket总数 tcpsck 当前正在被使用的TCP的socket总数 udpsck 当前正在被使用的UDP的socket总数 rawsck 当前正在被使用于RAW的skcket总数 if-frag 当前的IP分片的数目 tcp-tw TCP套接字中处于TIME-WAIT状态的连接数量 这些sar命令可用于监控系统资源的使用情况，帮助诊断性能问题和优化系统配置。您可以通过调整命令的参数来获取特定的监控数据，并结合其他工具和命令进行综合分析 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:2:1","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"telnet telnet是一个用于远程登录和管理远程主机的网络协议和工具。以下是一些常用的telnet命令和相应的使用场景： 1.telnet host：连接到指定的远程主机。 使用场景：远程登录到其他计算机，进行命令行管理和操作。 2.telnet host port：连接到指定主机的特定端口。 使用场景：检查特定服务的可用性，如通过telnet测试SMTP服务器是否能够连接。 3.Ctrl + ]：进入telnet控制台。 使用场景：在telnet会话中，按下Ctrl + ]键后，可以进入控制台，执行一些附加功能，如终止会话、更改设置等。 4.send string：发送字符串到远程主机。 使用场景：在telnet会话中，可以使用send命令发送特定的字符串至远程主机，用于模拟用户输入等情景。 5.display或toggle options：显示或切换选项状态。 使用场景：在telnet会话中，可以使用这两个命令查看或切换当前会话的选项状态，例如回显、行编辑等。 6.quit：退出当前telnet会话。 使用场景：在完成telnet会话后，使用该命令退出并关闭连接。 telnet命令可以用于远程登录和管理远程主机，但由于安全性较差，已逐渐被SSH等更安全的协议取代。在实际应用中，建议使用更加安全的远程连接方法，如SSH（Secure Shell）。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:3:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"ss ss（Socket Statistics）是一个Linux系统中用于显示当前活动套接字（socket）信息的命令。它可以提供更详细和全面的套接字统计数据。以下是一些常用的ss命令和相应的使用场景： 1.ss -t：显示TCP套接字信息。 使用场景：查看当前系统上的TCP连接信息，包括本地地址和端口、远程地址和端口、连接状态等。 2.ss -u：显示UDP套接字信息。 使用场景：查看当前系统上的UDP连接信息，包括本地地址和端口、远程地址和端口等。 3.ss -l：显示监听套接字信息。 使用场景：查看当前系统上正在监听的套接字信息，包括监听的协议、本地地址和端口等。 4.ss -p：显示套接字及其关联进程信息。 使用场景：查看每个套接字对应的关联进程的详细信息，包括进程ID、用户、命令等。 5.ss -n：以数字形式展示套接字信息。 使用场景：显示IP地址和端口号时不进行反向解析，加快输出速度。 6.ss -s：显示套接字统计摘要(包含IPv6)。 使用场景：显示系统级别的套接字统计信息，包括打开的套接字数、活动连接数、侦听套接字数等。 7.ss -o：显示定时器相关信息。 使用场景：查看系统中的定时器事件相关信息，如计时器名称、超时时间、重复间隔等。 这些ss命令可用于监控和诊断网络连接和套接字的状态，帮助定位网络问题并进行性能调优。在实际应用中，可以结合其他命令和工具（如grep、netstat等）进行更深入的分析和排查。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:4:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["hugo"],"content":"test test LoveIt提供了admonition shortcode，支持 12 种样式，可以在页面中插入提示的横幅。代码如下 ","date":"2021-09-15","objectID":"/first_post/:1:0","tags":["hugo"],"title":"First_post","uri":"/first_post/"},{"categories":["hugo"],"content":"主题自带的admonition样式 注意\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\r注意\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\r","date":"2021-09-15","objectID":"/first_post/:2:0","tags":["hugo"],"title":"First_post","uri":"/first_post/"},{"categories":[""],"content":"前言 [info] About Life 我们在人的一生中最为辉煌的一天，并不是功成名就的那一天，而是从悲叹和绝望中产生对人生挑战的欲望，并且勇敢的迈向这种挑战的那一天。 人生当中成功只是一时的，失败却是主旋律。但是如何面对失败却把人分成了不同的样子。有的人会被失败击垮，有的人能够不断的爬起来，继续向前。 我想真正的成熟，应该并不是追求完美，而是直面自己的缺憾，这才是生活的本质。 也许他们会明白莫泊桑的一句话：生活可能不像你想象的那么好，但是也不会像你想象的那么糟。人的脆弱和坚强都超乎了自己的想象。有时候，可能脆弱的一句话就泪流满面，有时候，你发现自己咬着牙已经走过了很长的路， 人的一生中最大的问题不是找不到正确的方法而是在实践中长久的悖逆人性 [success] About Persistence(最淡的墨水也胜过最强的记忆) Nothing in the world can take the place of Persistence. Talent will not; nothing is more common than unsuccessful men with talent. Genius will not; unrewarded genius is almost a proverb. Education will not; the world is full of educated derelicts. Persistence and Determination alone are omnipotent. The slogan “Press On” has solved and will always solve the problems of the human race. 个人介绍 简要介绍：人间大白,目前居住南京，目前在一家云计算公司从事DevOps相关的工作，喜欢云原生并通过了CKA,CKS认证,对监控落地有相关经验， 熟练使用Docker、K8s，有生产k8s集群的构建、运维、优化、排错经验，熟悉各种开源中间件的部署和优化，有运维自动化、监控系统 相关开发经验。 个人信息：卢栓 拿手菜：清蒸鲈鱼、酸辣土豆丝、西红柿炒蛋、炒花蛤、香辣小炒肉 兴趣爱好：篮球、音乐、阅读、掼蛋 微信交流：shuanlu0614 邮箱：lushuan2071@126.com 工具链: Kubernetes Prometheus InfluxDB Grafana Ansible Docker Helm Linux GoLang Shell MySQL ","date":"2021-09-15","objectID":"/about/:0:0","tags":[""],"title":"About","uri":"/about/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 ContainerCreating 或 Waiting 状态 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 配置错误 检查是否打包了正确的镜像 检查配置了正确的容器参数 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"磁盘爆满 启动 Pod 会调 CRI 接口创建容器，容器运行时创建容器时通常会在数据目录下为新建的容器创建一些目录和文件，如果数据目录所在的磁盘空间满了就会创建失败并报错: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 2m (x4307 over 16h) kubelet, 10.179.80.31 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod \"apigateway-6dc48bf8b6-l8xrw\": Error response from daemon: mkdir /var/lib/docker/aufs/mnt/1f09d6c1c9f24e8daaea5bf33a4230de7dbc758e3b22785e8ee21e3e3d921214-init: no space left on device ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"limit 设置太小或者单位不对 如果 limit 设置过小以至于不足以成功运行 Sandbox 也会造成这种状态，常见的是因为 memory limit 单位设置不对造成的 limit 过小，比如误将 memory 的 limit 单位像 request 一样设置为小 m，这个单位在 memory 不适用，会被 k8s 识别成 byte， 应该用 Mi 或 M。， 举个例子: 如果 memory limit 设为 1024m 表示限制 1.024 Byte，这么小的内存， pause 容器一起来就会被 cgroup-oom kill 掉，导致 pod 状态一直处于 ContainerCreating。 这种情况通常会报下面的 event: Pod sandbox changed, it will be killed and re-created。 kubelet报错 to start sandbox container for pod ... Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused \"process_linux.go:301: running exec setns process for init caused \\\"signal: killed\\\"\": unknown ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"拉取镜像失败 镜像拉取失败也分很多情况，这里列举下: 配置了错误的镜像 Kubelet 无法访问镜像仓库（比如默认 pause 镜像在 gcr.io 上，国内环境访问需要特殊处理） 拉取私有镜像的 imagePullSecret 没有配置或配置有误 镜像太大，拉取超时（可以适当调整 kubelet 的 —image-pull-progress-deadline 和 —runtime-request-timeout 选项） ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"controller-manager 异常 查看 master 上 kube-controller-manager 状态，异常的话尝试重启。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:5","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Error 状态 通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括： 依赖的 ConfigMap、Secret 或者 PV 等不存在 请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等 违反集群的安全策略 容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 ImagePullBackOff 状态 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"http 类型 registry，地址未加入到 insecure-registry dockerd 默认从 https 类型的 registry 拉取镜像，如果使用 https 类型的 registry，则必须将它添加到 insecure-registry 参数中，然后重启或 reload dockerd 生效。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"https 自签发类型 resitry，没有给节点添加 ca 证书 如果 registry 是 https 类型，但证书是自签发的，dockerd 会校验 registry 的证书，校验成功才能正常使用镜像仓库， 要想校验成功就需要将 registry 的 ca 证书放置到 /etc/docker/certs.d/\u003cregistry:port\u003e/ca.crt 位置。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"私有镜像仓库认证失败 如果 registry 需要认证，但是 Pod 没有配置 imagePullSecret，配置的 Secret 不存在或者有误都会认证失败。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"镜像文件损坏 如果 push 的镜像文件损坏了，下载下来也用不了，需要重新 push 镜像文件 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"镜像拉取超时 如果节点上新起的 Pod 太多就会有许多可能会造成容器镜像下载排队，如果前面有许多大镜像需要下载很长时间，后面排队的 Pod 就会报拉取超时。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:5","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"镜像不存在 kubelet 日志： PullImage \"imroc/test:v0.2\" from image service failed: rpc error: code = Unknown desc = Error response from daemon: manifest for imroc/test:v0.2 not found ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:6","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Pending 状态 Pending 状态说明 Pod 还没有被调度到某个节点上，需要看下 Pod 事件进一步判断原因，比如: $ kubectl describe pod tikv-0 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 3m (x106 over 33m) default-scheduler 0/4 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient cpu, 3 Insufficient memory. 下面列举下可能原因和解决方法。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"节点资源不够 节点资源不够有以下几种情况: CPU 负载过高 剩余可以被分配的内存不够 如果判断某个 Node 资源是否足够？ 通过 kubectl describe node 查看 node 资源情况，关注以下信息： Allocatable: 表示此节点能够申请的资源总和 Allocated resources: 表示此节点已分配的资源 (Allocatable 减去节点上所有 Pod 总的 Request) $ sudo kubectl describe node 10.10.192.220|grep Allo -A 6 Allocatable: cpu: 8 ephemeral-storage: 33806352329 hugepages-2Mi: 0 memory: 24543516Ki pods: 110 System Info: -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3650m (45%) 50150m (626%) memory 5092Mi (21%) 81290Mi (339%) ephemeral-storage 0 (0%) 0 (0%) 可以看到能够申请的资源总和，当前节点可以创建110个pods，cpu 8核，cpu requests占比45% 前者与后者相减，可得出剩余可申请的资源。如果这个值小于 Pod 的 request，就不满足 Pod 的资源要求，Scheduler 在 Predicates (预选) 阶段就会剔除掉这个 Node，也就不会调度上去 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"不满足 nodeSelector 与 affinity 如果 Pod 包含 nodeSelector 指定了节点需要包含的 label，调度器将只会考虑将 Pod 调度到包含这些 label 的 Node 上，如果没有 Node 有这些 label 或者有这些 label 的 Node 其它条件不满足也将会无法调度。参考官方文档： https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/ 如果 Pod 包含 affinity（亲和性）的配置，调度器根据调度算法也可能算出没有满足条件的 Node，从而无法调度。affinity 有以下几类: nodeAffinity: 节点亲和性，可以看成是增强版的 nodeSelector，用于限制 Pod 只允许被调度到某一部分 Node。 podAffinity: Pod 亲和性，用于将一些有关联的 Pod 调度到同一个地方，同一个地方可以是指同一个节点或同一个可用区的节点等。 podAntiAffinity: Pod 反亲和性，用于避免将某一类 Pod 调度到同一个地方避免单点故障，比如将集群 DNS 服务的 Pod 副本都调度到不同节点，避免一个节点挂了造成整个集群 DNS 解析失败，使得业务中断。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Node 存在 Pod 没有容忍的污点 如果节点上存在污点 (Taints)，而 Pod 没有响应的容忍 (Tolerations)，Pod 也将不会调度上去。通过 describe node 可以看下 Node 有哪些 Taints: $ kubectl describe nodes host1 ... Taints: special=true:NoSchedule ... 污点既可以是手动添加也可以是被自动添加 手动添加污点 $ kubectl taint node host1 special=true:NoSchedule node \"host1\" tainted 另外，有些场景下希望新加的节点默认不调度 Pod，直到调整完节点上某些配置才允许调度，就给新加的节点都加上 node.kubernetes.io/unschedulable 这个污点 自动添加污点 如果节点运行状态不正常，污点也可以被自动添加，从 v1.12 开始，TaintNodesByCondition 特性进入 Beta 默认开启，controller manager 会检查 Node 的 Condition，如果命中条件就自动为 Node 加上相应的污点，这些 Condition 与 Taints 的对应关系如下: Conditon Value Taints -------- ----- ------ OutOfDisk True node.kubernetes.io/out-of-disk Ready False node.kubernetes.io/not-ready Ready Unknown node.kubernetes.io/unreachable MemoryPressure True node.kubernetes.io/memory-pressure PIDPressure True node.kubernetes.io/pid-pressure DiskPressure True node.kubernetes.io/disk-pressure NetworkUnavailable True node.kubernetes.io/network-unavailable 解释下上面各种条件的意思: OutOfDisk 为 True 表示节点磁盘空间不够了 Ready 为 False 表示节点不健康 Ready 为 Unknown 表示节点失联，在 node-monitor-grace-period 这么长的时间内没有上报状态 controller-manager 就会将 Node 状态置为 Unknown (默认 40s) MemoryPressure 为 True 表示节点内存压力大，实际可用内存很少 PIDPressure 为 True 表示节点上运行了太多进程，PID 数量不够用了 DiskPressure 为 True 表示节点上的磁盘可用空间太少了 NetworkUnavailable 为 True 表示节点上的网络没有正确配置，无法跟其它 Pod 正常通信 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"kube-scheduler 没有正常运行 检查 maser 上的 kube-scheduler 是否运行正常，异常的话可以尝试重启临时恢复。 $ sudo kubectl -n kube-system get pod|grep kube-scheduler kube-scheduler-10.10.192.220 1/1 Running 183 (40d ago) 424d ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Terminating 状态 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"磁盘爆满 如果 docker 的数据目录所在磁盘被写满，docker 无法正常运行，无法进行删除和创建操作，所以 kubelet 调用 docker 删除容器没反应，看 event 类似这样： Normal Killing 39s (x735 over 15h) kubelet, 10.179.80.31 Killing container with id docker://apigateway:Need to kill Pod ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"存在 “i” 文件属性 如果容器的镜像本身或者容器启动后写入的文件存在 “i” 文件属性，此文件就无法被修改删除，而删除 Pod 时会清理容器目录，但里面包含有不可删除的文件，就一直删不了，Pod 状态也将一直保持 Terminating，kubelet 报错: Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.922965 14109 remote_runtime.go:250] RemoveContainer \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\" from runtime service failed: rpc error: code = Unknown desc = failed to remove container \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\": Error response from daemon: container 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver \"overlay2\" failed to remove root filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868/diff/usr/bin/bash: operation not permitted Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.923027 14109 kuberuntime_gc.go:126] Failed to remove container \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\": rpc error: code = Unknown desc = failed to remove container \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\": Error response from daemon: container 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver \"overlay2\" failed to remove root filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868/diff/usr/bin/bash: operation not permitted 通过 man chattr 查看 “i” 文件属性描述: A file with the 'i' attribute cannot be modified: it cannot be deleted or renamed, no link can be created to this file and no data can be written to the file. Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear this attribute. 彻底解决当然是不要在容器镜像中或启动后的容器设置 “i” 文件属性，临时恢复方法： 复制 kubelet 日志报错提示的文件路径，然后执行 chattr -i : chattr -i /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868/diff/usr/bin/bash 执行完后等待 kubelet 自动重试，Pod 就可以被自动删除了。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"docker 17 的 bug docker hang 住，没有任何响应，看 event: Warning FailedSync 3m (x408 over 1h) kubelet, 10.179.80.31 error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded 怀疑是17版本dockerd的BUG。可通过 kubectl -n cn-staging delete pod apigateway-6dc48bf8b6-clcwk –force –grace-period=0 强制删除pod，但 docker ps 仍看得到这个容器 处置建议： 升级到docker 18. 该版本使用了新的 containerd，针对很多bug进行了修复。 如果出现terminating状态的话，可以提供让容器专家进行排查，不建议直接强行删除，会可能导致一些业务上问题。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"存在 Finalizers k8s 资源的 metadata 里如果存在 finalizers，那么该资源一般是由某程序创建的，并且在其创建的资源的 metadata 里的 finalizers 加了一个它的标识，这意味着这个资源被删除时需要由创建资源的程序来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。 处理建议：kubectl edit 手动编辑资源定义，删掉 finalizers，这时再看下资源，就会发现已经删掉了。通过prometheus-operator 创建prometheus时，最后无法删除namespace 也是此原因。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Unknown 状态 通常是节点失联，没有上报状态给 apiserver，到达阀值后 controller-manager 认为节点失联并将其状态置为 Unknown。 可能原因: 节点高负载导致无法上报 节点宕机 节点被关机 网络不通 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:6:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 健康检查失败 Kubernetes 健康检查包含就绪检查(readinessProbe)和存活检查(livenessProbe) pod 如果就绪检查失败会将此 pod ip 从 service 中摘除，通过 service 访问，流量将不会被转发给就绪检查失败的 pod pod 如果存活检查失败，kubelet 将会杀死容器并尝试重启 健康检查失败的可能原因有多种，除了业务程序BUG导致不能响应健康检查导致 unhealthy，还能有有其它原因，下面我们来逐个排查。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"健康检查配置不合理 initialDelaySeconds 太短，容器启动慢，导致容器还没完全启动就开始探测，如果 successThreshold 是默认值 1，检查失败一次就会被 kill，然后 pod 一直这样被 kill 重启。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"节点负载高 cpu 占用高（比如跑满）会导致进程无法正常发包收包，通常会 timeout，导致 kubelet 认为 pod 不健康。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"容器内进程端口监听挂掉 使用 netstat -tunlp 检查端口监听是否还在，如果不在了，抓包可以看到会直接 reset 掉健康检查探测的连接: 20:15:17.890996 IP 172.16.2.1.38074 \u003e 172.16.2.23.8888: Flags [S], seq 96880261, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0 20:15:17.891021 IP 172.16.2.23.8888 \u003e 172.16.2.1.38074: Flags [R.], seq 0, ack 96880262, win 0, length 0 20:15:17.906744 IP 10.0.0.16.54132 \u003e 172.16.2.23.8888: Flags [S], seq 1207014342, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0 20:15:17.906766 IP 172.16.2.23.8888 \u003e 10.0.0.16.54132: Flags [R.], seq 0, ack 1207014343, win 0, length 0 连接异常，从而健康检查失败。发生这种情况的原因可能在一个节点上启动了多个使用 hostNetwork 监听相同宿主机端口的 Pod，只会有一个 Pod 监听成功，但监听失败的 Pod 的业务逻辑允许了监听失败，并没有退出，Pod 又配了健康检查，kubelet 就会给 Pod 发送健康检查探测报文，但 Pod 由于没有监听所以就会健康检查失败。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 处于 CrashLoopBackOff 状态 Pod 如果处于 CrashLoopBackOff 状态说明之前是启动了，只是又异常退出了，应用实例状态为CrashLoopBackOff， 表现为实例不断重启，由ready状态变为Complete再变成CrashLoopBackOff。 一般由于应用实例容器异常退出导致的实例异常，需要排查应用日志确定异常退出原因。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"容器进程主动退出 如果是容器进程主动退出，退出状态码一般在 0-128 之间，除了可能是业务程序 BUG，还有其它许多可能原因 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"系统OOM 如果发生系统 OOM，可以看到 Pod 中容器退出状态码是 137，表示被 SIGKILL 信号杀死，同时内核会报错: Out of memory: Kill process …。大概率是节点上部署了其它非 K8S 管理的进程消耗了比较多的内存，或者 kubelet 的 –kube-reserved 和 –system-reserved 配的比较小，没有预留足够的空间给其它非容器进程，节点上所有 Pod 的实际内存占用总量不会超过 /sys/fs/cgroup/memory/kubepods 这里 cgroup 的限制，这个限制等于 capacity - “kube-reserved” - “system-reserved”，如果预留空间设置合理，节点上其它非容器进程（kubelet, dockerd, kube-proxy, sshd 等) 内存占用没有超过 kubelet 配置的预留空间是不会发生系统 OOM 的，可以根据实际需求做合理的调整。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"cgroup OOM 如果是 cgroup OOM 杀掉的进程，从 Pod 事件的下 Reason 可以看到是 OOMKilled，说明容器实际占用的内存超过 limit 了，同时内核日志会报: ``。 可以根据需求调整下 limit ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["linux"],"content":"awk 是 Linux/Unix 系统中的一个强大的文本处理工具,主要用于处理文本文件和字符串。 ","date":"2021-06-29","objectID":"/awk/:0:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"特性 awk 的一些关键特性: 可以根据字段进行操作,如打印指定列、求列的总和等 支持条件判断和循环语句,可以实现复杂的文本处理 使用正则表达式进行模式匹配和内容提取 内置字符串操作函数,如匹配、替换、截取等 ","date":"2021-06-29","objectID":"/awk/:1:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"适用场景 awk 适用于报表生成、格式转换、数学运算等场景。掌握了 awk,可以大大提高 Bash 脚本的文本处理能力 ","date":"2021-06-29","objectID":"/awk/:2:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"内置函数 内置函数 command 释义 gsub(r,s) 在整个$0中用s替代r 相当于 sed ’s///g' gsub(r,s,t) 在整个t中用s替代r index(s,t) 返回s中字符串t的第一位置 length(s) 返回s长度 match(s,r) 测试s是否包含匹配r的字符串 split(s,a,fs) 在fs上将s分成序列a sprint(fmt,exp) 返回经fmt格式化后的exp sub(r,s) 用$0中最左边最长的子串代替s 相当于 sed ’s///' substr(s,p) 返回字符串s中从p开始的后缀部分 substr(s,p,n) 返回字符串s中从p开始长度为n的后缀部分 ","date":"2021-06-29","objectID":"/awk/:3:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"awk 判断 awk ‘{print ($1\u003e$2)?“第一排”$1:“第二排”$2}’ # 条件判断 括号代表if语句判断 “?“代表then “:“代表else awk ‘{max=($1\u003e$2)? $1 : $2; print max}’ # 条件判断 如果$1大于$2,max值为为$1,否则为$2 awk ‘{if ( $6 \u003e 50) print $1 \" Too high” ;else print “Range is OK”}’ file awk ‘{if ( $6 \u003e 50) { count++;print $3 } else { x+5; print $2 } }’ file ","date":"2021-06-29","objectID":"/awk/:4:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"awk 循环 awk ‘{i = 1; while ( i \u003c= NF ) { print NF, $i ; i++ } }’ file awk ‘{ for ( i = 1; i \u003c= NF; i++ ) print NF,$i }’ file ","date":"2021-06-29","objectID":"/awk/:5:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"示例 awk ‘{print $1, $3}’ file.txt 每行按空格分割成多个字段,输出第1个和第3个字段 awk ‘{sum+=$1*$1} END {print sum}’ file.txt 求第一列的平方和 awk ’length \u003e 80’ file.txt 查找长度大于80的行 awk ‘/regex/ {print $0}’ log.txt 使用正则表达式匹配内容 awk ‘{print $NF}’ file.txt 打印最后一列 awk ‘{print $(NF-1) }’ file.txt 打印倒数第二列 awk ‘{OFS=”#”;$1=$1;print $0}’ file.txt OFS 输出记录分隔符 awk ‘{if(NR==3){print $0} else {print “不是第三行”}}’ file.txt 打印指定行 awk ‘NR==1||NR==3{print $0}’ test.txt 打印第一行或者第三行 awk ‘/Tom/’ file # 打印匹配到得行 awk ‘/^Tom/{print $1}’ # 匹配Tom开头的行 打印第一个字段 awk ‘$1 !~ /ly$/’ # 显示所有第一个字段不是以ly结尾的行 awk ‘$3 \u003c40’ # 如果第三个字段值小于40才打印 awk ‘$4==90{print $5}’ # 取出第四列等于90的第五列 awk ‘/^(no|so)/’ test # 打印所有以模式no或so开头的行 awk ‘$3 * $4 \u003e 500’ # 算术运算(第三个字段和第四个字段乘积大于500则显示) awk ‘{print NR” “$0}’ # 加行号 awk ‘/tom/,/suz/’ # 打印tom到suz之间的行 awk ‘{a+=$1}END{print a}’ # 列求和 awk ‘sum+=$1{print sum}’ # 将$1的值叠加后赋给sum awk ‘{a+=$1}END{print a/NR}’ # 列求平均值 awk ‘!s[$1 $3]++’ file # 根据第一列和第三列过滤重复行 awk -F’[ :\\t]’ ‘{print $1,$2}’ # 以空格、:、制表符Tab为分隔符 awk ‘{print “’\"$a”’”,\"’\"$b\"’\"}’ # 引用外部变量 awk ‘{if(NR==52){print;exit}}’ # 显示第52行 awk ‘/关键字/{a=NR+2}a==NR {print}’ # 取关键字下第几行 awk ‘gsub(/liu/,“aaaa”,$1){print $0}’ # 只打印匹配替换后的行 ll | awk -F’[ ]+|[ ][ ]+’ ‘/^$/{print $8}’ # 提取时间,空格不固定 awk ‘{$1=\"\";$2=\"\";$3=\"\";print}’ # 去掉前三列 echo aada:aba|awk ‘/d/||/b/{print}’ # 匹配两内容之一 echo aada:abaa|awk -F: ‘$1~/d/||$2~/b/{print}’ # 关键列匹配两内容之一 echo Ma asdas|awk ‘$1~/^[a-Z][a-Z]$/{print }’ # 第一个域匹配正则 echo aada:aaba|awk ‘/d/\u0026\u0026/b/{print}’ # 同时匹配两条件 awk ’length($1)==“4”{print $1}’ # 字符串位数 awk ‘{if($2\u003e3){system (“touch “$1)}}’ # 执行系统命令 awk ‘{sub(/Mac/,“Macintosh”,$0);print}’ # 用Macintosh替换Mac awk ‘{gsub(/Mac/,“MacIntosh”,$1); print}’ # 第一个域内用Macintosh替换Mac awk -F ’’ ‘{ for(i=1;i\u003cNF+1;i++)a+=$i ;print a}’ # 多位数算出其每位数的总和.比如 1234， 得到 10 awk ‘{ i=$1%10;if ( i == 0 ) {print i}}’ # 判断$1是否整除(awk中定义变量引用时不能带 $ ) awk ‘BEGIN{a=0}{if ($1\u003ea) a=$1 fi}END{print a}’ # 列求最大值 设定一个变量开始为0，遇到比该数大的值，就赋值给该变量，直到结束 awk ‘BEGIN{a=11111}{if ($1\u003ca) a=$1 fi}END{print a}’ # 求最小值 awk ‘{if(A)print;A=0}/regexp/{A=1}’ # 查找字符串并将匹配行的下一行显示出来，但并不显示匹配行 awk ‘/regexp/{print A}{A=$0}’ # 查找字符串并将匹配行的上一行显示出来，但并不显示匹配行 awk ‘{if(!/mysql/)gsub(/1/,“a”);print $0}’ # 将1替换成a，并且只在行中未出现字串mysql的情况下替换 awk ‘BEGIN{srand();fr=int(100*rand());print fr;}’ # 获取随机数 awk ‘{if(NR==3)F=1}{if(F){i++;if(i%7==1)print}}’ # 从第3行开始，每7行显示一次 awk ‘{if(NF\u003c1){print i;i=0} else {i++;print $0}}’ # 显示空行分割各段的行数 echo +null:null |awk -F: ‘$1!~\"^+”\u0026\u0026$2!=“null”{print $0}’ # 关键列同时匹配 awk -v RS=@ ‘NF{for(i=1;i\u003c=NF;i++)if($i) printf $i;print “”}’ # 指定记录分隔符 awk ‘{b[$1]=b[$1]$2}END{for(i in b){print i,b[i]}}’ # 列叠加 awk ‘{ i=($1%100);if ( $i \u003e= 0 ) {print $0,$i}}’ # 求余数 awk ‘{b=a;a=$1; if(NR\u003e1){print a-b}}’ # 当前行减上一行 awk ‘{a[NR]=$1}END{for (i=1;i\u003c=NR;i++){print a[i]-a[i-1]}}’ # 当前行减上一行 awk -F: ‘{name[x++]=$1};END{for(i=0;i\u003cNR;i++)print i,name[i]}’ # END只打印最后的结果,END块里面处理数组内容 awk ‘{sum2+=$2;count=count+1}END{print sum2,sum2/count}’ # $2的总和 $2总和除个数(平均值) awk -v a=0 -F ‘B’ ‘{for (i=1;i\u003cNF;i++){ a=a+length($i)+1;print a }}’ # 打印所以B的所在位置 awk ‘BEGIN{ “date” | getline d; split(d,mon) ; print mon[2]}’ file # 将date值赋给d，并将d设置为数组mon，打印mon数组中第2个元素 awk ‘BEGIN{info=“this is a test2010test!\";print substr(info,4,10);}’ # 截取字符串(substr使用) awk ‘BEGIN{info=“this is a test2010test!\";print index(info,“test”)?“ok”:“no found”;}’ # 匹配字符串(index使用) awk ‘BEGIN{info=“this is a test2010test!\";print match(info,/[0-9]+/)?“ok”:“no found”;}’ # 正则表达式匹配查找(match使用) awk ‘{for(i=1;i\u003c=4;i++)printf $i\"“FS; for(y=10;y\u003c=13;y++) printf $y\"“FS;print “”}’ # 打印前4列和后4列 awk ‘BEGIN{for(n=0;n++\u003c9;){for(i=0;i++\u003cn;)printf i\"x\"n”=“i*n” “;print “”}}’ # 乘法口诀 awk ‘BEGIN{info=“this is a test”;split(info,tA,” “);print length(tA);for(k in tA){print k,tA[k];}}’ # 字符串分割(split使用) awk ‘{if (system (“grep “$2” tmp/* \u003e /dev/null 2\u003e\u00261”) == 0 ) {print $1,“Y”} else {print $1,“N”} }’ a # 执行系统命令判断返回状态 awk ‘{for(i=1;i\u003c=NF;i++) a[i,NR]=$i}END{for(i=1;i\u003c=NF;i++) {for(j=1;j\u003c=NR;j++) printf a[i,j] \" “;print “”}}’ # 将多行转多列 常用示例 #删除temp文件的重复行 awk '!($0 in array) { array[$0]; print }' temp #查看最长使用的10个unix命令 awk '{print $1}' ~","date":"2021-06-29","objectID":"/awk/:6:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"思维导图 ","date":"2021-06-29","objectID":"/awk/:7:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["Kubernetes"],"content":"资源对象概述 Kubernetes中的基本概念和术语大多是围绕资源对象（Resource Object）来说的，而资源对象在总体上可分为以下两类。 某种资源的对象，例如节点（Node）、Pod、服务（Service）、存储卷（Volume） 与资源对象相关的事物与动作，例如标签（Label）、注解（Annotation）、命名空间（Namespace）、部署（Deployment）、HPA、PVC。 集群类 集群（Cluster）表示一个由Master和Node组成的Kubernetes集群。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:0:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Master Master指的是集群的控制节点。在每个Kubernetes集群中都需要有一个或一组被称为Master的节点，来负责整个集群的管理和控制。Master通常占据一个独立的服务器（在高可用部署中建议至少使用3台服务器），是整个集群的\"大脑\"，如果它发生宕机或者不可用，那么对集群内容器应用的管理都将无法实施。 在Master上运行着以下关键进程。 Kubernetes API Server（kube-apiserver）：提供HTTP RESTfulAPI接口的主要服务，是Kubernetes里对所有资源进行增、删、改、查等操作的唯一入口，也是集群控制的入口进程。 Kubernetes Controller Manager（kube-controller-manager）：Kubernetes里所有资源对象的自动化控制中心，可以将其理解为资源对象的\"大总管\"。 Kubernetes Scheduler（kube-scheduler）：负责资源调度（Pod调度）的进程，相当于公交公司的调度室。 另外，在Master上通常还需要部署etcd服务 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:1:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Node Kubernetes集群中除Master外的其他服务器被称为Node，Node在较早的版本中也被称为Minion。与Master一样，Node可以是一台物理主机，也可以是一台虚拟机。Node是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他Node上。 在每个Node上都运行着以下关键进程。 kubelet：负责Pod对应容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能。 kube-proxy：实现Kubernetes Service的通信与负载均衡机制的服务。 容器运行时（如Docker）：负责本机的容器创建和管理。Node可以在运行期间动态增加到Kubernetes集群中，前提是在这个Node上已正确安装、配置和启动了上述关键进程。在默认情况下，kubelet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。一旦Node被纳入集群管理范畴，kubelet进程就会定时向Master汇报自身的情报，例如操作系统、主机CPU和内存使用情况，以及当前有哪些Pod在运行等，这样Master就可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。而某个Node在超过指定时间不上报信息时，会被Master判定为\"失联\"，该Node的状态就被标记为不可用（NotReady），Master随后会触发\"工作负载大转移\"的自动流程。 应用类 Kubernetes中属于应用类的概念和相应的资源对象类型最多，所以应用类也是需要重点学习的一类。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:2:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Service与Pod 应用类相关的资源对象主要是围绕Service（服务）和Pod这两个核心对象展开的。 一般说来，Service指的是无状态服务，通常由多个程序副本提供服务，在特殊情况下也可以是有状态的单实例服务，比如MySQL这种数据存储类的服务。与我们常规理解的服务不同，Kubernetes里的Service具有一个全局唯一的虚拟ClusterIP地址，Service一旦被创建，Kubernetes就会自动为它分配一个可用的ClusterIP地址，而且在Service的整个生命周期中，它的ClusterIP地址都不会改变，客户端可以通过这个虚拟IP地址+服务的端口直接访问该服务，再通过部署Kubernetes集群的DNS服务，就可以实现Service Name（域名）到ClusterIP地址的DNS映射功能，我们只要使用服务的名称（DNS名称）即可完成到目标服务的访问请求。“服务发现\"这个传统架构中的棘手问题在这里首次得以完美解决，同时，凭借ClusterIP地址的独特设计，Kubernetes进一步实现了Service的透明负载均衡和故障自动恢复的高级特性。 通过分析、识别并建模系统中的所有服务为微服务——Kubernetes Service，我们的系统最终由多个提供不同业务能力而又彼此独立的微服务单元组成，服务之间通过TCP/IP进行通信，从而形成强大又灵活的弹性网格，拥有强大的分布式能力、弹性扩展能力、容错能力，程序架构也变得简单和直观许多 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:3:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Pod 为什么Kubernetes会设计出一个全新的Pod概念并且Pod有这样特殊的组成结构？原因如下。 为多进程之间的协作提供一个抽象模型，使用Pod作为基本的调度、复制等管理工作的最小单位，让多个应用进程能一起有效地调度和伸缩。 Pod里的多个业务容器共享Pause容器的IP，共享Pause容器挂接的Volume，这样既简化了密切关联的业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题。 Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。Kubernetes要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术实现，例如Flannel、OpenvSwitch等，因此我们需要牢记一点：在Kubernetes里，一个Pod里的容器与另外主机上的Pod容器能够直接通信。 Pod其实有两种类型：普通的Pod及静态Pod（Static Pod）。后者比较特殊，它并没被存放在Kubernetes的etcd中，而是被存放在某个具体的Node上的一个具体文件中，并且只能在此Node上启动、运行。而普通的Pod一旦被创建，就会被放入etcd中存储，随后被Kubernetes Master调度到某个具体的Node上并绑定（Binding），该Pod被对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动。在默认情况下，当Pod里的某个容器停止时，Kubernetes会自动检测到这个问题并且重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，就会将这个Node上的所有Pod都重新调度到其他节点上。 Pod、容器与Node的关系 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:4:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Label与标签选择器 Label（标签）是Kubernetes系统中的另一个核心概念，相当于我们熟悉的\"标签”。一个Label是一个key=value的键值对，其中的key与value由用户自己指定。Label可以被附加到各种资源对象上，例如Node、Pod、Service、Deployment等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。Label通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。我们可以通过给指定的资源对象捆绑一个或多个不同的Label来实现多维度的资源分组管理功能，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作，例如，部署不同版本的应用到不同的环境中，以及监控、分析应用（日志记录、监控、告警）等。一些常用的Label示例如下。 版本标签：release：stable和release：canary。 环境标签：environment：dev、environment：qa和environment：production。 架构标签：tier：frontend、tier：backend和tier：middleware。 分区标签：partition：customerA和partition：customerB。 质量管控标签：track：daily和track：weekly Label也是Pod的重要属性之一，其重要性仅次于Pod的端口，我们几乎见不到没有Label的Pod Service很重要的一个属性就是标签选择器，如果我们不小心把标签选择器写错了，就会出现指鹿为马的闹剧。如果恰好匹 配到了另一种Pod实例，而且对应的容器端口恰好正确，服务可以正常连接，则很难排查问题，特别是在有众多Service的复杂系统中。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:5:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Pod与Deployment 前面提到，大部分Service都是无状态的服务，可以由多个Pod副本实例提供服务。通常情况下，每个Service对应的Pod服务实例数量都是固定的，如果一个一个地手工创建Pod实例，就太麻烦了，最好是用模板的思路，即提供一个Pod模板（Template），然后由程序根据我们指定的模板自动创建指定数量的Pod实例。这就是Deployment这个资源对象所要完成的事情了。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:6:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Service的ClusterIP地址 既然每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint（Pod IP+containerPort）以被客户端访问，那么现在多个Pod副本组成了一个集群来提供服务，客户端如何访问它们呢？传统的做法是部署一个负载均衡器（软件或硬件），为这组Pod开启一个对外的服务端口如8000端口，并且将这些Pod的Endpoint列表加入8000端口的转发列表中，客户端就可以通过负载均衡器的对外IP地址+8000端口来访问此服务了。Kubernetes也是类似的做法，Kubernetes内部在每个Node上都运行了一套全局的虚拟负载均衡器，自动注入并自动实时更新集群中所有Service的路由表，通过iptables或者IPVS机制，把对Service的请求转发到其后端对应的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。不仅如此，Kubernetes还采用了一种很巧妙又影响深远的设计——ClusterIP地址。我们知道，Pod的Endpoint地址会随着Pod的销毁和重新创建而发生改变，因为新Pod的IP地址与之前旧Pod的不同。Service一旦被创建，Kubernetes就会自动为它分配一个全局唯一的虚拟IP地址——ClusterIP地址，而且在Service的整个生命周期内，其ClusterIP地址不会发生改变，这样一来，每个服务就变成了具备唯一IP地址的通信节点，远程服务之间的通信问题就变成了基础的TCP网络通信问题。 之所以说ClusterIP地址是一种虚拟IP地址，原因有以下几点。 ClusterIP地址仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配IP地址（来源于ClusterIP地址池），与Node和Master所在的物理网络完全无关。 因为没有一个\"实体网络对象\"来响应，所以ClusterIP地址无法被Ping通。ClusterIP地址只能与Service Port组成一个具体的服务访问端点，单独的ClusterIP不具备TCP/IP通信的基础。 ClusterIP属于Kubernetes集群这个封闭的空间，集群外的节点要访问这个通信端口，则需要做一些额外的工作 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:7:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Service的外网访问问题 前面提到，服务的ClusterIP地址在Kubernetes集群内才能被访问，那么如何让集群外的应用访问我们的服务呢？这也是一个相对复杂的问题。要弄明白这个问题的解决思路和解决方法，我们需要先弄明白 Kubernetes的三种IP，这三种IP分别如下。 Node IP：Node的IP地址。 Pod IP：Pod的IP地址。 Service IP：Service的IP地址。 首先，Node IP是Kubernetes集群中每个节点的物理网卡的IP地址，是一个真实存在的物理网络，所有属于这个网络的服务器都能通过这个网络直接通信，不管其中是否有部分节点不属于这个Kubernetes集群。这也表明Kubernetes集群之外的节点访问Kubernetes集群内的某个节点或者TCP/IP服务时，都必须通过Node IP通信。 其次，Pod IP是每个Pod的IP地址，在使用Docker作为容器支持引擎的情况下，它是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟二层网络。前面说过，Kubernetes要求位于不同Node上的Pod都能够彼此直接通信，所以Kubernetes中一个Pod里的容器访问另外一个Pod里的容器时，就是通过Pod IP所在的虚拟二层网络进行通信的，而真实的TCP/IP流量是通过Node IP所在的物理网卡流出的。 在Kubernetes集群内，Service的ClusterIP地址属于集群内的地址，无法在集群外直接使用这个地址。为了解决这个问题，Kubernetes首先引入了NodePort这个概念，NodePort也是解决集群外的应用访问集群内服务的直接、有效的常见做法 NodePort的实现方式是，在Kubernetes集群的每个Node上都为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要用任意一个Node的IP地址+NodePort端口号即可访问此服务，在任意Node上运行netstat命令，就可以看到有NodePort端口被监听。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:8:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"NodePort 存在的一问题引出了Ingress对象 NodePort的确功能强大且通用性强，但也存在一个问题，即每个Service都需要在Node上独占一个端口，而端口又是有限的物理资源，那能不能让多个Service共用一个对外端口呢？这就是后来增加的Ingress资源对象所要解决的问题。在一定程度上，我们可以把Ingress的实现机制理解为基于Nginx的支持虚拟主机的HTTP代理。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:9:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"有状态的应用集群 我们知道，Deployment对象是用来实现无状态服务的多副本自动控制功能的，那么有状态的服务，比如ZooKeeper集群、MySQL高可用集群（3节点集群）、Kafka集群等是怎么实现自动部署和管理的呢？这个问题就复杂多了，这些一开始是依赖StatefulSet解决的，但后来发现对于一些复杂的有状态的集群应用来说，StatefulSet还是不够通用和强大，所以后面又出现了Kubernetes Operator。 我们先说说StatefulSet。StatefulSet之前曾用过PetSet这个名称，很多人都知道，在IT世界里，有状态的应用被类比为宠物（Pet），无状态的应用则被类比为牛羊，每个宠物在主人那里都是\"唯一的存在\"，宠物生病了，我们是要花很多钱去治疗的，需要我们用心照料，而无差别的牛羊则没有这个待遇。总结下来，在有状态集群中一般有如下特殊共性。 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信。 集群的规模是比较固定的，集群规模不能随意变动。 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中，每个节点在重启后都需要使用原有的持久化数据。 集群中成员节点的启动顺序（以及关闭顺序）通常也是确定的。 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损 StatefulSet从本质上来说，可被看作Deployment/RC的一个特殊变种，它有如下特性。 StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设StatefulSet的名称为kafka，那么第1个Pod叫kafka-0，第2个叫kafka-1，以此类推。 StatefulSet控制的Pod副本的启停顺序是受控的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态。 StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷（为了保证数据安全）。 StatefulSet除了要与PV卷捆绑使用，以存储Pod的状态数据，还要与Headless Service配合使用，即在每个StatefulSet定义中都要声明它属于哪个Headless Service。StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod实例都创建了一个DNS域名，这个域名的格式 如下： ${podname}.${headless service name} StatefulSet的建模能力有限，面对复杂的有状态集群时显得力不从心，所以就有了后来的Kubernetes Operator框架和众多的Operator实现了。需要注意的是，Kubernetes Operator框架并不是面向普通用户的，而是面向Kubernetes平台开发者的。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:10:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"批处理应用 除了无状态服务、有状态集群、常见的第三种应用，还有批处理应用。批处理应用的特点是一个或多个进程处理一组数据（图像、文件、视频等），在这组数据都处理完成后，批处理任务自动结束。为了支持这类应用，Kubernetes引入了新的资源对象——Job。 Jobs控制器提供了两个控制并发数的参数：completions和parallelism，completions表示需要运行任务数的总数，parallelism表示并发运行的个数，例如设置parallelism为1，则会依次运行任务，在前面的任务运行后再运行后面的任务。Job所控制的Pod副本是短暂运行的，可以将其视为一组容器，其中的每个容器都仅运行一次。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与Deployment等副本控制器不同，Job生成的Pod副本是不能自动重启的，对应Pod副本的restartPolicy都被设置为Never，因此，当对应的Pod副本都执行完成时，相应的Job也就完成了控制使命。后来，Kubernetes增加了CronJob，可以周期性地执行某个任务。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:11:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"应用的配置问题 通过前面的学习，我们初步理解了三种应用建模的资源对象，总结如下。 无状态服务的建模：Deployment。 有状态集群的建模：StatefulSet。 批处理应用的建模：Job。 在进行应用建模时，应该如何解决应用需要在不同的环境中修改配置的问题呢？这就涉及ConfigMap和Secret两个对象。 ConfigMap顾名思义，就是保存配置项（key=value）的一个Map，如果你只是把它理解为编程语言中的一个Map，那就大错特错了。ConfigMap是分布式系统中\"配置中心\"的独特实现之一。我们知道，几乎所有应用都需要一个静态的配置文件来提供启动参数，当这个应用是一个分布式应用，有多个副本部署在不同的机器上时，配置文件的分发就成为一个让人头疼的问题，所以很多分布式系统都有一个配置中心组件，来解决这个问题。但配置中心通常会引入新的API，从而导致应用的耦合和侵入。 用户将配置文件的内容保存到ConfigMap中，文件名可作为key，value就是整个文件的内容，多个配置文件都可被放入同一个ConfigMap。 在建模用户应用时，在Pod里将ConfigMap定义为特殊的Volume进行挂载。在Pod被调度到某个具体Node上时，ConfigMap里的配置文件会被自动还原到本地目录下，然后映射到Pod里指定的配置目录下，这样用户的程序就可以无感知地读取配置了。 在ConfigMap的内容发生修改后，Kubernetes会自动重新获取ConfigMap的内容，并在目标节点上更新对应的文件。 接下来说说Secret。Secret也用于解决应用配置的问题，不过它解决的是对敏感信息的配置问题，比如数据库的用户名和密码、应用的数字证书、Token、SSH密钥及其他需要保密的敏感配置。对于这类敏感信息，我们可以创建一个Secret对象，然后被Pod引用。Secret中的数据要求以BASE64编码格式存放。注意，BASE64编码并不是加密的，在Kubernetes 1.7版本以后，Secret中的数据才可以以加密的形式进行保存，更加安全。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:12:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"应用的运维问题 最后说说与应用的自动运维相关的几个重要对象。 首先就是HPA（Horizontal Pod Autoscaler），如果我们用Deployment来控制Pod的副本数量，则可以通过手工运行kubectl scale命令来实现Pod扩容或缩容。如果仅仅到此为止，则显然不符合谷歌对Kubernetes的定位目标——自动化、智能化。在谷歌看来，分布式系统要能够根据当前负载的变化自动触发水平扩容或缩容，因为这一过程可能是频繁发生、不可预料的，所以采用手动控制的方式是不现实的，因此就有了后来的HPA这个高级功能。我们可以将HPA理解为Pod横向自动扩容，即自动控制Pod数量的增加或减少。通过追踪分析指定Deployment控制的所有目标Pod的负载变化情况，来确定是否需要有针对性地调整目标Pod的副本数量，这是HPA的实现原理。Kubernetes内置了基于Pod的CPU利用率进行自动扩缩容的机制，应用开发者也可以自定义度量指标如每秒请求数，来实现自定义的HPA功能。 存储类 存储类的资源对象主要包括Volume、Persistent Volume、PVC和StorageClass。 首先看看基础的存储类资源对象——Volume（存储卷） Volume是Pod中能够被多个容器访问的共享目录。Kubernetes中的Volume概念、用途和目的与Docker中的Volume比较类似，但二者不能等价。首先，Kubernetes中的Volume被定义在Pod上，被一个Pod里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume中的数据也不会丢失；最后，Kubernetes支持多种类型的Volume，例如GlusterFS、Ceph等分布式文件系统。Volume的使用也比较简单，在大多数情况下，我们先在Pod上声明一个Volume，然后在容器里引用该Volume并将其挂载（Mount）到容器里的某个目录下。举例来说，若我们要给之前的Tomcat Pod增加一个名为datavol的Volume，并将其挂载到容器的某个路径/mydata-data目录下，则只对Pod的定义文件做下修正即可。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:13:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"emptyDir 一个emptyDir是在Pod分配到Node时创建的。从它的名称就可以看出，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也被永久移除。emptyDir的一些用途如下。 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留。 长时间任务执行过程中使用的临时目录。 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）。 在默认情况下，emptyDir使用的是节点的存储介质，例如磁盘或者网络存储。还可以使用emptyDir.medium属性，把这个属性设置为\"Memory\"，就可以使用更快的基于内存的后端存储了。需要注意的是，这种情况下的emptyDir使用的内存会被计入容器的内存消耗，将受到资源限制和配额机制的管理。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:14:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"hostPath hostPath为在Pod上挂载宿主机上的文件或目录，通常可以用于以下几方面。 在容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统对其进行存储。 需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部的应用可以直接访问Docker的文件系统。 在使用这种类型的Volume时，需要注意以下几点。 在不同的Node上具有相同配置的Pod，可能会因为宿主机上的目录和文件不同，而导致对Volume上目录和文件的访问结果不一致。 如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:15:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"公有云Volume 公有云提供的Volume类型包括谷歌公有云提供的GCEPersistentDisk、亚马逊公有云提供的AWS Elastic Block Store（EBSVolume）等。当我们的Kubernetes集群运行在公有云上或者使用公有云厂家提供的Kubernetes集群时，就可以使用这类Volume。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:16:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"其他类型的Volume iscsi：将iSCSI存储设备上的目录挂载到Pod中。 nfs：将NFS Server上的目录挂载到Pod中。 glusterfs：将开源GlusterFS网络文件系统的目录挂载到Pod中。 rbd：将Ceph块设备共享存储（Rados Block Device）挂载到Pod中。 gitRepo：通过挂载一个空目录，并从Git库克隆（clone）一个git repository以供Pod使用。 configmap：将配置数据挂载为容器内的文件。 secret：将Secret数据挂载为容器内的文件。 安全类 安全始终是Kubernetes发展过程中的一个关键领域。 从本质上来说，Kubernetes可被看作一个多用户共享资源的资源管理系统，这里的资源主要是各种Kubernetes里的各类资源对象，比如Pod、Service、Deployment等。只有通过认证的用户才能通过Kubernetes的API Server查询、创建及维护相应的资源对象，理解这一点很关键。 Kubernetes里的用户有两类：我们开发的运行在Pod里的应用；普通用户，如典型的kubectl命令行工具，基本上由指定的运维人员（集群管理员）使用。在更多的情况下，我们开发的Pod应用需要通过API Server查询、创建及管理其他相关资源对象，所以这类用户才是Kubernetes的关键用户。为此，Kubernetes设计了Service Account这个特殊的资源对象，代表Pod应用的账号，为Pod提供必要的身份认证。在此基础上，Kubernetes进一步实现和完善了基于角色的访问控制权限系统——RBAC（Role-Based Access Control）。 在默认情况下，Kubernetes在每个命名空间中都会创建一个默认的名称为default的Service Account，因此Service Account是不能全局使用的，只能被它所在命名空间中的Pod使用。通过以下命令可以查看集群中的所有Service Account： sudo kubectl get sa -A Service Account是通过Secret来保存对应的用户（应用）身份凭证的，这些凭证信息有CA根证书数据（ca.crt）和签名后的Token信息（Token）。在Token信息中就包括了对应的Service Account的名称，因此API Server通过接收到的Token信息就能确定Service Account的身份。在默认情况下，用户创建一个Pod时，Pod会绑定对应命名空间中的default这个Service Account作为其\"公民身份证\"。当Pod里的容器被创建时，Kubernetes会把对应的Secret对象中的身份信息（ca.crt、Token等）持久化保存到容器里固定位置的本地文件中，因此当容器里的用户进程通过Kubernetes提供的客户端API去访问API Server时，这些API会自动读取这些身份信息文件，并将其附加到HTTPS请求中传递给API Server以完成身份认证逻辑。在身份认证通过以后，就涉及\"访问授权\"的问题，这就是RBAC要解决的问题了。 首先我们要学习的是Role这个资源对象，包括Role与ClusterRole两种类型的角色。角色定义了一组特定权限的规则，比如可以操作某类资源对象。局限于某个命名空间的角色由Role对象定义，作用于整个Kubernetes集群范围内的角色则通过ClusterRole对象定义。 在RoleBinding中使用subjects（目标主体）来表示要授权的对象，这是因为我们可以授权三类目标账号：Group（用户组）、User（某个具体用户）和Service Account（Pod应用所使用的账号）。 在安全领域，除了以上针对API Server访问安全相关的资源对象，还有一种特殊的资源对象——NetworkPolicy（网络策略），它是网络安全相关的资源对象，用于解决用户应用之间的网络隔离和授权问题。NetworkPolicy是一种关于Pod间相互通信，以及Pod与其他网络端点间相互通信的安全规则设定。 NetworkPolicy资源使用标签选择Pod，并定义选定Pod所允许的通信规则。在默认情况下，Pod间及Pod与其他网络端点间的访问是没有限制的，这假设了Kubernetes集群被一个厂商（公司/租户）独占，其中部署的应用都是相互可信的，无须相互防范。但是，如果存在多个厂商共同使用一个Kubernetes集群的情况，则特别是在公有云环境中，不同厂商的应用要相互隔离以增加安全性，这就可以通过NetworkPolicy来实现了。 ","date":"2021-06-26","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:17:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["linux"],"content":"文本处理三剑客之SED Stream EDitor, 流编辑器 sed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为\"模式空间\"（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。然后读入下行，执行下一个循环。如果没有使诸如‘D’的特殊命令，那会在两个循环之间清空模式空间，但不会清空保留空间。这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 功能：主要用来自动编辑一个或多个文件,简化对文件的反复操作,编写转换程序等 参考： http://www.gnu.org/software/sed/manual/sed.html 简单的说sed主要是用来编辑文件，那么可以从四个角度来学这个命令，增删查改 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:0:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"初体验 在行首添加一行 # sed '1i\\AAA' aa.txt 在行尾添加一行 # sed '$a\\AAA' aa.txt ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:1:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"基础 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"用法 用法： sed [option]... 'script' inputfile... 常用选项： -n 不输出模式空间内容到屏幕，即不自动打印 -e 多点编辑 -f /PATH/SCRIPT_FILE 从指定文件中读取编辑脚本 -r 支持使用扩展正则表达式 -i.bak 备份文件并原处编辑 script: ‘地址命令’ ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:1","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"地址定界 (1) 不给地址：对全文进行处理 (2) 单地址： #：指定的行，$：最后一行 /pattern/：被此处模式所能够匹配到的每一行 (3) 地址范围： #,# #,+# /pat1/,/pat2/ #,/pat1/ (4) ~：步进 1~2 奇数行 2~2 偶数行 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:2","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"编辑命令 命令 说明 d 删除模式空间匹配的行，并立即启用下一轮循环 p 打印当前模式空间内容，追加到默认输出之后 a [\\]text 在指定行后面追加文本，支持使用\\n实现多行追加 i [\\]text 在行前面插入文本 c [\\]text 替换行为单行或多行文本 w /path/file 保存模式匹配的行至指定文件 r /path/file 读取指定文件的文本至模式空间中匹配到的行后 = 为模式空间中的行打印行号 ! 模式空间中匹配行取反处理 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:3","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"查找替换 s/// 查找替换,支持使用其它分隔符，s@@@，s### 替换标记： g 行内全局替换 p 显示替换成功的行 w /PATH/FILE 将替换成功的行保存至文件中 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:4","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"示例 命令 介绍 cp /etc/passwd /tmp/passwd 样本文件 sed ‘2p’ /tmp/passwd 打印第二行 sed -n ‘2p’ /tmp/passwd 打印第二行静默输出第二行 sed -n ‘1,4p’ /tmp/passwd 静默输出一至四行 sed -n ‘/root/p’ /tmp/passwd 静默输出匹配包含root字符串的行 sed -n ‘2,/root/p’ /tmp/passwd 从2行开始，静默输出匹配包含root字符串的行 sed -n ‘/^$/=’ file 显示空行行号 sed -n -e ‘/^$/p’ -e ‘/^$/=’ file sed ‘/root/a\\superman’ /tmp/passwd 在匹配root字符串行后追加superman sed ‘/root/i\\superman’ /tmp/passwd 在匹配root字符串行前追加superman sed ‘/root/c\\superman’ /tmp/passwd 重写匹配root字符串的行 sed ‘/^$/d’ file 清空文件 sed ‘1,10d’ file 删除文件的1-10行 nl /tmp/passwd sed ‘2,5d’ nl /tmp/passwd sed ‘2a tea’ sed ’s/test/mytest/g’ example 文件替换 sed -n ’s/root/\u0026superman/p’ /tmp/passwd 在匹配的单词后添加内容 sed -n ’s/root/superman\u0026/p’ /tmp/passwd 在匹配的单词前添加内容 sed -e ’s/dog/cat/’ -e ’s/hi/lo/’ pets 多个正则匹配 sed –i ’s/dog/cat/g’ pets 替换生效 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:5","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"总结 添加使用append,insert命令，删除使用delete,修改使用copy,subsitute命令,查看print 结合-n 静默输出，可以覆盖百分之八十以上的场景 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:3:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"高级编辑命令 P： 打印模式空间开端至\\n内容，并追加到默认输出之前 h: 把模式空间中的内容覆盖至保持空间中 H：把模式空间中的内容追加至保持空间中 g: 从保持空间取出数据覆盖至模式空间 G：从保持空间取出内容追加至模式空间 x: 把模式空间中的内容与保持空间中的内容进行互换 n: 读取匹配到的行的下一行覆盖至模式空间 N：读取匹配到的行的下一行追加至模式空间 d: 删除模式空间中的行 D：如果模式空间包含换行符，则删除直到第一个换行符的模式空间中的文本，并不会读取新的输入行，而使用合成的模式空间重新启动循环。如果模式空间 不包含换行符，则会像发出d命令那样启动正常的新循环 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:4:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"练习 1、删除centos7系统/etc/grub2.cfg文件中所有以空白开头的行行首的空白字符 2、删除/etc/fstab文件中所有以#开头，后面至少跟一个空白字符的行的行首的#和空白字符 3、在centos6系统/root/install.log每一行行首增加#号 4、在/etc/fstab文件中不以#开头的行的行首增加#号 5、处理/etc/fstab路径,使用sed命令取出其目录名和基名 6、利用sed 取出ifconfig命令中本机的IPv4地址 7、统计centos安装光盘中Package目录下的所有rpm文件的以.分隔倒数第二个字段的重复次数 8、统计/etc/init.d/functions文件中每个单词的出现次数，并排序（用grep和sed两种方法分别实现） 9、将文本文件的n和n+1行合并为一行，n为奇数行 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:5:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"思维导图 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:6:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"参考 linux-sed-command sed常用命令 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:7:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"为什么使用Linux grep命令? Grep是一个非常有用的命令行工具，可以用来在文件中查找指定的文本模式。通过使用grep命令，你可以快速定位和提取包含特定模式的行，从而加快查找和处理文本数据的效率。这是一个在日常工作中经常需要使用的任务，因此grep命令在Linux中非常流行。 ","date":"2021-06-20","objectID":"/grep/:1:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"Linux grep命令是什么？ Grep是一个在Linux和其他类Unix系统上可用的命令行实用程序，用于搜索和匹配文本。它的名字来自于全局正则表达式（global regular expression print），它的主要功能是根据给定的模式搜索文件中的文本，并打印匹配的行。grep命令支持使用简单的文本模式或正则表达式进行搜索，并且可以通过命令选项进行进一步的定制。 ","date":"2021-06-20","objectID":"/grep/:2:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"如何使用Linux grep命令？ 要使用grep命令，在命令行中输入grep，后跟要搜索的模式和要搜索的文件的路径。下面是一些常用的grep命令选项和示例： 示例列表 命令 解释 grep -i “pattern” file.txt 忽略大小写进行搜索 grep -r “pattern” directory/ 递归搜索子目录 grep -E “pattern” file.txt 使用正则表达式进行搜索 grep -A 2 “pattern” file.txt 显示匹配模式之前或之后的文本行 grep -C 2 “pattern” file.txt 输出匹配模式的上下文行 grep -i “pattern” file.txt 忽略大小写 grep -w “pattern” file.txt 精确匹配 grep -e hello -e world file.txt 多个关键词匹配 grep -v “pattern” file.txt 反向查找，不包含某个关键词的行 grep -lr “pattern” file.txt 递归匹配哪些文件名包含匹配的关键词 grep -o “pattern” file.txt 只输出匹配的内容 ","date":"2021-06-20","objectID":"/grep/:3:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"参考 是真的很详细了！Linux中的Grep命令使用实例 ","date":"2021-06-20","objectID":"/grep/:4:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["Kubernetes"],"content":"背景 通过 kubeadm 安装k8s集群报错 操作系统环境信息 $ cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"18.04.5 LTS (Bionic Beaver)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 18.04.5 LTS\" VERSION_ID=\"18.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic kubeadm init 安装报错信息 [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused. Unfortunately, an error has occurred: timed out waiting for the condition This error is likely caused by: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands: - 'systemctl status kubelet' - 'journalctl -xeu kubelet' Additionally, a control plane component may have crashed or exited when started by the container runtime. To troubleshoot, list all containers using your preferred container runtimes CLI. Here is one example how you may list all Kubernetes containers running in docker: - 'docker ps -a | grep kube | grep -v pause' Once you have found the failing container, you can inspect its logs with: - 'docker logs CONTAINERID' ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:1:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"排查思路 查看官网介绍为 docker 和 kubelet 服务中的 cgroup 驱动不一致，有两种方法 方式一：驱动向 docker 看齐 方式二：驱动为向 kubelet 看齐 如果docker 不方便重启则统一向 kubelet看齐，并重启对应的服务即可 ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:2:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"解决方式 ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:3:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"docker 配置文件 这里采取的是方式二，docker 默认驱动为 cgroupfs ,只需要添加 \"exec-opts\": [ \"native.cgroupdriver=systemd\" ], 修改后配置文件 $ cat /etc/docker/daemon.json { \"exec-opts\": [ \"native.cgroupdriver=systemd\" ], \"bip\":\"172.12.0.1/24\", \"registry-mirrors\": [ \"http://docker-registry-mirror.kodekloud.com\" ] } 重启docker systemctl restart docker ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:3:1","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"kublete 配置文件 grep 截取一下,可以看得出来kubelet默认 cgoup 驱动为systemd $ cat /var/lib/kubelet/config.yaml |grep group cgroupDriver: systemd 重启kubelet （optional） systemctl restart kubelet ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:3:2","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"参考 配置cgroup驱动 Docker中的Cgroup Driver:Cgroupfs 与 Systemd 为什么要修改docker的cgroup driver ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:4:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":null,"content":"友联 test ","date":"0001-01-01","objectID":"/friends/:0:0","tags":null,"title":"友链墙","uri":"/friends/"}]