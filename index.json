[{"categories":["安全"],"content":"前言 跨站请求伪造(Cross-Site Request Forgery, CSRF)攻击。它与XSS攻击的目标不同，但危害同样巨大，是Web安全的核心威胁之一。 ","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:1","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["安全"],"content":"CSRF攻击原理 CSRF攻击的核心在于欺骗用户的浏览器，让其以用户的身份在已认证（登录）的网站上执行攻击者指定的操作。 攻击者的目标： 攻击者诱使已经登录了目标网站（例如银行网站 bank.com）的用户，在用户不知情的情况下，向该目标网站发起一个恶意请求（例如转账给攻击者）。 用户已登录： 受害者用户之前已经成功登录了目标网站（bank.com），浏览器保存了该网站的会话Cookie（或其他认证凭证）。只要这个Cookie未过期，浏览器在向目标网站发送任何请求时都会自动带上它。 攻击者伪造请求： 攻击者精心构造一个指向目标网站关键功能（例如转账接口 bank.com/transfer）的HTTP请求。这个请求包含了执行恶意操作所需的所有参数（如收款账户、转账金额）。 关键点： 构造的请求可以是 GET（更容易，比如隐藏在图片链接或链接里）或 POST（常用，隐藏在表单里）请求。 诱骗用户触发请求： 攻击者需要设法让已经登录了目标网站的用户访问一个恶意页面（这个页面可以在攻击者控制的网站、论坛帖子、嵌入邮件的页面、甚至被XSS攻击的网站）。 当受害者访问这个恶意页面时，该页面会包含自动触发或诱使用户点击向目标网站发起伪造请求的代码。 请求被发送并执行： 用户的浏览器访问恶意页面，并按照页面指示（自动或用户触发）向目标网站 bank.com/transfer 发起伪造的请求。 由于用户之前已登录 bank.com，浏览器会自动将用户的 bank.com 会话Cookie添加到该请求头中。 bank.com 服务器收到这个请求：检查Cookie有效 -\u003e 验证用户身份通过 -\u003e 执行请求中的操作（转账）-\u003e 返回结果。 攻击完成： 攻击者在用户不知情、无意的情况下，以用户的身份和权限成功执行了恶意操作（如转账给攻击者账户）。受害者直到发现自己账户变动时才会察觉到攻击。 ","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:2","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["安全"],"content":"CSRF攻击的核心要素 受害者用户已登录目标网站A（身份认证有效）。 目标网站A的会话认证机制仅依赖于Cookie（或其他浏览器自动发送的信息）。 目标网站A存在关键操作接口（如转账、改密、改邮箱、发帖），该接口仅通过会话Cookie鉴别用户身份，对于请求的来源没有足够验证（只要Cookie对就执行）。 攻击者能够构造出该关键操作的完整HTTP请求（知道URL、参数格式）。 攻击者能诱使受害者用户在登录状态下访问包含能触发该请求的恶意页面。 ","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:3","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["安全"],"content":"常见的CSRF攻击实践（构造恶意请求的方式） GET请求伪装： 攻击者直接在恶意页面上嵌入指向目标接口的 GET 请求（通过 \u003cimg\u003e, \u003cscript\u003e, \u003ciframe\u003e 等标签的 src 属性）。 例子： 恶意页面中嵌入： \u003cimg src=\"http://bank.com/transfer?to=attacker_account\u0026amount=10000\" width=\"0\" height=\"0\" /\u003e 用户访问该恶意页面时，浏览器会尝试加载图片，自动向 bank.com 发送转账的 GET 请求（带上用户Cookie），完成攻击。攻击者账号收到10000元。 问题： GET 请求通常不应该用于修改数据的操作，但现在也能看到（危害更大）。 POST请求伪装： 攻击者在恶意页面中构造一个隐藏的表单 (\u003cform\u003e) ，表单的 action 指向目标接口地址（bank.com/transfer），method 为 POST，表单中包含恶意操作所需的参数（隐藏的输入框）。 使用JS在页面加载时自动提交表单 (form.submit())。 例子： \u003cbody onload=\"document.forms[0].submit()\"\u003e \u003cform action=\"http://bank.com/transfer\" method=\"POST\"\u003e \u003cinput type=\"hidden\" name=\"to\" value=\"attacker_account\" /\u003e \u003cinput type=\"hidden\" name=\"amount\" value=\"10000\" /\u003e \u003cinput type=\"submit\" value=\"Click for Prize!\" /\u003e \u003c!-- 也可完全隐藏，靠onload提交 --\u003e \u003c/form\u003e \u003c/body\u003e 用户访问恶意页面时，表单被自动提交（即使提交按钮是诱骗性的或隐藏的），浏览器向 bank.com 发送包含恶意参数和用户Cookie的 POST 请求。 利用其他标签/方法： 除了图片和表单，攻击者还可以利用 \u003clink\u003e (rel=stylesheet 或 prerender/prefetch)、\u003cvideo\u003e、\u003cobject\u003e、XMLHttpRequest、fetch()（如果允许跨域）等方法来发送伪造请求。 ","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:4","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["安全"],"content":"CSRF攻击的后果 资金损失： 伪造银行转账操作。 账户被接管： 伪造修改密码、修改密保邮箱操作。 数据泄露/篡改： 伪造用户资料修改、删除重要数据操作。 非授权操作： 伪造发送消息、发布帖子、投票、购物车结算等用户权限范围内的任何操作。 声誉损害： 受害者在不知情的情况下进行的恶意操作可能损害其个人或公司的声誉。 ","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:5","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["安全"],"content":"CSRF攻击的防范措施（核心思想：增加请求的不可预测性） CSRF Tokens（最常用、最有效的方案）： 原理： 为每个用户会话生成一个唯一的、随机的、高强度的Token（令牌）。将这个Token包含在： 服务端生成的表单中（作为隐藏字段 \u003cinput type=\"hidden\" name=\"csrf_token\" value=\"random_value\"\u003e）。 AJAX请求的请求头中（X-CSRF-Token: random_value）。 服务器验证： 在处理任何会改变状态的请求（POST, PUT, DELETE, PATCH）时，服务器检查请求中是否携带了有效的CSRF Token，并且该Token与当前用户会话中存储的Token一致。 安全性分析： 攻击者无法在自己的恶意页面中预先得知这个Token（因为Token是随机的且关联用户会话）。 同源策略（Same-Origin Policy）阻止了恶意页面通过JavaScript从合法页面读取Token（因为恶意页面和目标网站不同源）。 因此，攻击者无法在伪造的请求中包含正确的Token，服务器会拒绝执行请求。 实现要点： Token应绑定到用户会话（session），并在用户登录后生成。 Token应具有足够的熵（长度和随机性）以防猜测。 Token应在表单提交后立即失效（防止表单重复提交）或者在一个会话期内有效（后者更常见，但风险稍高），登出后应销毁。 使用POST方法保护携带Token的请求（避免Token泄漏在URL上）。 双重提交Cookie验证： 原理： 用户登录后，服务器除了设置会话Cookie（Set-Cookie: SessionId=...; ...），还会在客户端设置一个单独的CSRF Token Cookie（通常不能标记为 HttpOnly），如 Set-Cookie: CSRF-Token=random_value; ...。 客户端在发起敏感请求（通常是非 GET）时，除了浏览器自动在 Cookie 头中带上会话Cookie和CSRF-Token Cookie，还需要在请求头（如 X-CSRF-Token）或者请求体（如参数 csrf_token）中也带上这个CSRF Token值。 服务器在验证请求时： 从请求头或请求体中取出CSRF Token值（Token值）。 从请求头中的Cookie里取出CSRF Token Cookie值。 比较这两个Token值是否一致。一致则执行操作。 安全性分析： 攻击者可以通过构造恶意请求让浏览器带上合法的会话Cookie（因为浏览器自动做），但无法让浏览器带上合法的CSRF Token Cookie发送到攻击者控制的服务器（攻击者拿不到Cookie）。 攻击者在伪造请求的 X-CSRF-Token 头或请求体中放入什么值？他不知道用户真实的CSRF Token值（因为Cookie不可被恶意JS读取），也无法猜测。 因此，服务器收到的伪造请求中，请求头/体中的Token值和Cookie中的Token值不一致（甚至缺失），验证失败。 优缺点： 相比单纯使用Token在表单中，此方法JS更容易获取Token（从Cookie读取）。需要额外注意防止子域泄露问题（设置Cookie的 Domain 和 Path 属性严格控制范围）。现代框架常用此方式结合Token使用。 检查 Origin / Referer HTTP请求头： 原理： 服务器在处理敏感请求时，检查HTTP请求头 Origin（更可靠）或 Referer（存在隐私问题和可被篡改风险，但旧浏览器兼容好）。 Origin 头： 表示请求发起的源（协议+域名+端口）。不能由JS设置。浏览器在跨域请求和同站 POST 请求中通常会发送。 Referer 头： 表示请求来源的完整页面URL。可能会被一些防火墙或浏览器插件屏蔽或隐私保护策略移除（空），也可能被篡改（不安全）。 服务器验证： 如果请求头中包含 Origin，服务器检查它是否是预期的合法源（一个或多个白名单源，通常是网站自己的源地址）。如果请求头中没有 Origin（旧浏览器或简单请求如GET），可以检查 Referer（如果存在），并确保其来源域是合法的。 优缺点： 实现简单。 Origin 头比 Referer 可靠且安全。 不能完全依赖作为唯一防线。Referer 头可能被禁用/篡改/为空。在特定配置的嵌套页面（如 file: 协议或HTTPS跳HTTP）下可能存在问题。攻击者在某些中间人攻击（如Wifi热点）或浏览器插件漏洞场景下可能绕过。 建议： 作为深度防御措施，与CSRF Token等方法结合使用，增加攻击难度。 要求自定义请求头（配合CORS）： 原理： 服务端要求处理敏感操作的请求（通常是 POST, PUT, DELETE 等）必须携带一个特定的、非标准的安全自定义HTTP请求头（如 X-Requested-With: XMLHttpRequest 或自定义的 X-Custom-Header: Value）。 攻击者构造的CSRF攻击通常使用 \u003cform\u003e 提交，浏览器默认不会（也无法）自动设置自定义的请求头（只会在同源请求中自动设置Cookie）。 CORS策略限制： 如果服务器设置了严格的CORS（跨域资源共享）策略，只允许特定的源（自身）发起请求，那么恶意页面发起的跨域AJAX请求： 需要预检请求 (OPTIONS)。 由于请求携带了自定义头（即使AJAX设置了X-Requested-With等头），这个预检请求会被浏览器发送以检查是否被目标服务器允许。 关键： 除非目标服务器的CORS策略明确允许了攻击者恶意网站的源访问并携带该自定义头，否则预检请求会被服务器拒绝。浏览器就不会发送后续真正的敏感请求。即使恶意网站绕过AJAX使用\u003cform\u003e，也无法添加自定义头。 因此，服务器收到没有这个自定义请求头的请求（例如通过 \u003cform\u003e 提交的CSRF攻击请求），可以拒绝处理。 优缺点： 实现相对简单。 利用浏览器安全机制。 依赖于CORS策略的有效配置，需要严格限制允许的源（Access-Control-Allow-Origin）、允许的自定义头（Access-Control-Allow-Headers）。错误的CORS配置会引入其他严重漏洞（如泄露API数据）。 只对XMLHttpRequest / fetch()请求有效，无法防御通过\u003cform\u003e提交的简单CSRF攻击（但\u003cform\u003e提交无法携带自定义头，服务器会拒绝此类没带头的请求）。 对于使用JS库发起JSONP请求的老旧应用无效。 建议： 对于主要使用API（AJAX）进行交互的应用，可以作为CSRF Token的有力补充。对于传统多表单提交的应用，效果受限。 SameSite Cookie属性： 原理： 在设置Cookie时（特别是会话Cookie），添加 SameSite 属性来控制Cookie在跨站请求时是否被发送。 SameSite=Strict： 最严格。浏览器只会在同站请求(即请求来源和目标网站同源)中发送此Cookie。这意味着用户点击外部链接进入目标网站时，最开始访问的页面是没有会话Cookie的（相当于用户还没登录状态），需要重新登录，然后再执行操作（如从邮件中的链接进入银行）。安全但用户体验差。 SameSite=Lax： 推荐的平衡方案（现代浏览器默认行为）。 在跨站请求中，对于“安全的HTTP方法”（如 GET, HEAD），且该请求会导致浏览器在顶层导航栏切换页面（如 \u003ca\u003e 链接点击），浏览器会发送Cookie。但对于非安全的请求（如 POST, PUT, DELETE）、或通过 \u003cimg\u003e, \u003cscript\u003e, \u003cform\u003e（没有导致顶层导航改变）发起的跨站请求，浏览器则不会发送Cookie。这意味着攻击者伪造的 POST 转账请求不会携带用户的 SameSite=Lax 会话Cookie，服务器认为用户未登录（或会话无效），攻击失败。攻击者伪造的 GET 请求（如转账链接，设置Strict或Lax后）虽然可能发送Cookie，但要求是顶层导航跳转（用户会看到页面跳转到银行并转账成功页面，更容易察觉），并且安全方法通常不用来做数据修改操作（银行也不该用GET做转账）。 SameSite=None; Secure： 最宽松。浏览器会在跨站请求中发送Cookie，但必须标记为 Secure（要求HTTPS）。只有特殊场景（如跨多个域名的登录系统，嵌入的第三方iframe内容需要认证）才需要。容易受到CSRF攻击。 优缺点： 浏览器内置支持，服务端仅需在设置Cookie时添加属性（如 Set-Cookie: SessionId=...; Secure; HttpOnly; SameSite=Lax; ...）。 实现成本最低。 SameSite=Lax 能有效防御绝大多数基于 \u003cform\u003e POST的CSRF攻击，且不影响用户正常浏览体验（点击链接登录状态有效）。 SameSite=Strict 防御最强但体验差。 存在一些兼容性问题（旧版本浏览器支持度有限），但现在主流浏览器支持良好。 不能作为唯一防御措施。 浏览器兼容性问题、应用本身存在低危CSRF漏洞（如用GET做状态修改）、以及 SameSite=None 的设置不当都会削弱效果。 建议： 强烈推荐为所有会话Cookie设置 Secure; HttpOnly; SameSite=Lax（或更严格的Strict）。 作为基础防御层，并与其他措施（如CSRF","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:6","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["安全"],"content":"实践防御建议总结 黄金标准： 核心防御：对所有敏感操作（非 GET）强制使用 CSRF Tokens + 验证（包括表单和AJAX）。 基础加固：为会话Cookie设置 Secure, HttpOnly, SameSite=Lax（或Strict）。 深度防御： 启用合理的CORS策略，并考虑对于API请求要求使用自定义安全头 (配合CORS)。 检查关键请求的 Origin 头（作为补充验证）。 避免使用 GET 请求进行任何有状态或敏感的操作修改（应使用 POST, PUT, DELETE）。 实施最小权限原则。 使用现代安全框架，它们通常内置CSRF防护（如Django的CSRF中间件，Spring Security的CSRF防护）。 用户体验与安全的平衡： SameSite=Lax 通常是会话Cookie的最佳设置。 通过综合运用这些措施，尤其是Token + SameSite=Lax，可以非常有效地抵御绝大多数的CSRF攻击，保护用户账户和数据的安全。安全是一个多层次的过程，纵深防御是关键。 ","date":"2025-03-13","objectID":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/:0:7","tags":["安全","Web安全"],"title":"跨站请求伪造CSRF攻击的原理以及防范措施","uri":"/%E8%B7%A8%E7%AB%99%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0csrf%E6%94%BB%E5%87%BB%E7%9A%84%E5%8E%9F%E7%90%86%E4%BB%A5%E5%8F%8A%E9%98%B2%E8%8C%83%E6%8E%AA%E6%96%BD/"},{"categories":["OpenSSH"],"content":"背景 Linux操作系统环境：CentOS 7.x 前言\r由于安全扫描不同主机的 OpenSSH 版本不一，有些版本存在安全漏洞问题，现将 OpenSSH 统一升级至v9.9p2 版本，这里记录下整个操作的过程。 如果 OpenSSL 相对版本较低，这里建议先将OpenSS版本升级至OpenSSL 1.1.1w 11 Sep 2023，然后再升级 OpenSSH v9.9p2,如果你的主机当前的OpenSSH 版本为v9.9p1，则同样建议升级至v9.9p2，因为v9.9p1 含有两个中危漏洞，OpenSSH 安全漏洞(CVE-2025-26465 ) 和 OpenSSH 资源管理错误漏洞(CVE-2025-26466 )，此漏洞同样在 v9.9p2 版本进行了修复。 ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:1:0","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"相关安装包 地址\ropenssh下载地址：http://ftp.openbsd.org/pub/OpenBSD/OpenSSH/portable/ openssl下载地址：https://www.openssl.org/source/ zlib下载地址：http://www.zlib.net/ openssl-1.1.1w: https://www.openssl.org/source/openssl-1.1.1w.tar.gz\r","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:2:0","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"升级 OpenSSL 查找openssl相关目录，然后备份 $ whereis openssl #openssl: /usr/bin/openssl /usr/lib64/openssl /usr/include/openssl /usr/share/man/man1/openssl.1ssl.gz $ cp -rp /usr/bin/openssl /usr/bin/openssl`date '+%Y%m%d'` $ cp -rp /usr/lib64/openssl /usr/lib64/openssl`date '+%Y%m%d'` $ cp -rp /usr/include/openssl /usr/include/openssl`date '+%Y%m%d'` ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:3:0","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"卸载旧版本 OpenSSL(可选) $ yum -y remove openssl ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:3:1","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"安装 OpenSSL v1.1.1w $ wget --inet4-only https://www.openssl.org/source/openssl-1.1.1w.tar.gz $ tar -xzvf openssl-1.1.1w.tar.gz $ cd openssl-1.1.1w/ #目录选择/usr，因为系统最初始的openssl的目录就是/usr，这样可以省去软连接、更新链接库的问题 $ ./config --prefix=/usr #如果不加prefix ,openssl的默认路径如下 #Bin： /usr/local/bin/openssl #include库 ：/usr/local/include/openssl #lib库：/usr/local/lib64/ #engine库：/usr/lib64/openssl/engines #编译安装,这两步相对会耗费点时间 $ make $ make install ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:3:2","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"验证 $ whereis openssl openssl: /usr/bin/openssl /usr/include/openssl /usr/local/openssl /usr/share/man/man1/openssl.1ssl.gz /usr/share/man/man1/openssl.1ossl3.gz /usr/share/man/man1/openssl.1 $ openssl version OpenSSL 1.1.1w 11 Sep 2023 ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:3:3","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"升级 OpenSSH 至 v9.9p2 ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:0","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"查看当前版本 不同环境的ssh 版本会有所不同 $ ssh -V OpenSSH_8.0p1, OpenSSL 1.1.1k FIPS 25 Mar 2021 ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:1","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"升级前先备份 #通过whereis ssh sshd找出bin文件、源文件，然后备份(man手册不需要备份) $ whereis ssh sshd #ssh: /usr/bin/ssh /etc/ssh /usr/share/man/man1/ssh.1.gz #sshd: /usr/sbin/sshd /usr/share/man/man8/sshd.8.gz #cp -rp /etc/ssh /etc/ssh`date '+%Y%m%d%H%M%S'` $ cp -rp /etc/ssh /etc/ssh`date '+%Y%m%d'` $ cp -p /usr/bin/ssh /usr/bin/ssh`date '+%Y%m%d'` $ cp -p /usr/sbin/sshd /usr/sbin/sshd`date '+%Y%m%d%'` $ cp -p /etc/ssh/sshd_config /etc/ssh/sshd_config`date '+%Y%m%d'` #备份pam验证文件 $ cp -p /etc/pam.d/sshd /etc/pam.d/sshd`date '+%Y%m%d'` ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:2","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"卸载旧版OpenSSH yum -y remove openssh ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:3","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"OpenSSH 离线升级一键脚本（v9.9p2） vi upgrade_openssh.sh #!/bin/bash # OpenSSH 离线升级一键脚本（v9.9p2） # 检查执行权限 if [[ \"$(whoami)\" != \"root\" ]]; then echo -e \"\\033[31m错误：必须使用 root 用户执行此脚本！\\033[0m\" \u003e\u00262 exit 1 fi # 环境检查,可自行调整，并非CentOS Linux 7.x 皆可 check_environment() { echo -e \"\\n\\033[34m[1/7] 正在检查系统环境...\\033[0m\" if ! grep -q \"CentOS Linux release 7.9\" /etc/redhat-release; then echo -e \"\\033[31m错误：仅支持 CentOS 7 操作系统！\\033[0m\" # exit 1 fi if [ \"$(uname -m)\" != \"x86_64\" ]; then echo -e \"\\033[31m错误：仅支持 64 位系统！\\033[0m\" exit 1 fi echo -e \"\\033[32m环境检查通过\\033[0m\" } # 安装依赖包 install_dependencies() { echo -e \"\\n\\033[34m[3/7] 安装基础依赖...\\033[0m\" mkdir /opt/yilai cd /opt/yilai tar -xvf yilai.tar.gz cd yilai rpm -ivh *.rpm --nodeps --force echo -e \"\\033[32m依赖包安装完成\\033[0m\" } # 编译安装 zlib build_zlib() { echo -e \"\\n\\033[34m[4/7] 编译安装 zlib...\\033[0m\" cd /opt tar -xvf zlib-1.3.1.tar.gz cd zlib-1.3.1 ./configure --prefix=/usr/local/zlib make \u0026\u0026 make install echo '/usr/local/zlib/lib' \u003e\u003e /etc/ld.so.conf ldconfig -v } # 编译安装 OpenSSL build_openssl() { echo -e \"\\n\\033[34m[5/7] 编译安装 OpenSSL...\\033[0m\" cd /opt tar -xvf openssl-1.1.1o.tar.gz cd openssl-1.1.1o ./config --prefix=/usr/local/ssl -d shared make \u0026\u0026 make install echo'/usr/local/ssl/lib' \u003e\u003e /etc/ld.so.conf ldconfig -v } # 安装 OpenSSH install_openssh() { echo -e \"\\n\\033[34m[6/7] 升级 OpenSSH 到 v9.9p2...\\033[0m\" # 卸载旧版本 # rpm -e --nodeps openssh-server openssh openssh-clients 2\u003e/dev/null yum -y remove openssh # 编译安装 cd /opt wget --inet4-only https://ftp.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-9.9p2.tar.gz tar -xvf openssh-9.9p2.tar.gz cd openssh-9.9p2 ./configure --prefix=/usr/local/openssh \\ --with-zlib=/usr/local/zlib \\ --with-ssl-dir=/usr/local/ssl \\ --without-openssl-header-check make \u0026\u0026 make install # 配置文件 #检查是否有PasswordAuthentication yes，若没有则需进行添加 cat /etc/ssh/sshd_config | grep PasswordAuthentication echo 'PermitRootLogin yes' \u003e\u003e /usr/local/openssh/etc/sshd_config echo 'PubkeyAuthentication yes' \u003e\u003e /usr/local/openssh/etc/sshd_config echo 'PasswordAuthentication yes' \u003e\u003e /usr/local/openssh/etc/sshd_config echo 'UsePAM yes' \u003e\u003e /usr/local/openssh/etc/sshd_config echo 'HostKeyAlgorithms ssh-rsa,ssh-dss ' \u003e\u003e /etc/ssh/sshd_config cp /usr/local/openssh/etc/sshd_config /etc/ssh/sshd_config # 还原备份的sshd_config(二选一) # /usr/bin/cp -fp /etc/ssh/sshd_config`date '+%Y%m%d'` /etc/ssh/sshd_config # 替换系统命令（修复关键点） if [ -f /usr/sbin/sshd ]; then mv /usr/sbin/sshd /usr/sbin/sshd.bak fi cp -f /usr/local/openssh/sbin/sshd /usr/sbin/sshd # 使用新编译的二进制文件 # 修复权限 chmod 755 /usr/sbin/sshd cp /usr/local/openssh/bin/ssh-keygen /usr/bin/ssh-keygen # 复制 ssh 命令 cp /usr/local/openssh/bin/ssh /usr/bin/ssh chmod 755 /usr/bin/ssh # 启动脚本 cp -p contrib/redhat/sshd.init /etc/init.d/sshd chmod +x /etc/init.d/sshd chkconfig --add sshd chkconfig sshd on } # 最终验证 final_check() { echo -e \"\\n\\033[34m[7/7] 执行最终检查...\\033[0m\" systemctl daemon-reload systemctl restart sshd ssh -V 2\u003e\u00261 | grep -q \"OpenSSH_9.9p2\" if [ $? -eq 0 ]; then echo -e \"\\033[32m升级成功！当前SSH版本：$(ssh -V 2\u003e\u00261)\\033[0m\" echo -e \"\\033[33m警告：请通过新SSH端口连接确认无误后，再关闭Telnet服务！\\033[0m\" else echo -e \"\\033[31m错误：升级失败，请检查日志！\\033[0m\" exit 1 fi } # 主执行流程,建议分步执行 main() { check_environment install_dependencies build_zlib build_openssl install_openssh final_check } # 执行主函数 main ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:4","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"执行 $ chmod +x upgrade_openssh.sh $ ./upgrade_openssh.sh ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:5","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["OpenSSH"],"content":"验证 $ ssh -V # 应显示 \"OpenSSH_9.9p2\" OpenSSH_9.9p2, OpenSSL 1.1.1j 16 Feb 2021 $ systemctl status sshd ","date":"2025-02-21","objectID":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/:4:6","tags":["OpenSSH"],"title":"OpenSSH v9.9p2源码离线一键升级","uri":"/openssh_v9.9p2%E6%BA%90%E7%A0%81%E7%A6%BB%E7%BA%BF%E4%B8%80%E9%94%AE%E5%8D%87%E7%BA%A7/"},{"categories":["GPU"],"content":"背景 部署通义千问大模型qwen2.5-7b-awq基础环境预安装 ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:1:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"服务器配置 操作环境：openEuler 22.03 (欧拉-LTS-SP1) cpu: 40c memory: 186G disk: 2T GPU物理机，显卡型号为NVIDIA Tesla T4 (Tesla T4 被广泛应用于云端推理服务、虚拟桌面基础设施(VDI)、视频转码等领域) 查看 GPU 型号 $ ls /proc/driver/nvidia/gpus/ 0000:3b:00.0 $ cat 0000\\:3b\\:00.0/information Model: Tesla T4 IRQ: 218 GPU UUID: GPU-e42994d9-5240-0163-fbf5-deb9d57a348c Video BIOS: 90.04.38.00.03 Bus Type: PCIe DMA Size: 47 bits DMA Mask: 0x7fffffffffff Bus Location: 0000:3b:00.0 Device Minor: 0 GPU Firmware: 570.124.06 GPU Excluded: No ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:2:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"检测主机是否为虚拟化环境 systemd-detect-virt 是一个专门用于检测虚拟化环境的工具。 systemd-detect-virt 如果是物理机，输出为： none 如果是虚拟机，输出可能是： kvm vmware virtualbox xen ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:2:1","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"1. 检查仓库源 # debian $ sudo apt install -y g++ gcc gcc-c++ make build-essential pciutils bzip2 dkms # redhat $ sudo yum install -y g++ gcc gcc-c++ make pciutils bzip2 dkms 查看是否已安装gcc和picutils # 验证系统是否有GCC编译环境 $ gcc --version $ lspci 查看内核版本和GPU $ uname -a # 查找与 NVIDIA 相关的硬件设备显卡 $ lspci | grep -i nvidia ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:3:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"内核版本检查 Redhat $ uname -r 3.10.0-1160.119.1.el7.x86_64 $ yum list installed|grep kernel kernel-headers-3.10.0-1160.108.1.el7.x86_64 kernel-devel-3.10.0-1160.71.1.el7.x86_64 kernel-3.10.0-1160.119.1.el7.x86_64 kernel-3.10.0-1160.el7.x86_64 kernel-tools-libs-3.10.0-1160.119.1.el7.x86_64 kernel-tools-3.10.0-1160.119.1.el7.x86_64 注意\r查看内核版本是否匹配，如果匹配则跳过重新安装，不匹配需要重新安装进行匹配\r在线安装 # optional可选 sudo yum remove kernel-devel-$(uname -r) kernel-headers-$(uname -r) # 安装 sudo yum install kernel-devel-$(uname -r) kernel-headers-$(uname -r) 离线安装下载centos源码包 devel地址：http://rpmfind.net/linux/rpm2html/search.php?query=kernel-devel headers地址：http://rpmfind.net/linux/rpm2html/search.php?query=kernel-headers 安装 $ rpm -ivh kernel-devel-3.10.0-1160.119.1.el7.x86_64.rpm --nodeps --force $ rpm -ivh kernel-rpm -ivh kernel-headers-3.10.0-1160.119.1.el7.x86_64.rpm --force ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:3:1","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"2. 检查显卡 查看显卡信息和驱动信息 # 查看主显卡是哪一个 # 为了查看nvidia 显卡，可以安装nvidia-detect,并运行此软件 # debian $ apt-get install nvidia-detect # Redhat $ lspci |grep -i vga 03:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 41) # 查找与 VGA（视频图形阵列）兼容的显示控制器的信息 $ lspci -s 03:00.0 -v 03:00.0 VGA compatible controller: ASPEED Technology, Inc. ASPEED Graphics Family (rev 41) (prog-if 00 [VGA controller]) DeviceName: Onboard IGD Subsystem: ASPEED Technology, Inc. ASPEED Graphics Family Flags: bus master, medium devsel, latency 0, IRQ 17, NUMA node 0 Memory at 98000000 (32-bit, non-prefetchable) [size=64M] Memory at 9c000000 (32-bit, non-prefetchable) [size=128K] I/O ports at 2000 [size=128] Expansion ROM at 000c0000 [virtual] [disabled] [size=128K] Capabilities: [40] Power Management version 3 Capabilities: [50] MSI: Enable- Count=1/2 Maskable- 64bit+ Kernel driver in use: ast Kernel modules: ast ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:4:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"3. 安装 安装先检查有没有 cuda 支持的GPU $ lspci | grep -i nvidia 3b:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1) ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:5:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"Nouveau开源图形驱动禁用 后面安装时的告警信息，提前预知 WARNING: The Nouveau kernel driver is currently in use by your system. This driver is incompatible with the NVIDIA driver，and must be disabled before proceeding. 警告：您的系统当前正在使用Nouveau内核驱动程序。此驱动程序与NVIDIA驱动程序不兼容，必须先禁用才能继续。 解释： Nouveau是由第三方为NVIDIA显卡提供的开源图形驱动，但它与NVIDIA官方提供的驱动可能不兼容，尤其在运行特定的游戏或应用程序时可能会出现性能问题。如果系统报告Nouveau驱动正在使用中，可能意味着你的系统没有使用NVIDIA官方提供的驱动，这可能会降低图形性能或者导致系统不稳定。 解决方法： 禁用Nouveau驱动： Nouveau 是一个开源的图形驱动程序，用于支持 NVIDIA 显卡在 Linux 系统上的使用。它是由社区开发的，旨在提供对 NVIDIA 显卡的开源驱动支持，以便在 Linux 系统上实现图形加速和其他相关功能。在安装专有的 NVIDIA 驱动之前，通常需要禁用 Nouveau 驱动 创建文件/etc/modprobe.d/blacklist-nouveau.conf，并添加以下内容： # 查看nouveau 是否被禁用，如果没有任何输出，则说明 Nouveau 已被成功禁用 $ lsmod | grep nouveau nouveau 2424832 0 mxm_wmi 16384 1 nouveau video 61440 1 nouveau ttm 118784 3 drm_vram_helper,drm_ttm_helper,nouveau drm_kms_helper 286720 5 drm_vram_helper,ast,nouveau drm 638976 7 drm_kms_helper,drm_vram_helper,ast,drm_ttm_helper,ttm,nouveau i2c_algo_bit 16384 3 igb,ast,nouveau wmi 36864 2 mxm_wmi,nouveau # 从未手动禁用 Nouveau 驱动程序（开源的 NVIDIA 显卡驱动），则该文件不会存在，直接添加配置内容即可 $ sudo vi /etc/modprobe.d/blacklist-nouveau.conf #将nvidiafb注释掉: #blacklist nvidiafb # 添加以下内容 blacklist nouveau options nouveau modeset=0 禁用Nouveau驱动后更新initramfs： 修改模块黑名单后，需要更新 initramfs 以确保更改生效 # debian sudo update-initramfs -u # redhat sudo dracut --force 备份当前的镜像并建立新的镜像 #备份当前的镜像 $ sudo mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak #建立新的镜像 $ sudo dracut /boot/initramfs-$(uname -r).img $(uname -r) # 重启,不重启 Nouveau 模块可能仍然被加载到内核中，影响nvidia 驱动的安装 $ sudo reboot #最后输入上面的命令验证 $ lsmod | grep nouveau 要卸载nouveau驱动并清除相关配置文件，请执行以下命令：(可选) 禁用掉即可 # debian sudo apt-get remove --purge nouveau # redhat sudo yum t remove --purge nouveau ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:5:1","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"安装Nvidia驱动 GPU 云服务器正常工作需安装正确的基础设施软件，对 NVIDIA 系列 GPU 而言，有两个层次的软件包需要安装： 驱动 GPU 工作的硬件驱动程序。 上层应用程序所需要的库。 若把 NVIDIA GPU 用作通用计算，需要安装 GeForce Driver + CUDA。 处理 Secure Boot（可选） 若系统启用 Secure Boot，需为 NVIDIA 内核模块签名或禁用 Secure Boot（需进入 BIOS 设置）。 禁用 Secure Boot 会降低系统对恶意内核代码的防护能力，但允许加载未签名的驱动（如 NVIDIA 驱动） 方法1. 检查是否启用 Secure Boot # 安装 mokutil（如果未安装） $ sudo yum install mokutil -y # 查看 Secure Boot 状态 $ sudo mokutil --sb-state SecureBoot disabled Platform is in Setup Mode 输出结果： SecureBoot enabled 表示已启用。 SecureBoot disabled 表示已禁用。 方法2.检查 UEFI 变量文件 # 确认系统使用 UEFI 启动（非传统 BIOS） ls /sys/firmware/efi # 查看 Secure Boot 状态 cat /sys/firmware/efi/efivars/SecureBoot-* 2\u003e/dev/null 输出 1 表示启用，0 表示禁用。 禁用 Secure Boot Secure Boot 是 UEFI 固件的功能，禁用需进入主板的 BIOS/UEFI 设置界面。以下是通用步骤： 步骤 1：重启并进入 BIOS/UEFI 设置 重启计算机，在开机自检界面按下 特定按键（常见按键：F2、F10、Del、Esc，具体取决于主板型号）。 对于虚拟机（如 VMware/VirtualBox），需在启动时按 Esc 或 F2 进入设置。 ** 步骤 2：找到 Secure Boot 选项** 在 BIOS/UEFI 界面中，导航到 Security 或 Boot 选项卡。 查找 Secure Boot 选项（可能位于 Advanced 或 Authentication 子菜单中）。 步骤 3：禁用 Secure Boot 将 Secure Boot 设为 Disabled。 保存设置并退出（通常按 F10 或选择 Save \u0026 Exit）。 步骤 4：验证禁用结果 重启后回到 CentOS 7，再次运行检查命令（如 sudo mokutil --sb-state），确认输出为 SecureBoot disabled。 安装驱动后缀分为.run和.rpm 1.打开 NVIDIA 驱动下载链接 Advanced Driver Search | NVIDIA 。 2.选择支持 RPM 或者RUN的操作系统，并获取该包的下载链接。例如：选择 CentOS 7.x， 得到下载链接：Download NVIDIA, GeForce, Quadro, and Tesla Drivers 可选安装 安装NVIDIA官方驱动 关闭图形界面以避免冲突： sudo systemctl isolate multi-user.target 运行安装脚本： 确定你的显卡型号。 从NVIDIA官网下载对应显卡的驱动程序。 安装驱动请查看以下文章： $ chmod +x NVIDIA-Linux-x86_64-(版本号).run $ sudo ./NVIDIA-Linux-x86_64-570.124.06.run 1: Multiple kernel module types are available for this system. Which would you like to use? - 选择 NVIDIA Proprietary 2. Building kernel modules 进度条... 3. WARNING: nvidia-installer was forced to guess the X library path '/usr/lib64' and X module path '/usr/lib64/xorg/modules'; these paths were not queryable from the system. If X fails to find the NVIDIA X driver module, please install the `pkg-config` utility and the X.Org SDK/development package for your distribution and reinstall the driver. 警告可以忽略 4. Install NVIDIA's 32-bit compatibility libraries? 根据需要自行选择,这里选择兼容 5. Would you like to register the kernel module sources with DKMS? This will allow DKMS to automatically build a new module, if your kernel changes later. 选择YES 6. Registering the kernel modules with DKMS: 进度条... 7. Installation of the NVIDIA Accelerated Graphics Driver for Linux-x86_64 (version: 570.124.06) is now complete. 安装完成 按提示操作，选择以下选项： Yes 安装 DKMS（需提前安装 dkms 包）。前面步骤已经提前预安装 Yes 覆盖已有的 Nouveau 配置。 Yes 安装 32 位兼容库（如需）。 备注： DKMS: DKMS（Dynamic Kernel Module Support）是一个框架，用于在内核更新时自动重新编译和安装内核模块。 Would you like to register the kernel module sources with DKMS? This will allow DKMS to automatically build a new module when a new kernel is installed. 这是询问您是否希望将 NVIDIA 驱动模块注册到 DKMS（Dynamic Kernel Module Support）中。 如果选择“是”，DKMS 将自动为未来的内核版本重新编译和安装 NVIDIA 驱动模块。 X.Org 是否修改 X.Org 配置文件 Would you like to run the nvidia-xconfig utility to automatically update your X configuration file so that the NVIDIA X driver is used when you restart X? 如果您计划使用图形界面，建议选择“是”。这将确保 X Server 启动时使用 NVIDIA 驱动。 32位兼容库 如果您在一个 64 位系统上安装驱动，安装程序可能会询问是否安装 32 位兼容库： Install NVIDIA's 32-bit compatibility libraries? 如果您需要运行 32 位应用程序（例如某些游戏或旧版软件），请选择“是”。 ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:5:2","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"安装报错 ERROR: Unable to find the kernel source tree for the currently running kernel. Please make sure you have installed the kernel source files for your kernel and that they are properly configured; on Red Hat Linux systems, for example, be sure you have the 'kernel-source' or 'kernel-devel' RPM installed. If you know the correct kernel source files are installed, you may specify the kernel source path with the '--kernel-source-path' command line option. 查看一下内核版本是否一致 ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:5:3","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"验证安装 重新启动系统 $ sudo reboot # 查看显卡信息，验证驱动是否已成功安装 # 检查主机 GPU 和 NVIDIA 驱动是否正常工作，返回 GPU 的详细信息（如型号、驱动版本、显存使用情况等），说明驱动安装成功 $ nvidia-smi # 显示 GPU 状态 Tue Mar 25 16:23:30 2025 +-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 570.124.06 Driver Version: 570.124.06 CUDA Version: 12.8 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 Tesla T4 Off | 00000000:3B:00.0 Off | 0 | | N/A 70C P0 32W / 70W | 1MiB / 15360MiB | 4% Default | | | | N/A | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+ # 检查 NVIDIA 模块是否加载 $ lsmod | grep nvidia # 确认模块已加载 # 查看 GPU 资源的使用情况 $ nvidia-smi pmon -s u -d 1 # gpu pid type sm mem enc dec jpg ofa command # Idx # C/G % % % % % % name 0 43043 C 0 0 - - - - funasr-wss-serv 0 43043 C 0 0 - - - - funasr-wss-serv 注意\r需要注意的是，第一次安装显卡驱动的话，是不用重启服务器的，后续更新驱动版本的话，则是需要的。但是建议第一次安装驱动之后，最好还是重启下，防止意外情况的出现和发生。\r","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:5:4","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"安装 CUDA（可选） CUDA (Compute Unified Device Architecture) 是显卡厂商 NVIDIA 推出的运算平台。 CUDA™ 是一种由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。 它包含了 CUDA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。 开发人员现在可以使用 C 语言, C++ , FORTRAN 来为 CUDA™ 架构编写程序，所编写出的程序可以在支持 CUDA™ 的处理器上以超高性能运行。 GPU 云服务器采用 NVIDIA 显卡，需要安装 CUDA 开发运行环境。 1.CUDA驱动下载 2.选择操作系统和安装包，以CentOS 7为例 Installation Instructions: `sudo rpm -i cuda-repo-rhel7-7-5-local-7.5-18.x86_64.rpm` `sudo yum clean all` `sudo yum install cuda` 在/usr/local/cuda/samples/1_Utilities/deviceQuery目录下，执行make命令，可以编译出deviceQuery程序。 $ cd /usr/local/cuda/samples/1_Utilities/deviceQuery $ make 执行deviceQuery正常显示设备信息，此刻认为CUDA安装正确。 注意\r如果使用rpm文件报错，则考虑使用run文件进行安装(runfile(local))\r# 下载run文件进行安装 sh cuda_*.run 建议最好不要使用GUI图形化界面操作，容易报错。 驱动程序和 CUDA 的关系： 可以把 驱动程序（Driver） 和 CUDA 想象成 “汽车引擎” 和 “赛车配件包” 的关系： 驱动程序（Driver）： 就像汽车的 引擎，它让 GPU 硬件能够被操作系统识别并基础运行（如显示画面、基础计算）。没有引擎，汽车根本无法启动。 CUDA： 则像一套 赛车专用配件包（如涡轮增压、高性能变速箱、赛用轮胎等），它提供了开发高性能计算程序所需的工具和接口（如并行计算库、编译器）。只有安装了 CUDA，你才能利用 GPU 的算力做深度学习、科学计算等复杂任务。 是否需要安装 CUDA？取决于你的需求 如果只用 GPU 显示图形（如日常办公、游戏）： 安装驱动程序就够了，不需要 CUDA。 如果要做 GPU 计算（如深度学习、科学计算）： 必须安装 CUDA，因为： 深度学习框架（PyTorch、TensorFlow）依赖 CUDA 调用 GPU 算力。 CUDA 提供了 nvcc 编译器、cuBLAS 等加速库，是开发 GPU 程序的基石。 驱动和 CUDA 的版本依赖关系 CUDA 版本依赖驱动程序版本： 每个 CUDA 版本需要 最低版本的驱动程序 支持（见 NVIDIA CUDA 文档）。例如： CUDA 12.x 需要驱动版本 ≥ 525.60.13 CUDA 11.x 需要驱动版本 ≥ 450.80.02 你安装的驱动版本 570.124.06 支持大部分 CUDA 版本（具体需查兼容性表）。 建议安装方式： 先安装驱动（已装好）。 再根据需求安装 CUDA Toolkit（例如通过 NVIDIA 官网或包管理器）。 如何安装 CUDA？ 查看驱动支持的 CUDA 版本： nvidia-smi # 输出顶部会显示支持的 CUDA 版本（例如：CUDA Version: 12.2） 从 NVIDIA 官网下载对应 CUDA Toolkit： CUDA Toolkit 下载页面 选择与驱动兼容的版本，按官方文档安装（通常用 runfile 或包管理器）。 验证安装： nvcc --version # 查看 CUDA 编译器版本 nvidia-smi # 确认驱动和 CUDA 版本兼容 总结 驱动程序：让 GPU 能被系统使用（基础运行）。 CUDA：让开发者能利用 GPU 做高性能计算（功能扩展）。 关系：驱动是“汽油”，CUDA 是“赛车改装套件”——想飙车？两者缺一不可！ ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:5:5","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"安装 NVIDIA container toolkit 配套工具 NVIDIA Container Toolkit介绍： NVIDIA Container Toolkit(容器工具包)使用户能够构建和运行 GPU 加速的容器，该工具包括一个容器运行时库和实用程序，用于自动配置容器以利用 NVIDIA GPU。 使用背景 在使用大模型的时候，很多情况需要经常更新模型或者使用平台，所以时常更新代码或环境。 但是若在实体机更新导致环境混乱，维护不便，所以使用 docker 环境进行更新。 使用 docker 运行模型的时候要让其支持 GPU 需要使用 nvidia-container-toolkit 工具。 NVIDIA Container Toolkit 为容器化 GPU 加速工作提供了高效且便捷的解决方案，是深度学习和科学计算等场景的核心工具之一。 核心功能 GPU 加速支持： 将主机的 NVIDIA GPU 映射到容器中，使得容器能够直接访问 GPU 资源。 轻松管理 CUDA 环境： 自动加载必要的 NVIDIA 驱动程序、CUDA 库和运行时，简化环境配置。 与容器平台集成： 支持 Docker 和其他容器运行时（如 containerd）。 灵活的 GPU 分配： 允许用户指定容器可以访问的 GPU 数量或特定 GPU。 组件结构 NVIDIA Container Runtime： Docker 运行时的扩展，用于在容器中加载 GPU 相关组件。 NVIDIA Container CLI： 提供了命令行工具 nvidia-container-cli，可以用于调试和管理 GPU 映射。 支持的库和工具： 自动为容器挂载 libcuda.so 和其他相关库，使其支持 GPU 加速。 前提条件 NVIDIA 驱动程序：确保主机已安装支持的 NVIDIA 驱动程序。 Docker：安装 Docker，版本需为 19.03 或更高。 检查nvidia-container-toolkit NVIDIA 容器工具包是否安装 # debian $ dpkg -l | grep nvidia-container-toolkit # redhat $ rpm -l | grep nvidia-container-toolkit # 添加 NVIDIA 容器运行时的官方存储库 curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo # 在线安装，在线安装异常可以尝试离线安装 sudo yum install -y nvidia-container-toolkit 离线下载安装 $ wget https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64/libnvidia-container1-1.14.6-1.x86_64.rpm $ wget https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64/libnvidia-container-tools-1.14.6-1.x86_64.rpm $ wget https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64/nvidia-container-toolkit-base-1.14.6-1.x86_64.rpm $ wget https://nvidia.github.io/libnvidia-container/stable/rpm/x86_64/nvidia-container-toolkit-1.14.6-1.x86_64.rpm #手动安装 RPM 包,如果您是手动安装 .rpm 文件，确保版本匹配后执行以下命令： $ sudo rpm -ivh libnvidia-container1-1.14.6-1.x86_64.rpm $ sudo rpm -ivh libnvidia-container-tools-1.14.6-1.x86_64.rpm $ sudo rpm -ivh nvidia-container-toolkit-base-1.14.6-1.x86_64.rpm $ sudo rpm -ivh nvidia-container-toolkit-1.14.6-1.x86_64.rpm 测试 $ nvidia-ctk --version NVIDIA Container Toolkit CLI version 1.14.6 commit: 5605d191332dcfeea802c4497360d60a65c7887e $ nvidia-container-cli --version cli-version: 1.14.6 lib-version: 1.14.6 build date: 2024-02-27T20:52+0000 build revision: d2eb0afe86f0b643e33624ee64f065dd60e952d4 build compiler: gcc 4.8.5 20150623 (Red Hat 4.8.5-44) build platform: x86_64 build flags: -D_GNU_SOURCE -D_FORTIFY_SOURCE=2 -DNDEBUG -std=gnu11 -O2 -g -fdata-sections -ffunction-sections -fplan9-extensions -fstack-protector -fno-strict-aliasing -fvisibility=hidden -Wall -Wextra -Wcast-align -Wpointer-arith -Wmissing-prototypes -Wnonnull -Wwrite-strings -Wlogical-op -Wformat=2 -Wmissing-format-attribute -Winit-self -Wshadow -Wstrict-prototypes -Wunreachable-code -Wconversion -Wsign-conversion -Wno-unknown-warning-option -Wno-format-extra-args -Wno-gnu-alignof-expression -Wl,-zrelro -Wl,-znow -Wl,-zdefs -Wl,--gc-sections # 查看nvidia-container-toolkit版本 $ nvidia-container-runtime --version NVIDIA Container Runtime version 1.14.6 commit: 5605d191332dcfeea802c4497360d60a65c7887e spec: 1.2.0 runc version 1.1.12 commit: v1.1.12-0-g51d5e94 spec: 1.0.2-dev go: go1.21.11 libseccomp: 2.5.3 $ nvidia-container-toolkit --version NVIDIA Container Runtime Hook version 1.14.6 commit: 5605d191332dcfeea802c4497360d60a65c7887e 注意\rdocker v20+ 以上用 nvidia-container-toolkit 代替 nvidia-docker2 nvidia-container-runtime现在叫 nvidia-container-toolkit ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:6:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"容器运行时配置 ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:7:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"Docker 配置 $ nvidia-ctk runtime configure --runtime=docker INFO[0000] Loading config from /etc/docker/daemon.json INFO[0000] Wrote updated config to /etc/docker/daemon.json INFO[0000] It is recommended that docker daemon be restarted. 这个命令实际上是对 docker 的配置文件/etc/docker/daemon.json进行修改，内容如下： # 新增配置： { ... \"storage-driver\": \"overlay2\", \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } ... } 修改后重启docker $ sudo systemctl daemon-reload $ systemctl restart docker containerd # 验证容器运行时 $ docker info | grep \"Runtimes\" Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux nvidia runc ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:7:1","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"在 docker-compose.yml 中添加支持 GPU 仅供参考 services: ollama: image: ollama/ollama:0.5.4 container_name: ${CONTAINER_NAME} restart: unless-stopped ports: - ${PANEL_APP_PORT_HTTP}:11434 networks: - 1panel-network tty: true privileged: true volumes: - ./data:/root/.ollama labels: createdBy: \"Apps\" # 添加 GPU 支持 deploy: resources: reservations: devices: - capabilities: [gpu] # 如果使用 NVIDIA Container Toolkit，可以添加以下环境变量 environment: NVIDIA_VISIBLE_DEVICES: all NVIDIA_DRIVER_CAPABILITIES: \"compute,utility\" networks: 1panel-network: external: true ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:7:2","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"Containerd 配置 # nvidia-ctk runtime configure --runtime=containerd 这个命令实际上是对 containerd 的配置文件 /etc/containerd/config.toml 进行修改，内容如下： version = 2 [plugins] [plugins.\"io.containerd.grpc.v1.cri\"] [plugins.\"io.containerd.grpc.v1.cri\".containerd] default_runtime_name = \"nvidia\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes] [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia] privileged_without_host_devices = false runtime_engine = \"\" runtime_root = \"\" runtime_type = \"io.containerd.runc.v2\" [plugins.\"io.containerd.grpc.v1.cri\".containerd.runtimes.nvidia.options] BinaryName = \"/usr/bin/nvidia-container-runtime\" ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:7:3","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"在线或者离线使用国外代理 # 在线 cd /etc/yum.repos.d/ wget https://mirrors.ustc.edu.cn/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo sed -i 's#nvidia.github.io#mirrors.ustc.edu.cn#g' nvidia-container-toolkit.repo yum makecache yum install -y nvidia-container-toolkit # 离线 $ cat \u003e\u003envidia-container-toolkit.repo\u003c\u003cEOF [nvidia-container-toolkit] name=nvidia-container-toolkit baseurl=https://mirrors.ustc.edu.cn/libnvidia-container/stable/rpm/\\$basearch repo_gpgcheck=1 gpgcheck=0 enabled=1 gpgkey=https://mirrors.ustc.edu.cn/libnvidia-container/gpgkey sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt [nvidia-container-toolkit-experimental] name=nvidia-container-toolkit-experimental baseurl=https://mirrors.ustc.edu.cn/libnvidia-container/experimental/rpm/$basearch repo_gpgcheck=1 gpgcheck=0 enabled=0 gpgkey=https://mirrors.ustc.edu.cn/libnvidia-container/gpgkey sslverify=1 sslcacert=/etc/pki/tls/certs/ca-bundle.crt EOF yum install -y nvidia-container-toolkit ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:7:4","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"nvidia docker, nvidia docker2, nvidia container toolkits区别 在docker容器中用GPU时，查阅了网上许多教程，教程之间概念模糊不清，相互矛盾，过时的教程和新的教程混杂在一起。主要原因是Nvidia为docker容器的支持发生了好几代变更，api发生了不少变化。 省流版总结 凡是使用了命令nvidia docker或者在docker中引入了–runtime=nvidia参数的都是过时教程，最新方法只需要下载nvidia-container-toolkits，在docker中引入–gpus参数即可。 nvidia docker(不推荐) nvidia docker是NVIDIA第一代支持docker容器内使用GPU资源的项目。运行时用nvidia-docker命令。 根据nvidia docker在github ( https://github.com/NVIDIA/nvidia-docker )上的描述，已经不再使用了 nvidia docker2(不推荐) nvidia docker2是NVIDIA第二代支持docker容器内使用GPU资源的项目。运行时用nvidia-docker命令，且需要指定参数–runtime=nvidia. 根据 github (https://github.com/NVIDIA/nvidia-docker#backward-compatibility) 的描述，一代和二代之间有如下兼容性。 Backward compatibility To help transitioning code from 1.0 to 2.0, a bash script is provided in /usr/bin/nvidia-docker for backward compatibility. It will automatically inject the --runtime=nvidia argument and convert NV_GPU to NVIDIA_VISIBLE_DEVICES. 也就是说，在二代中，既可以使用nvidia docker命令，这会自动引入参数–runtime=nvidia也可以使用docker命令，手动指定参数–runtime=nvidia nvidia-container-toolkits(推荐) 根据github这是最新的支持方案，如帖子描述，nvidia docker2 被Nvidia container toolkits取代，无需指定–runtime参数，只需要传递–gpus参数 ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:7:5","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"k8s 部署nvidia设备插件(扩展) $ kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/1.0.0-beta4/nvidia-device-plugin.yml # 或者安装 NVIDIA GPU Operator，适用K8S版本 1.23版本以上 # 添加 NVIDIA Helm repository $ helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \u0026\u0026 helm repo update $ helm install -n gpu-operator --create-namespace gpu-operator nvidia/gpu-operator \\ --set driver.enabled=false \\ --set dcgmExporter.enabled=true \\ --set migManager.enabled=true \\ --set driver.vgpu.enabled=true \\ --set migStrategy=mixed ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:8:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"k8s 调度GPU 任务测试 测试一gpu.yaml apiVersion: v1 kind: Pod metadata: name: tf-pod spec: containers: - name: tf-container image: tensorflow/tensorflow:2.14.0-gpu command: [ \"/bin/sh\" ] args: [ \"-c\", \"tail -f\" ] resources: limits: cpu: 200m memory: 512Mi nvidia.com/gpu: 1 # requesting 1 GPU 测试二 cat cuda-vectoradd.yaml apiVersion: v1 kind: Pod metadata: name: cuda-vectoradd spec: restartPolicy: OnFailure containers: - name: cuda-vectoradd image: \"nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04\" resources: limits: nvidia.com/gpu: 1 # requesting 1 GPU nodeSelector: accekerator: nvidia-rtx4090 测试三 $ docker run -d --gpus all--name pytorch swr.cn-south-1.myhuaweicloud.com/myj-prod-local-cluster-repository/pytorch:cuda11.8 docker exec -it pytorch bash 执行 python3,调用cuda 计算显卡 测试四 进pod 里面执行nvida-smi 是否有输出，有输出说明成功了 ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:8:1","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"排错答疑 问题\rcould not select device driver \"\" with capabilities: [[gpu]] 通常是由于 Docker 没有正确识别到 GPU，或者 NVIDIA Docker 配置不正确。可能是 nvidia-container-toolkit 工具包没有安装，docker配置文件中的默认运行时没有添加nvidia配置 Is there a difference between: nvidia-docker run and docker run --runtime=nvidia ? docker run --runtime=nvidia is only available since nvidia-docker v2. ","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:9:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["GPU"],"content":"参考 下载安装NVIDIA驱动程序 kernel 内核源码包 Installation Guide Linux :: CUDA Toolkit Documentation 这里查看受支持的Linux版本 NVIDIA Container Toolkit 介绍及安装 by Unbuntu 欧拉openEuler 22.03 (LTS-SP1)安装NVIDIA Driver Installing the NVIDIA Container Toolkit 官网 Openeuler v22.03 AI 大模型服务镜像使用指南 nvidia-container-toolkit github NVIDIA Container 运行时库 USTC 中科大官方镜像 CUDA 和GPU 驱动型兼容性 信息\r本文档所提供的指引和参考主要基于特定实践设备的操作经验。由于不同设备在硬件配置、软件版本、使用场景等方面可能存在差异，因此，当您在使用其他设备时，所遇到的问题可能与此文档所述有所不同。尽管如此，大部分设备的安装方法和基本步骤仍然保持相似。\r","date":"2025-01-11","objectID":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/:10:0","tags":["GPU"],"title":"Linux系统Nvidia驱动部署","uri":"/linux%E7%B3%BB%E7%BB%9Fnvidia%E9%A9%B1%E5%8A%A8%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"一、XtraDB Backup（Percona XtraBackup）介绍 MySQL冷备、mysqldump、MySQL热拷贝都无法实现对数据库进行增量备份。在实际生产环境中增量备份是非常实用的，如果数据大于50G或100G，存储空间足够的情况下，可以每天进行完整备份，如果每天产生的数据量较大，需要定制数据备份策略。例如每周实用完整备份，周一到周六实用增量备份。而Percona-Xtrabackup就是为了实现增量备份而出现的一款主流备份工具，xtrabakackup有2个工具，分别是xtrabakup、innobakupe。 XtraBackup 是由 Percona 开发的开源工具，专门用于 MySQL 及兼容数据库（如 MariaDB、Percona Server）的在线热备份。它支持 InnoDB/XtraDB 存储引擎的高效备份，具有以下核心特性： 非阻塞备份：备份过程中不阻塞数据库的读写操作。 增量备份：仅备份自上次备份以来更改的数据，节省时间和存储。 快速恢复：支持通过全量+增量备份链快速恢复数据。 备份压缩与加密：可选压缩（如 --compress）和加密功能，优化存储安全。 自动实现备份检验 开源，免费 ","date":"2024-12-17","objectID":"/xtrabackup/:1:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"XtraBackup 对备份 MySQL 数据表类型的支持 众所周知，Xtrabackup是一个对InnoDB做数据备份的工具，支持在线热备份（备份时不影响数据读写），是商业备份工具InnoDB Hotbackup的一个很好的替代品。但是在公司生产环境常用的版本还是MySQL v5.7 及之前的版本，也有一些业务升级到了MySQL 8.0版本，不同的业务使用的表类型不同，在使用Xtrabackup我们要了解它的适用范围以及会有哪些影响。 在 Percona XtraBackup 的历史版本中，2.4 版本 是最后一个支持同时备份 InnoDB 和 MyISAM 表的版本。 关键点： XtraBackup 2.4： 支持 InnoDB（包括 XtraDB）的热备份（不锁表）。 支持 MyISAM 的备份，但会短暂锁定表（FLUSH TABLES WITH READ LOCK）。 适用于 MySQL 5.1、5.5、5.6 和 5.7 (5.7.9 ~ 5.7.28)（部分版本）。 XtraBackup 8.0+： 从 XtraBackup 8.0 开始，Percona 移除了对 MyISAM 表的支持，仅专注于 InnoDB 和 Percona Server 的增强功能。 适用于 MySQL 8.0 及更高版本。 建议： 如果你的数据库包含 MyISAM 表，并且需要完整备份，请使用 XtraBackup 2.4。 如果只有 InnoDB 表，建议使用最新的 XtraBackup 8.0+ 以获得更好的性能和兼容性。 额外说明： XtraBackup 2.4 仍然可以备份 MyISAM 表，但备份期间会短暂锁表，可能影响写入操作。 对于纯 InnoDB 环境，推荐升级到 XtraBackup 8.0+ 以支持 MySQL 8.0 的新特性（如 redo log 格式变更）。 ","date":"2024-12-17","objectID":"/xtrabackup/:1:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"XtraBackup 工具介绍 xtrabackup工具文件组成: Xtrabackup2.2 版之前包括4个可执行文件: innobackupex: Perl 脚本 xtrabackup: C/C++，编译的二进制程序 xbcrypt: 加解密 xbstream: 支持并发写的流文件格式 xtrabackup 是用来备份 InnoDB 表的，不能备份非InnoDB 表，和 MySQLServer 没有交互 innobackupex 脚本用来备份非InnoDB 表，同时会调用 xtrabackup 命令来备份 InnoDB 表，还会和 MySQLServer 发送命令进行交互，如加全局读锁(FTWRL)、获取位点(SHOW SLAVESTATUS)等。即innobackupex是在 xtrabackup 之上做了一层封装实现的 xtrabackup的新版变化 xtrabackup版本升级到2.4后，相比之前的2.1有了比较大的变化:innobackupex 功能全部集成到 xtrabackup 里面，只有一个 binary程序，另外为了兼容考虑，innobackupex作为xtrabackup的软链接，即xtrabackup现在支持非Innodb表备份，并且Innobackupex在下一版本中移除，建议通过xtrabackup替换innobackupex ","date":"2024-12-17","objectID":"/xtrabackup/:1:2","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"xtrabackup备份流程图 xtrabackup备份流程图\r","date":"2024-12-17","objectID":"/xtrabackup/:1:3","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"二、XtraBackup 与 MySQL 的版本对应规则 ","date":"2024-12-17","objectID":"/xtrabackup/:2:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"1. 匹配规则 XtraBackup 版本 兼容的 MySQL 版本 说明 8.0.x MySQL 8.0、Percona Server 8.0 支持 MySQL 8.0 的新特性（如 REDO/UNDO 日志格式变更、数据字典优化） 2.4.x MySQL 5.6、5.7、Percona 5.7 支持 MySQL 5.6/5.7，不兼容 MySQL 8.0 2.3.x 及以下 MySQL 5.5 或更早版本 已逐渐淘汰 关键注意事项： MySQL 8.0 必须使用 XtraBackup 8.0+，否则备份会直接报错（例如 Unsupported redo log format）。 MySQL 5.7 建议使用 XtraBackup 2.4.x，部分 8.0 版本可能兼容但需验证。 Percona Server 需与 XtraBackup 版本对齐（如 Percona Server 5.7 → XtraBackup 2.4）。 xtrabackup-version-match-1\rxtrabackup-version-match-2\r","date":"2024-12-17","objectID":"/xtrabackup/:2:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"2. 版本升级与降级策略 1. MySQL 升级（如 5.7 → 8.0） 步骤： 升级 XtraBackup 到 8.0+。 创建新备份（旧备份可能无法直接恢复至 MySQL 8.0）。 2. XtraBackup 降级 步骤： # 卸载当前版本 sudo yum remove percona-xtrabackup-80 # 安装旧版本 sudo yum install percona-xtrabackup-24 ","date":"2024-12-17","objectID":"/xtrabackup/:2:2","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"三、安装 XtraBackup ","date":"2024-12-17","objectID":"/xtrabackup/:3:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"1. 安装 XtraBackup 1. 添加 Percona 官方仓库 避免使用默认仓库（可能版本过旧），优先通过 Percona 仓库安装： # 安装 Percona 仓库 sudo yum install https://repo.percona.com/yum/percona-release-latest.noarch.rpm # 启用仓库 sudo percona-release setup ps80 # 若需 MySQL 8.0 对应工具 # 或 sudo percona-release setup ps57 # 若需 MySQL 5.7 对应工具 2. 查看可安装的 XtraBackup 版本 sudo yum list percona-xtrabackup-* # 示例输出： # percona-xtrabackup-80.x86_64 8.0.33-28.1.el7 percona-release-x86_64 # percona-xtrabackup-24.x86_64 2.4.28-1.el7 percona-release-x86_64 3. 安装指定版本 根据 MySQL 版本选择对应的包： MySQL 8.0： sudo yum install percona-xtrabackup-80 MySQL 5.6/5.7： sudo yum install percona-xtrabackup-24 离线安装指定的版本 # In RPM-based distribution (like CentOS 7), you need to: sudo yum install cmake gcc gcc-c++ libaio libaio-devel bison ncurses-devel \\ libgcrypt-devel libcurl-devel libev-devel python-sphinx vim-common # 或者直接通过 yum 安装来解决依赖的问题 sudo yum install -y percona-xtrabackup-24-2.4.20-1.el7.x86_64.rpm sudo rpm -qa |grep xtrabackup percona-xtrabackup-24-2.4.20-1.el7.x86_64 4. 验证安装版本 # xtrabackup会自动读取mysql配置 xtrabackup --version # 输出示例： # xtrabackup version 2.4.20 based on MySQL server 5.7.26 Linux (x86_64) (revision id: c8b4056) 注意\rxtrabackup 和 mysql 数据库版本是有对应关系的\r","date":"2024-12-17","objectID":"/xtrabackup/:3:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"三、使用 XtraBackup 进行增量备份及还原 ","date":"2024-12-17","objectID":"/xtrabackup/:4:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"1. 全量备份 ","date":"2024-12-17","objectID":"/xtrabackup/:4:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"2. 执行全量备份 mkdir -p /path/to/full_backup xtrabackup --backup --target-dir=/path/to/full_backup \\ --user=\u003cmysql_user\u003e --password=\u003cmysql_password\u003e --target-dir：指定备份文件存储目录，目录不需要提前创建 备份完成后，输出提示 completed OK! 表示成功。 全量备份可以一周或者一天执行一次 查看备份信息 cat /path/to/full_backup/xtrabackup_checkpoints cat /path/to/full_backup/xtrabackup_info cat /path/to/full_backup/backup-my.cnf 2. 备份文件结构 全量备份：包含所有数据文件。 增量备份：仅含变化的数据页（文件后缀 .delta 和 .meta）。 ","date":"2024-12-17","objectID":"/xtrabackup/:4:2","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"2. 还原全量备份 还原前需要停掉MySQL 服务 确保MySQL 数据目录是清空状态 1. 预整理 预整理，目的是将未完成的事务进行回滚 xtrabackup --prepare --target-dir=/path/to/full_backup 注意\r适合单纯地进行全量备份的恢复，并且没有打算在其上应用任何增量备份\r2. 最终准备并恢复数据 xtrabackup --prepare --target-dir=/path/to/full_backup 停止 MySQL 服务，清空数据目录，将备份文件复制回数据目录： 3.复制到数据库目录 例如数据库主机宕机或者挂掉了，将备份的数据拷贝到新的主机或者修复的数据库主机目录下 xtrabackup --copy-back --target-dir=/path/to/full_backup 4.还原属性 修改所属者和所属组 chown -R mysql:mysql /var/lib/mysql /var/lib/mysql 是默认目录，可通过/etc/my.cnf来进行查看，如有修改动态调整 5.启动服务 systemctl start mysqld ","date":"2024-12-17","objectID":"/xtrabackup/:4:3","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"MySQL 增量备份 ","date":"2024-12-17","objectID":"/xtrabackup/:5:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"MySQL 增量备份（基于 xtrabackup） 1. 基于 LSN 的增量备份 XtraBackup 通过 LSN（Log Sequence Number）追踪数据页变化，增量备份需基于上一次全量或增量备份： # 第一次修改后增量备份 xtrabackup --backup --target-dir=/path/to/inc_backup_1 \\ --incremental-basedir=/path/to/full_backup \\ --user=\u003cmysql_user\u003e --password=\u003cmysql_password\u003e # 第二次修改后增量备份 xtrabackup --backup --target-dir=/path/to/inc_backup_2 \\ --incremental-basedir=/path/to/inc_backup_1 \\ --user=\u003cmysql_user\u003e --password=\u003cmysql_password\u003e --incremental-basedir：指向上次备份的目录（全量或增量）。 备份生成三个备份目录/path/to/{full_backup,inc_backup_1,inc_backup_2} ","date":"2024-12-17","objectID":"/xtrabackup/:5:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"MySQL 增量备份（基于 xtrabackup）还原 还原前需要停掉MySQL 服务 确保MySQL 数据目录是清空状态 1. 准备全量备份 预准备：确保数据一致，阻止回滚未完成的事务 xtrabackup --prepare --apply-log-only --target-dir=/path/to/full_backup –apply-log-only阻止回滚未完成的事务，它仅执行redo日志的应用而不执行undo操作。这意味着事务不会被回滚。 适用场景：当计划应用增量备份（Incremental Backup）到全量备份时，需要先使用这个命令来准备全量备份。这样做是为了确保后续的增量备份可以正确地应用到基础备份上。 也就是说你的恢复策略包括先做一次全量备份，然后在此基础上应用一个或多个增量备份，则首先需要使用第一个命令（带–apply-log-only选项）准备全量备份，之后按照顺序逐一准备并应用每个增量备份（同样使用–apply-log-only直到最后一个增量备份）。最后，使用（不带–apply-log-only选项）来完成准备过程，使得整个备份集（包括全量和所有增量部分）可以用于恢复。 2. 还原合并增量备份 合并增量备份到全量备份： # 合并第一次增量备份到完全备份 xtrabackup --prepare --apply-log-only --target-dir=/path/to/full_backup \\ --incremental-dir=/path/to/inc_backup_1 # 合并第二次增量备份到完全备份:最后一次还原不需要加--apply-log-only xtrabackup --prepare --target-dir=/path/to/full_backup \\ --incremental-dir=/path/to/inc_backup_2 3.复制到数据库目录 例如数据库主机宕机或者挂掉了，将备份的数据拷贝到新的主机或者修复的数据库主机目录下 xtrabackup --copy-back --target-dir=/path/to/full_backup 这里也可以使用 cp 命令进行复制 注意\r数据库目录必须为空，且MySQL服务不能启动\r4.还原属性 修改所属者和所属组 chown -R mysql:mysql /var/lib/mysql /var/lib/mysql 是默认目录，如有修改动态调整 5.启动服务 systemctl start mysqld ","date":"2024-12-17","objectID":"/xtrabackup/:5:2","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"MySQL 增量备份（基于 Binlog） ","date":"2024-12-17","objectID":"/xtrabackup/:5:3","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"1. 启用 Binlog 在 my.cnf 中配置： [mysqld] log_bin = /var/lib/mysql/mysql-bin server_id = 1 ","date":"2024-12-17","objectID":"/xtrabackup/:5:4","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"2. 定期备份 Binlog # 刷新日志，生成新 Binlog 文件 mysqladmin flush-logs # 备份旧的 Binlog 文件（如 mysql-bin.000001） cp /var/lib/mysql/mysql-bin.000001 /backup/ ","date":"2024-12-17","objectID":"/xtrabackup/:5:5","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"3. 使用 Binlog 恢复数据 # 恢复到指定时间点 mysqlbinlog /backup/mysql-bin.000001 | mysql -u root -p ","date":"2024-12-17","objectID":"/xtrabackup/:5:6","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"五、方案对比与选择 备份方式 XtraBackup 增量备份 Binlog 增量备份 粒度 页面级（物理备份） SQL 语句级（逻辑备份） 恢复速度 快（直接文件替换） 慢（需重放 SQL） 适用场景 大型数据库快速恢复 精细恢复（如误删单表） 存储占用 中等（仅变化页） 较大（记录所有变更） ","date":"2024-12-17","objectID":"/xtrabackup/:6:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"六、 XtraBackup 2.4 常用参数 以下是 Percona XtraBackup 2.4 的常用参数解释表格，涵盖备份、恢复、压缩、加密等核心功能： ","date":"2024-12-17","objectID":"/xtrabackup/:7:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"XtraBackup 2.4 核心参数表 参数 类型 说明 默认值 示例 通用参数 --backup 命令 执行全量备份。 - xtrabackup --backup --target-dir=/backup --prepare 命令 准备备份文件（应用日志，使其可恢复）。 - xtrabackup --prepare --target-dir=/backup --copy-back 命令 将备份文件复制到 MySQL 数据目录。 - xtrabackup --copy-back --target-dir=/backup --target-dir 选项 指定备份文件的存储目录。 - --target-dir=/backup/full --datadir 选项 指定 MySQL 数据目录路径。 /var/lib/mysql --datadir=/data/mysql 增量备份 --incremental 选项 执行增量备份。 false --incremental --target-dir=/backup/inc1 --incremental-basedir 选项 指定增量备份的基础目录（上一次全量或增量备份的路径）。 - --incremental-basedir=/backup/full --incremental-lsn 选项 手动指定增量备份的起始 LSN（替代 --incremental-basedir）。 - --incremental-lsn=12345678 压缩与解压 --compress 选项 启用备份文件压缩（使用 QuickLZ 算法）。 false --compress --compress-threads 选项 指定压缩线程数（并行压缩）。 1 --compress --compress-threads=4 --decompress 选项 解压已压缩的备份文件（需在 --prepare 阶段使用）。 false xtrabackup --prepare --decompress --target-dir=/backup 加密 --encrypt 选项 启用备份文件加密（支持 AES128/AES192/AES256）。 false --encrypt=AES256 --encrypt-key 选项 直接指定加密密钥（不推荐，存在安全风险）。 - --encrypt-key=\"MySecretKey123\" --encrypt-key-file 选项 从文件读取加密密钥（推荐方式）。 - --encrypt-key-file=/path/to/keyfile --encrypt-threads 选项 指定加密线程数（并行加密）。 1 --encrypt=AES256 --encrypt-threads=4 并行处理 --parallel 选项 指定备份或恢复时的并行线程数（加速大文件操作）。 1 --parallel=4 --use-memory 选项 指定 --prepare 阶段可用的内存大小（提高处理速度）。 100MB --use-memory=2G 日志与监控 --log 选项 指定日志文件路径。 输出到标准错误流 --log=/var/log/xtrabackup.log --stream 选项 将备份流式传输到标准输出（通常结合压缩或加密使用）。 false --stream=xbstream | gzip \u003e backup.xbstream.gz --stats 选项 输出备份统计信息（需配合 --backup 使用）。 false --stats 高级选项 --slave-info 选项 备份从库时记录复制信息（用于构建新从库）。 false --slave-info --safe-slave-backup 选项 停止复制线程以确保备份一致性（从库备份时使用）。 false --safe-slave-backup --throttle 选项 限制备份时的 I/O 操作速率（单位：IOPS）。 0（无限制） --throttle=100 --version-check 选项 检查 Percona Server 或 MySQL 版本兼容性。 true --version-check=false ","date":"2024-12-17","objectID":"/xtrabackup/:7:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"关键参数使用示例 1. 全量备份 + 压缩 + 加密 xtrabackup --backup \\ --target-dir=/backup/full \\ --compress \\ --compress-threads=4 \\ --encrypt=AES256 \\ --encrypt-key-file=/etc/mysql/xtrabackup.key \\ --encrypt-threads=4 2. 增量备份 # 第一次增量（基于全量） xtrabackup --backup \\ --target-dir=/backup/inc1 \\ --incremental-basedir=/backup/full # 第二次增量（基于前一次增量） xtrabackup --backup \\ --target-dir=/backup/inc2 \\ --incremental-basedir=/backup/inc1 3. 准备备份并解压 # 解压并应用日志 xtrabackup --prepare \\ --target-dir=/backup/full \\ --decompress 补充 安装 qpress 解压缩 qpress 便携式高速文件归档器 当全量备份进行压缩后，在还原时解压需要解压时需要qpress命令 $ wget -d --user-agent=\"Mozilla/5.0 (Windows NT x.y; rv:10.0) Gecko/20100101 Firefox/10.0\" https://docs-tencentdb-1256569818.cos.ap-guangzhou.myqcloud.com/qpress-11-linux-x64.tar $ tar -xf qpress-11-linux-x64.tar -C /usr/local/bin $ source /etc/profile ","date":"2024-12-17","objectID":"/xtrabackup/:7:2","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"注意事项 加密密钥管理： 优先使用 --encrypt-key-file 替代 --encrypt-key，避免密钥泄露。 密钥文件需严格限制权限（如 chmod 600）。 并行与资源占用： 高并发（--parallel、--compress-threads）可能显著增加 CPU 和内存消耗。 生产环境中建议根据硬件资源调整。 版本兼容性： XtraBackup 2.4 支持 MySQL 5.1~5.7 和 Percona Server 对应版本。 确保备份工具与数据库版本匹配。 增量备份依赖链： 增量备份必须基于完整的全量备份链，恢复时需按顺序合并所有增量。 定期清理过时的备份链以节省存储空间。 通过合理组合这些参数，可以实现高效、安全的数据库备份与恢复操作。 ","date":"2024-12-17","objectID":"/xtrabackup/:7:3","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"七、生产环境数据库备份策略 ","date":"2024-12-17","objectID":"/xtrabackup/:8:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"一、备份频率建议 1. 全量备份频率 建议周期：每周 1 次全量备份 理由： 全量备份是恢复的基础，确保数据的完整性。 如果数据量较小（如 TB 级以下），建议每周执行一次全量备份。 如果数据量极大（如数十 TB），可延长至每 2 周一次，但需结合增量备份策略降低风险。 执行时间： 选择业务低峰期（如凌晨）以减少对 I/O 和 CPU 的影响。 2. 增量备份频率 建议周期：每日 1 次增量备份 理由： 增量备份仅捕获自上次全量或增量后的数据变化，资源占用低。 每日一次可平衡 RPO（恢复点目标）与存储成本。 特殊情况调整： 若数据变更频繁（如电商大促期间），可缩短至每小时一次增量备份。 若数据变更极少（如静态配置库），可放宽至每 2-3 天一次。 ","date":"2024-12-17","objectID":"/xtrabackup/:8:1","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"二、备份策略示例 周一：全量备份（base） 周二~周日：每日增量备份（inc1, inc2, ..., inc6） 下周一：新一轮全量备份 + 清理旧备份链 ","date":"2024-12-17","objectID":"/xtrabackup/:8:2","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"三、关键注意事项 1. 备份保留策略 保留至少 2-3 个全量备份链（如最近 2 周的备份），防止备份损坏或误删。 增量备份需与其依赖的全量备份共同保留，确保恢复链完整。 2. 恢复时间优化 增量备份链不宜过长（建议不超过 7 次增量），否则恢复时合并时间可能超出 RTO。 若增量链过长，可调整全量备份频率（如每 3 天一次全量）。 3. 结合 Binlog 实现精细化恢复 启用 MySQL Binlog，按需（如每 5-15 分钟）归档 Binlog 文件。 恢复时，先通过 XtraBackup 还原到最近备份点，再通过 Binlog 恢复到精确时间点。 4. 监控与验证 监控备份任务状态，确保无失败或遗漏。 定期演练恢复流程（如每季度一次），验证备份有效性。 ","date":"2024-12-17","objectID":"/xtrabackup/:8:3","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"四、典型场景调整建议 场景 全量备份频率 增量备份频率 备注 小型数据库（\u003c 100GB） 每周 1 次 每日 1 次 恢复速度快，资源压力小。 中型数据库（100GB~1TB） 每周 1 次 每日 1 次（或 12h/次） 增量频率可根据白天/夜间负载调整。 大型数据库（\u003e 1TB） 每周 1 次 每日 2-4 次 缩短增量间隔，避免单次增量过大。 超高写入负载数据库 每 3 天 1 次 每小时 1 次 通过更频繁全量备份减少增量链长度。 ","date":"2024-12-17","objectID":"/xtrabackup/:8:4","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"总结 核心原则：全量备份频率由数据量和恢复时间容忍度决定，增量备份频率由数据变更速度决定。 平衡点：在存储成本、备份资源开销、RTO/RPO 之间找到最佳平衡。 动态调整：根据业务周期（如节假日、促销）灵活调整备份策略。 ","date":"2024-12-17","objectID":"/xtrabackup/:8:5","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"八、关键注意事项 定期验证备份：通过恢复测试确保备份有效性。 监控备份任务：结合 cron 或工具（如 Rundeck）自动化备份。 备份保留策略：保留多个全量+增量备份点，避免单点故障。 混合使用策略：XtraBackup + Binlog 提供双重保障，支持时间点恢复（PITR）。 通过合理选择工具和策略，可确保 MySQL 数据的高效备份与可靠恢复。 ","date":"2024-12-17","objectID":"/xtrabackup/:9:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"九、最佳实践 始终检查版本兼容性：在升级 MySQL 或 XtraBackup 前，查阅 Percona 官方文档。 测试备份恢复流程：在非生产环境验证备份文件的可用性。 自动化版本管理：使用 Ansible/Puppet 等工具确保环境版本一致。 将被备份数据放到远程数据库主机下：提升安全性 ","date":"2024-12-17","objectID":"/xtrabackup/:10:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["MySQL"],"content":"参考 XtraDB Backup 官网 Percona Operator for MySQL based on Percona Server for MySQL XtraDB Backup 下载 github autoxtrabackup script gitee autoxtrabackup script ","date":"2024-12-17","objectID":"/xtrabackup/:11:0","tags":["MySQL","XtraBackup"],"title":"XtraDB Backup（Percona XtraBackup）备份MySQL详细介绍","uri":"/xtrabackup/"},{"categories":["linux"],"content":"iptables 是运维中的重点但不算是特别的难点，是网站访问的大门，本篇记录对 iptables 的常用梳理 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:0:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"概述 封端口 封IP 实现Nat上网 共享上网 端口映射(端口转发)，ip 映射 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:1:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"核心解释 iptables适用linux所有发行版，yum安装方式为 “yum install iptables-services” iptables基于linux netfilter, Netfilter是Linux 2.4.x引入的一个子系统，它作为一个通用的、抽象的框架，提供一整套的hook函数的管理机制，使得诸如数据包过滤、网络地址转换(NAT)和基于协议类型的连接跟踪成为了可能 linux 内核中，netfilter， nf_* 的配置都会影响netfilter的性能 iptables结构：iptables -\u003e Tables -\u003e Chains -\u003e Rules. 即tables由chains组成，而chains由rules组成 iptables 支持表类型有 filter （默认）， nat， mangle， raw ， 优先级 raw \u003e mangle \u003e nat \u003e filter iptables 的 nat 功能依赖linux内核的net.ipv4.ip_forward， net.ipv6.ip_forward iptables 的 nat 功能包含snat，dnat，其中snat仅用于postrouting MASQUERADE是SNAT的一个特例，主要用于动态返回snat地址 防火墙实例: (/etc/sysconfig/iptables） ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:2:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables和netfilter的关系 Iptables和netfilter的关系是一个很容易让人搞不清的问题。很多的知道iptables却不知道netfilter。其实iptables只是Linux防火墙的管理工具而已，位于/sbin/iptables。真正实现防火墙功能的是netfilter，它是Linux内核中实现包过滤的内部结构。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:3:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables 管理 安装了iptables-service后方便通过systemd管理iptables服务 $ sudo yum install iptables-services $ sudo rpm -ql iptables-services /etc/sysconfig/ip6tables /etc/sysconfig/iptables # 防火墙配置文件 /usr/lib/systemd/system/ip6tables.service /usr/lib/systemd/system/iptables.service # 防火墙配置文件(命令) systemctl start iptables ... $ sudo rpm -ql iptables ... /usr/sbin/ip6tables /usr/sbin/ip6tables-restore /usr/sbin/ip6tables-save /usr/sbin/iptables # iptables 命令 添加/删除/查看 规则(4表5链) /usr/sbin/iptables-restore # 恢复 /usr/sbin/iptables-save # iptables规则 输出(保存) /usr/sbin/xtables-multi ... 注意: 使用iptables 记得将firewalld 关闭掉systemctl disable firewalld 防火墙激活 防护墙是默认嵌入到linux内核中，但是没有默认激活,lsmod查看内核中加载了哪些模块 $ sudo lsmod|egrep \"iptables|nat|filter\" xt_nat 12681 4 nf_nat_masquerade_ipv4 13463 1 ipt_MASQUERADE iptable_nat 12875 1 nf_nat_ipv4 14115 1 iptable_nat nf_nat 26583 3 nf_nat_ipv4,xt_nat,nf_nat_masquerade_ipv4 nf_conntrack 143411 6 nf_nat,nf_nat_ipv4,xt_conntrack,nf_nat_masquerade_ipv4,nf_conntrack_netlink,nf_conntrack_ipv4 iptable_filter 12810 1 br_netfilter 22256 0 bridge 155432 1 br_netfilter ip_tables 27126 2 iptable_filter,iptable_nat libcrc32c 12644 3 xfs,nf_nat,nf_conntrack 临时加载和永久加载 # 1. 写入到开机启动 $ modprobe iptable_filter $ modprobe iptable_nat $ modprobe iptable_mangle $ modprobe iptable_raw $ modprobe nf_conntrack $ modprobe nf_conntrack_ipv4 $ modprobe nf_nat # 验证是否加载成功 $ lsmod | grep iptable $ lsmod | grep nf_conntrack # 2. 永久 $ cat \u003e /etc/modules-load.d/iptables.conf \u003c\u003cEOF iptable_filter iptable_nat iptable_mangle iptable_raw nf_conntrack nf_conntrack_ipv4 nf_nat EOF # 验证是否加载成功 $ sudo lsmod|egrep \"iptables|nat|filter\" 查看指定表中的规则 # 1. 默认是查看filter 表 $ iptables -nL # 2. 查看nat 表的规则 $ iptables -t nat -nL 示例 $ iptables -nL Chain INPUT (policy ACCEPT) # 链默认规则 target prot opt source destination ACCEPT tcp -- 192.168.47.0/24 0.0.0.0/0 tcp dpt:30181 # 规则 Chain FORWARD (policy ACCEPT) # 链默认规则 target prot opt source destination DOCKER-USER all -- 0.0.0.0/0 0.0.0.0/0 # 规则 Chain OUTPUT (policy ACCEPT) target prot opt source destination ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:4:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables传输数据包的过程 当一个数据包进入网卡时，它首先进入PREROUTING链，内核根据数据包目的IP判断是否需要转送出去。 如果数据包就是进入本机的，它就会沿着图向下移动，到达INPUT链。数据包到了INPUT链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包会经过OUTPUT链，然后到达POSTROUTING链输出。 如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过FORWARD链，然后到达POSTROUTING链输出。 iptables传输数据包的过程\r","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:5:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"4表-5链-规则 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:6:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"防火墙的种类及使用说明 硬件：整个企业入口 三层路由：H3C 华为 Cisco(思科) 防护墙：深信服 软件：开源软件 网站内部 封ip iptables 写入到Linux 内核中 以后服务docker 工作在4层 firewalld C7 nftables C8 ufw(ubuntu fire wall) Ubuntu 云防火墙(公有云) 阿里云 安全组(封ip,封端口) NAT网关(共享上网，端口映射) waf应用防火墙 waf防火墙(应用防火墙，处理7层的攻击)SQL注入 名词简要熟悉 容器 表：存放链的容器，防火墙最大概念 链：存放规则的容器 规则：准许或拒绝规则，未来写的防火墙条件就是各种防火墙规则 详细解释： 表（tables）提供特定的功能，iptables内置了4个表，即filter表、nat表、mangle表和raw表，分别用于实现包过滤，网络地址转换、包重构(修改)和数据跟踪处理。 链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以有一条或数条规则。当一个数据包到达一个链时，iptables就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则iptables将继续检查下一条规则，如果该数据包不符合链中任一条规则，iptables就会根据该链预先定义的默认策略来处理数据包。 Iptables采用“表”和“链”的分层结构。在REHL4中是三张表五个链。现在REHL5、6成了四张表五个链了，不过多出来的那个表用的也不太多，所以基本还是和以前一样。下面罗列一下这四张表和五个链。注意一定要明白这些表和链的关系及作用。 四表五链\r","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:6:1","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"规则表 filter表——三个链：INPUT、FORWARD、OUTPUT 作用：过滤数据包 内核模块：iptables_filter. Nat表——三个链：PREROUTING、POSTROUTING、OUTPUT 作用：用于网络地址转换（IP、端口） 内核模块：iptable_nat Mangle表——五个链：PREROUTING、POSTROUTING、INPUT、OUTPUT、FORWARD 作用：修改数据包的服务类型、TTL、并且可以配置路由实现QOS内核模块：iptable_mangle(别看这个表这么麻烦，咱们设置策略时几乎都不会用到它) Raw表——两个链：OUTPUT、PREROUTING 作用：决定数据包是否被状态跟踪机制处理 内核模块：iptable_raw 备注：常用的表有filter、Nat这两个表 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:6:2","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"规则链 INPUT——进来的数据包应用此规则链中的策略 OUTPUT——外出的数据包应用此规则链中的策略 FORWARD——转发数据包时应用此规则链中的策略 PREROUTING——对数据包作路由选择前应用此链中的规则 （记住！所有的数据包进来的时侯都先由这个链处理） POSTROUTING——对数据包作路由选择后应用此链中的规则 （所有的数据包出来的时侯都先由这个链处理） 备注：常用的规则链有INPUT、OUTPUT、FORWARD ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:6:3","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"四表五链详解 iptables 是 Linux 系统中用于配置防火墙规则的核心工具，基于 Netfilter 框架实现。它的核心逻辑围绕 四表五链 展开，用于控制网络数据包的传输和处理流程。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:7:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"一、四表（Tables） 四表是规则的分类容器，每个表负责不同的功能： filter 表（默认表） 功能：数据包的过滤（允许/拒绝）。 应用场景：控制访问权限（如禁止某个 IP 访问）。 关联链：INPUT、FORWARD、OUTPUT。 nat 表（Network Address Translation） 功能：网络地址转换（修改源/目标 IP 或端口）。 应用场景：端口转发（DNAT）、共享上网（SNAT）。 关联链：PREROUTING（DNAT）、POSTROUTING（SNAT）、OUTPUT。 mangle 表 功能：修改数据包内容（如 TTL、TOS 字段）或标记数据包。 应用场景：高级流量控制（如 QoS 标记）。 关联链：所有五链（PREROUTING、INPUT、FORWARD、OUTPUT、POSTROUTING）。 raw 表 功能：绕过连接跟踪（Connection Tracking）。 应用场景：提高性能（如禁用某些连接的跟踪）。 关联链：PREROUTING、OUTPUT。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:7:1","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"二、五链（Chains） 五链是数据包在传输过程中经过的“检查点”，决定了规则生效的时机： PREROUTING 触发时机：数据包进入网卡后，路由决策之前。 常用表：nat（DNAT）、mangle、raw。 INPUT 触发时机：数据包目标是本机（如访问本地服务）。 常用表：filter（允许/拒绝）、mangle。 FORWARD 触发时机：数据包需要转发到其他设备（如路由器）。 常用表：filter（是否允许转发）、mangle。 OUTPUT 触发时机：本机生成的出站数据包（如本机发起的请求）。 常用表：filter、nat、mangle、raw。 POSTROUTING 触发时机：数据包离开网卡前，路由决策之后。 常用表：nat（SNAT）、mangle。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:7:2","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"三、表和链的对应关系 表名 支持的链 filter INPUT, FORWARD, OUTPUT nat PREROUTING, OUTPUT, POSTROUTING mangle 所有五链（PREROUTING, INPUT, FORWARD, OUTPUT, POSTROUTING） raw PREROUTING, OUTPUT ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:7:3","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"四、处理优先级 表的优先级：raw → mangle → nat → filter。 （例如：PREROUTING 链中，先处理 raw 表规则，再处理 mangle 表，最后是 nat 表。） 链的流程： 入站流量：PREROUTING → INPUT 转发流量：PREROUTING → FORWARD → POSTROUTING 出站流量：OUTPUT → POSTROUTING ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:7:4","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"五、常见操作示例 禁止某个 IP 访问本机（filter 表 + INPUT 链）： iptables -A INPUT -s 192.168.1.100 -j DROP 将 80 端口转发到 8080（nat 表 + PREROUTING 链）： iptables -t nat -A PREROUTING -p tcp --dport 80 -j REDIRECT --to-port 8080 修改出站数据包的 TTL（mangle 表 + OUTPUT 链）： iptables -t mangle -A OUTPUT -j TTL --ttl-set 64 通过理解四表五链的作用和流程，可以更精准地控制网络行为，例如实现防火墙、NAT、流量标记等功能。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:7:5","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables 执行流程 工作流程小结： 防火墙是层层过滤的，实际是按照配置规则的顺序从上到下，从前到后进行过滤的。 如果匹配成功规则，即明确表示是拒绝(DROP)还是接受(ACCEPT),数据包就不再向下匹配新的规则 如果规则中没有明确表明是阻止还是通过的，也就是没有匹配规则，向下进行匹配，直到匹配默认规则得到明确的阻止还是通过 防火墙的默认规则是所有规则都匹配完才会匹配的 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:8:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables filter 表案例 # 屏蔽单个IP的命令 $ iptables -I INPUT -s 223.70.232.41 -j DROP # 查看防火墙规则信息 $ iptables -nvL --line-numbers $ sudo iptables-save \u003e /etc/sysconfig/iptables $ sudo iptables-restore \u003c /etc/sysconfig/iptables # 改动之前先备份，养成好习惯 $ cp /etc/sysconfig/iptables /etc/sysconfig/iptables.bak $ iptables -F #清空防火墙规则 # 只是拒绝某个ip 的访问不需要加协议 $ iptables -I INPUT 1 -s 192.168.143.102 -j DROP # 拒绝网段访问本机 $ iptables -I INPUT 1 -s 192.168.143.0/24 -j DROP # 感叹号取反，注意不要自己跳板机的 ip 也给屏蔽掉了，否则 xshell 无法连接 $ iptables -I INPUT 1 ! -s 192.168.143.0/24 -j DROP # 主机对172.21.29.116 ip 开放22 端口权限，加端口则需要加协议 $ iptables -I INPUT 1 -s 172.21.29.116 -p tcp --dport 22 -j ACCEPT # 禁止网段访问指定的端口 $ iptables -I INPUT 1 -s 192.168.143.0/24 -p tcp --dport 8080 -j ACCEPT # 禁止网段访问指定的多个端口 $ iptables -I INPUT 1 -s 192.168.143.0/24 -p tcp -m multiport --dports 8080,80,9090 -j ACCEPT # 禁止网段访问指定的端口范围 $ iptables -I INPUT 1 -s 192.168.143.0/24 -p tcp --dports 1000:2000 -j ACCEPT # 删除第一条规则 $ iptables -D INPUT 1 # 主机 禁止ping icmp $ iptables -I INPUT -p icmp --icmp-type 8 -j DROP # 通过防护墙控制连接状态 $ iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT $ iptables -A OUTPUT -m state --state ESTABLISHED,RELATED -j ACCEPT ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:9:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"实践：限制 ip 连接数 限制来自任何IP地址的TCP连接到端口22（SSH默认端口），如果来自同一IP的并发连接数超过100，则拒绝这些连接，并使用ICMP端口不可达消息作为响应 $ iptables -A INPUT -p tcp --dport 22 -m connlimit --connlimit-above 100 --connlimit-mask 32 -j REJECT --reject-with icmp-port-unreachable $ iptables -nL --line-numbers Chain INPUT (policy ACCEPT) num target prot opt source destination 1 REJECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:22 #conn src/32 \u003e 100 reject-with icmp-port-unreachable ... 命令参数释义： -A INPUT: 将规则添加到INPUT链中。INPUT链处理所有入站数据包。 -p tcp: 指定协议为TCP。 --dport 22: 目标端口是22，即SSH服务的默认端口。 -m connlimit: 使用connlimit模块来限制每个客户端IP的并发连接数。 --connlimit-above 100: 如果一个源IP地址发起的并发连接数超过100，则应用此规则。 --connlimit-mask 32: 对于IPv4，这表示针对单个IP地址进行限制（子网掩码长度为32意味着每个独立的IP地址）。 -j REJECT: 匹配条件后执行的目标动作是REJECT（拒绝）。 --reject-with icmp-port-unreachable: 当拒绝数据包时，发送ICMP端口不可达的消息作为响应。 这条规则将会对尝试连接到服务器上SSH服务（端口22）的所有流量生效，若某个IP地址的并发连接数超过了100，则新的连接请求将被拒绝，并返回ICMP端口不可达的消息给发起者。这样可以有效防止单一来源的过多连接请求，有助于防范一些类型的Dos攻击或滥用行为。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:9:1","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"生产配置参考 # 1. 放开22 端口访问 $ iptables -A INPUT -p tcp --dport 22 -j ACCEPT # 2. 回环端口放开 $ iptables -A INPUT -i lo -j ACCEPT $ iptables -A OUTPUT -o lo -j ACCEPT # 3. 放开常用端口 $ iptables -A INPUT -p tcp -m multiport --dports 80,443 -j ACCEPT # 4. 指定网段的访问 $ iptables -A INPUT -s 192.168.143.0/24 -j ACCEPT $ iptables -A INPUT -s 10.168.143.0/24 -j ACCEPT # 5. 连接状态 $ iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 6. # 拒绝所有其他入站流量 $ iptables -P INPUT DROP # 汇总 $ iptables-save ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:9:2","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables 常用命令参数 iptables 是 Linux 系统下管理防火墙规则的核心工具，其参数用于定义规则的匹配条件、操作目标和规则管理。以下是常用参数分类及详细说明： ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"一、规则管理参数 参数 说明 -A \u003c链名\u003e 在指定链的末尾添加规则（Append）。 示例：iptables -A INPUT -s 192.168.1.0/24 -j ACCEPT -I \u003c链名\u003e [规则编号] 在指定链的指定位置插入规则（Insert），默认插入到链首。 示例：iptables -I INPUT 2 -p tcp --dport 80 -j DROP -D \u003c链名\u003e [规则编号] 删除指定链中的某条规则（Delete）。 示例：iptables -D INPUT 3 -F [链名] 清空指定链的所有规则（Flush），不指定链则清空所有链。 示例：iptables -F INPUT -L [链名] 列出指定链的规则（List），不指定链则列出所有链。 示例：iptables -L INPUT -nv -N \u003c链名\u003e 创建自定义链（New）。 示例：iptables -N MY_CHAIN -X \u003c链名\u003e 删除自定义空链（Delete chain）。 示例：iptables -X MY_CHAIN -P \u003c链名\u003e \u003c动作\u003e 设置链的默认策略（Policy），如 ACCEPT 或 DROP。 示例：iptables -P INPUT DROP ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:1","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"二、匹配条件参数 参数 说明 -s \u003cIP/网段\u003e 匹配源IP地址。 示例：-s 192.168.1.100 或 -s 10.0.0.0/24 -d \u003cIP/网段\u003e 匹配目标IP地址。 -p \u003c协议\u003e 匹配协议类型（如 tcp, udp, icmp, all）。 示例：-p tcp --sport \u003c端口\u003e 匹配源端口（需配合 -p tcp 或 -p udp）。 示例：--sport 8080 --dport \u003c端口\u003e 匹配目标端口。 示例：--dport 22 -i \u003c网卡\u003e 匹配输入网卡（如 eth0）。 示例：-i eth0 -o \u003c网卡\u003e 匹配输出网卡。 -m \u003c模块\u003e 使用扩展模块（如 state, multiport, iprange）。 示例：-m state --state ESTABLISHED --state \u003c状态\u003e 匹配连接状态（如 NEW, ESTABLISHED, RELATED）。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:2","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"三、动作参数（-j 参数指定） 动作 说明 ACCEPT 允许数据包通过。 DROP 丢弃数据包（无响应）。 REJECT 拒绝数据包（返回错误响应）。 LOG 记录日志到系统日志（如 /var/log/messages）。 示例：-j LOG --log-prefix \"Blocked: \" SNAT 源地址转换（用于 nat 表的 POSTROUTING 链）。 示例：-j SNAT --to-source 1.2.3.4 DNAT 目标地址转换（用于 nat 表的 PREROUTING 链）。 示例：-j DNAT --to-destination 192.168.1.100:8080 MASQUERADE 动态源地址转换（适用于动态 IP，如拨号网络）。 示例：-j MASQUERADE REDIRECT 端口重定向（常用于透明代理）。 示例：-j REDIRECT --to-port 8080 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:3","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"四、其他常用参数 参数 说明 -t \u003c表名\u003e 指定操作的表（默认 filter）。 示例：iptables -t nat -L -v 显示详细信息（如数据包计数）。 示例：iptables -L -v -n 不解析域名（直接显示 IP 地址）。 示例：iptables -L -n --line-numbers 显示规则编号（便于删除或插入）。 示例：iptables -L INPUT --line-numbers ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:4","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"五、典型示例 1. 基础防火墙规则 # 允许本地回环接口 $ iptables -A INPUT -i lo -j ACCEPT # 允许已建立的连接 $ iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 允许 SSH 连接（端口 22） $ iptables -A INPUT -p tcp --dport 22 -j ACCEPT # 拒绝所有其他入站流量 $ iptables -P INPUT DROP 2. 端口转发（NAT） # 启用 IP 转发 $ echo 1 \u003e /proc/sys/net/ipv4/ip_forward # 将外部 80 端口转发到内网 192.168.1.100:8080 $ iptables -t nat -A PREROUTING -p tcp --dport 80 -j DNAT --to-destination 192.168.1.100:8080 $ iptables -t nat -A POSTROUTING -j MASQUERADE 3. 限速规则（使用 limit 模块） # 限制 ICMP 请求频率（每秒 1 个） $ iptables -A INPUT -p icmp -m limit --limit 1/s -j ACCEPT $ iptables -A INPUT -p icmp -j DROP ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:5","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"六、注意事项 规则顺序：规则按顺序匹配，一旦匹配成功，后续规则不再执行。 默认策略：设置默认策略为 DROP 前，需确保允许必要流量（如 SSH）。 持久化规则：重启后规则会丢失，需使用 iptables-save 和 iptables-restore 保存规则： iptables-save \u003e /etc/iptables/rules.v4 iptables-restore \u003c /etc/iptables/rules.v4 掌握这些参数后，可以灵活配置防火墙规则，实现流量过滤、NAT 转换、日志记录等功能。 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:10:6","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"iptables 总结 iptables 在工作中也是常用到的服务，重要但是不是很复杂，这里找了一个iptables面试参考，自己可以写一写，加深巩固下,在生产环境层面执行前最好是现在测试环境验证下 ","date":"2024-11-19","objectID":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/:11:0","tags":["linux","防火墙"],"title":"Linux 防火墙iptables详解","uri":"/linux%E9%98%B2%E7%81%AB%E5%A2%99iptables%E8%AF%A6%E8%A7%A3/"},{"categories":["MySQL"],"content":"背景信息 为了增加数据的冗余备份，增强服务的高可用性，从传统的单服务应用升级到集群服务应用是一个合适方案，这里介绍一下MySQL 5.7 版本的部署，5.7 版本也是目前大多数企业当前在用的版本，虽然也有很多企业升级到了8.0 版本，不过从安装部署的角度来看其实是大同小异的 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:1:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"环境准备 操作系统版本 cpu 内存 磁盘 IP 部署方式 MySQL版本 主从节点 centos 7.9 2c 4G 40G 192.168.143.101 rpm 5.7.20 Master centos 7.9 2c 4G 40G 192.168.143.102 rpm 5.7.20 Slave ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:2:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"安装MySQL 5.7 1、下载安装包 $ cd /path/to/directory $ wget https://downloads.mysql.com/archives/get/p/23/file/mysql-5.7.20-1.el7.x86_64.rpm-bundle.tar # 删除不需要的安装包 $ ls |grep -vE 'client|libs|common|server-5.7'|xargs rm -f 2、安装依赖 # mysql-community-common-5.7.20-1.el7.x86_64.rpm 依赖 $ sudo yum -y install gcc vim wget net-tools lrzsz libaio # mysql-community-client-5.7.20-1.el7.x86_64.rpm 依赖 $ sudo yum install ncurses-compat-libs -y # mysql-community-server-5.7.20-1.el7.x86_64.rpm 依赖 $ sudo yum install libaio numactl-libs -y 3、服务器时间校正 4、关闭防火墙和安全(主从) $ setenforce 0 $ systemctl stop firewalld $ systemctl disable firewalld 4.1 (可选)如果你的仓库中没有 ncurses-compat-libs，你可能需要添加一个额外的仓库，例如 EPEL (Extra Packages for Enterprise Linux)： $ sudo yum install epel-release -y $ sudo yum install ncurses-compat-libs -y 4.2 (可选) 检验系统是否包含mariadb-lib 检验系统是否包含mariadb-lib,如果有需要卸载，否则mysql 无法安装 rpm -qa | grep mariadb rpm -e --nodeps mariadb-libs-5.5.68-1.el7.x86_64 5、安装 $ sudo rpm -ivh mysql-community-common-5.7.20-1.el7.x86_64.rpm $ sudo rpm -ivh mysql-community-libs-5.7.20-1.el7.x86_64.rpm $ sudo rpm -ivh mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm $ sudo rpm -ivh mysql-community-client-5.7.20-1.el7.x86_64.rpm $ sudo rpm -ivh mysql-community-server-5.7.20-1.el7.x86_64.rpm $ rpm -qa|grep mysql mysql-community-common-5.7.20-1.el7.x86_64 mysql-community-libs-5.7.20-1.el7.x86_64 mysql-community-libs-compat-5.7.20-1.el7.x86_64 mysql-community-client-5.7.20-1.el7.x86_64 mysql-community-server-5.7.20-1.el7.x86_64 # 检查生成的mysql 目录和文件 $ ls /var/lib/mysql /etc/my.cnf /etc/my.cnf /var/lib/mysql: 6、安装过程摘取 # rpm -ivh mysql-community-server-5.7.20-1.el7.x86_64.rpm warning: mysql-community-server-5.7.20-1.el7.x86_64.rpm: Header V3 DSA/SHA1 Signature, key ID 5072e1f5: NOKEY Verifying... ################################# [100%] Preparing... ################################# [100%] Updating / installing... 1:mysql-community-server-5.7.20-1.e################################# [100%] /usr/lib/tmpfiles.d/mysql.conf:16: Line references path below legacy directory /var/run/, updating /var/run/mysqld → /run/mysqld; please update the tmpfiles.d/ drop-in file accordingly. 注意\r这个警告提示说明该 RPM 包未被 GPG 签名验证。如果你信任这个包的来源，可以忽略这个警告\r或者一次性安装会自动进行依赖顺序安装调整： sudo rpm -ivh *.rpm ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:3:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"安装包功能和作用 部署 MySQL 5.7.20 版本的 RPM 包时，需要确保按照正确的顺序安装这些包，因为它们之间存在依赖关系。以下是各个包的功能作用和推荐的安装顺序： mysql-community-common-5.7.20-1.el7.x86_64.rpm 功能：提供所有 MySQL 组件共享的文件。 作用：这个包是基础，其他组件依赖它提供的文件。 mysql-community-libs-5.7.20-1.el7.x86_64.rpm 功能：包含客户端库文件。 作用：为使用 MySQL 客户端连接到 MySQL 数据库服务器提供必要的库文件。 mysql-community-libs-compat-5.7.20-1.el7.x86_64.rpm 功能：提供兼容性库文件。 作用：为旧版本的应用程序提供与旧版 MySQL 库的向后兼容性。 mysql-community-client-5.7.20-1.el7.x86_64.rpm 功能：包含 MySQL 客户端命令行工具。 作用：允许用户通过命令行界面与 MySQL 数据库交互。 mysql-community-server-5.7.20-1.el7.x86_64.rpm 功能：提供 MySQL 数据库服务器。 作用：这是核心包，负责处理 SQL 查询并管理数据库。 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:3:1","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"配置主服务器 MySQL 的主配置文件通常位于 /etc/my.cnf 或 /etc/mysql/my.cnf，具体取决于你的操作系统。对于某些发行版或自定义安装，配置文件可能位于其他位置，如 /etc/my.cnf.d/ 目录下的某个文件中。你可以使用以下命令来查找配置文件的位置： $ mysql --help | grep \"Default options\" -A 1 Default options are read from the following files in the given order: /etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf 调整需要将数据放到/data/mysql 目录下，该目录为外挂磁盘，磁盘空间较大，而不是安装在默认的系统磁盘目录下 创建安装目录/data/mysql mkdir -p /data/mysql/binlog chown -R mysql:mysql /data/mysql chomod -R 755 /data/mysql 修改/etc/my.cnf配置文件 [mysqld] user = mysql port = 3306 sql_mode = NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER lower_case_table_names = 1 group_concat_max_len = 40960 max_allowed_packet = 1G datadir = /data/mysql socket = /data/mysql/mysql.sock log_error = /data/mysql/mysqld.log pid_file = /data/mysql/mysqld.pid # 设置server_id,注意要唯一 server_id = 1 # 忽略的数据，指不需要同步的数据库 #binlog-ignore-db=mysql # 指定同步的数据库 #binlog-do-db=test ###binlog### binlog_format = row ## 开启二进制日志功能，以备Slave作为其它Slave的Master时使用 log_bin = /data/mysql/binlog/binlog.log expire_logs_days = 7 master_info_repository = table relay_log_info_repository = table ###slave### slave_parallel_type = LOGICAL_CLOCK slave_parallel_workers = 4 log_bin_trust_function_creators = 1 skip_slave_start = ON relay_log_recovery = ON ###buffer_pool### join_buffer_size = 32M sort_buffer_size = 16M query_cache_size = 512M myisam_sort_buffer_size = 128M thread_cache_size = 128 tmp_table_size = 512M max_heap_table_size = 128M read_rnd_buffer_size = 2M #InnoDB# default_storage_engine = InnoDB innodb_file_per_table = ON innodb_flush_log_at_trx_commit = 2 innodb_buffer_pool_size = 4G innodb_log_file_size = 1024M innodb_flush_method = O_DIRECT ##### MyISAM##### key_buffer_size = 512M ### character_set##### character_set_server = utf8 collation_server = utf8_general_ci explicit_defaults_for_timestamp = ON ####name_resolves#### host_cache_size = 0 skip_name_resolve = ON ###connection### max_connections = 25000 max_connect_errors = 10000 ### Enable slow_query_log permanent ### slow_query_log = 1 long_query_time = 0.5 slow_query_log_file = /data/mysql/mysql_slow.log ### MASTER_MASTER ### auto_increment_increment = 1 auto_increment_offset = 1 [mysql] #prompt = 'mysql[\\u@\\h]:[\\d]:' [client] socket = /data/mysql/mysql.sock port = 3306 启动mysqld服务，并设置开机自启 $ sudo systemctl enable mysqld --now $ sudo systemctl status mysqld ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:4:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"配置从服务器 创建安装目录/data/mysql mkdir -p /data/mysql/binlog chown -R mysql:mysql /data/mysql chomod -R 755 /data/mysql 修改/etc/my.cnf配置文件 [mysqld] # 设置server_id,注意要唯一 server_id = 1 ... ### MASTER_MASTER ### auto_increment_increment = 1 auto_increment_offset = 2 # 可根据需要调整，确保从节点与主节点的 AUTO_INCREMENT 值不冲突 启动mysqld服务，并设置开机自启 $ sudo systemctl enable mysqld --now $ sudo systemctl status mysqld ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:5:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"修改登录密码 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:6:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"快捷操作 MYSQL_OLDPASSWORD=`awk '/A temporary password/{print $NF}' /data/mysql/mysqld.log` MYSQL_ROOT_PASSWORD=\"your_mysql_password\" mysqladmin -u root -p$MYSQL_OLDPASSWORD password $MYSQL_ROOT_PASSWORD \u0026\u003e/dev/null ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:6:1","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"分步操作 1、获取临时密码 默认是在/var/log/mysqld.log 文件下，这里修改了安装目录为/data/mysql sudo grep 'temporary password' /data/mysq/mysqld.log # grep 'temporary password' /var/log/mysqld.log 2024-11-25T06:34:37.637723Z 1 [Note] A temporary password is generated for root@localhost: hvqki:Gp\u003e0EI 2、临时密码登录 mysql -u root -p 3、登录后修改默认密码并进行安全配置： 首次登录后，建议立即更改 root 密码并执行 mysql_secure_installation 脚本来提高安全性。 登录后修改root密码 ALTER USER 'root'@'%' IDENTIFIED BY 'your_password'; mysql\u003e ALTER USER 'root'@'%' IDENTIFIED BY 'your_password'; Query OK, 0 rows affected (0.00 sec) mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 4 rows in set (0.00 sec) # 赋远程访问权限 mysql\u003e GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'your_password' WITH GRANT OPTION; Query OK, 0 rows affected, 1 warning (0.02 sec) mysql\u003e flush privileges; Query OK, 0 rows affected (0.00 sec) mysql\u003e select host, user, authentication_string, plugin from user; +-----------+---------------+-------------------------------------------+-----------------------+ | host | user | authentication_string | plugin | +-----------+---------------+-------------------------------------------+-----------------------+ | localhost | root | *4E02A7566A743B1042608F7ED03213DE937E7D0E | mysql_native_password | | localhost | mysql.session | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | | localhost | mysql.sys | *THISISNOTAVALIDPASSWORDTHATCANBEUSEDHERE | mysql_native_password | | % | root | *4E02A7566A743B1042608F7ED03213DE937E7D0E | mysql_native_password | +-----------+---------------+-------------------------------------------+-----------------------+ 4 rows in set (0.00 sec) 4、mysql_secure_installation MySQL 安全设置(可选) mysql_secure_installation 是一个用于提高 MySQL 安装安全性的脚本。它可以帮助你执行一系列的安全设置，例如设置 root 密码、移除匿名用户、禁用远程 root 登录、移除测试数据库等。以下是执行 mysql_secure_installation 脚本的详细步骤： [root@localhost mysql]\u003c20241225 14:42:33\u003e# sudo mysql_secure_installation Securing the MySQL server deployment. Enter password for user root: 跟随提示完成配置 运行脚本后，你会被问到一系列问题。下面是每个问题的大致内容和建议的回答方式： Enter password for user root: 输入你当前的 root 用户密码（即安装时生成的临时密码或你刚刚设置的新密码）。 Switch to unix_socket authentication [Y/n] 这个选项是关于是否切换到 Unix Socket 认证插件，默认情况下选择 N 不启用这个选项，这样可以继续使用基于密码的身份验证。 Change the root password? [Y/n] 如果你已经在登录时修改了 root 密码，可以选择 n；如果还没有修改，可以选择 y 并按照提示设置新密码。 Remove anonymous users? [Y/n] 移除匿名用户可以提高安全性，推荐选择 Y。 Disallow root login remotely? [Y/n] 禁止 root 用户从远程登录通常是个好主意，除非你需要远程管理 MySQL，推荐选择 Y。测试环境也可以选择n Remove test database and access to it? [Y/n] 移除测试数据库及其访问权限也是提高安全性的措施之一，推荐选择 Y。测试环境也可以选择n Reload privilege tables now? [Y/n] 重新加载权限表以使更改立即生效，推荐选择 Y。 对话参考 Securing the MySQL server deployment. Enter password for user root: ******** Estimated strength of the password: 100 Do you wish to continue with the password provided?(Press y|Y for Yes, any other key for No) : Y By default, a MySQL installation has an anonymous user, allowing anyone to log into MySQL without having to have a user account created for them. This is intended only for testing, and to make the installation go a bit smoother. You should remove them before moving into a production environment. Remove anonymous users? (Press y|Y for Yes, any other key for No) : Y ... Success! Normally, root should only be allowed to connect from 'localhost'. This ensures that someone cannot guess at the root password from the network. Disallow root login remotely? (Press y|Y for Yes, any other key for No) : Y ... Success! By default, MySQL comes with a database named 'test' that anyone can access. This is also intended only for testing, and should be removed before moving into a production environment. Remove test database and access to it? (Press y|Y for Yes, any other key for No) : Y - Dropping test database... ... Success! - Removing privileges on test database... ... Success! Reloading the privilege tables will ensure that all changes made so far will take effect immediately. Reload priv","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:6:2","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"安装后验证 可以尝试通过 MySQL 客户端登录到服务器。 使用 mysqladmin 工具验证服务器是否正在运行： $ mysqladmin -u root -p version Enter password: mysqladmin Ver 8.42 Distrib 5.7.20, for Linux on x86_64 Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Server version 5.7.20 Protocol version 10 Connection Localhost via UNIX socket UNIX socket /var/lib/mysql/mysql.sock Uptime: 14 min 46 sec Threads: 1 Questions: 12 Slow queries: 0 Opens: 106 Flush tables: 1 Open tables: 99 Queries per second avg: 0.013 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:7:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"开启主从同步 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:8:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"Master 节点 # 登录到 MySQL 主节点 $ mysql -u root -p # 创建复制用户（例如：`replica_user`） CREATE USER 'replica_user'@'192.168.143.102' IDENTIFIED BY 'your_password'; #授予复制权限 GRANT REPLICATION SLAVE ON *.* TO 'replica_user'@'192.168.143.102'; # 刷新权限 FLUSH PRIVILEGES; # 获取主节点的二进制日志位置 SHOW MASTER STATUS; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000001 | 154 | your_database_name | | +------------------+----------+--------------+------------------+ 记下 File二进制文件（例如：mysql-bin.000001）和 Position偏移量（例如：154）的值。 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:8:1","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"Slave 节点 # 登录到 MySQL 备节点 $ mysql -u root -p # 配置主节点信息 # 使用 `CHANGE MASTER TO` 命令配置从库指向主库。你需要提供主库的 IP 地址、用户名、密码以及上面提到的 `File` 和 `Position` 值： CHANGE MASTER TO MASTER_HOST='192.168.143.101', MASTER_USER='replica_user', MASTER_PASSWORD='your_password', MASTER_LOG_FILE='mysql-bin.000001', -- 主节点的日志文件 MASTER_LOG_POS=154; -- 主节点的日志位置 #启动复制进程 开始从库的 I/O 和 SQL 线程以开始复制过程： START SLAVE; # 查看复制状态 检查从库的状态，确认复制是否正常工作： SHOW SLAVE STATUS\\G; 关注以下字段： Slave_IO_Running: 应该是 Yes Slave_SQL_Running: 应该是 Yes Last_Error: 应为空或无错误信息 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:8:2","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"测试主从同步 在Master主服务器上创建数据库和表，并插入数据： mysql\u003e create database test; mysql\u003e use test; mysql\u003e create table test(age int); mysql\u003e insert into test values(1); mysql\u003e select * from test; +------+ | age | +------+ | 1 | +------+ 在从服务器上检查数据是否同步： mysql\u003e select * from test.test; +------+ | age | +------+ | 1 | +------+ 1 row in set (0.00 sec) 以上操作确保了主服务器的数据成功复制到从服务器，实现了主从同步。 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:9:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"卸载 MySQL 卸载或者卸载重新安装 systemctl stop mysqld systemctl status mysqld rpm -evh rpm包名 # 依次卸载rpm包，按照下面顺序卸载不然会报错 sudo rpm -evh mysql-community-server-5.7.20-1.el7.x86_64 sudo rpm -evh mysql-community-client-5.7.20-1.el7.x86_64 sudo rpm -evh mysql-community-libs-compat-5.7.20-1.el7.x86_64 sudo rpm -evh mysql-community-libs-5.7.20-1.el7.x86_64 sudo rpm -evh mysql-community-common-5.7.20-1.el7.x86_64 # 查询是否还存在遗漏文件 rpm -qa|grep mysql # 删除 mysql 数据库内容 sudo rm -rf /data/mysql sudo rm -rf /var/lib/mysql sudo rm -rf /usr/local/mysql sudo rm -rf /var/run/mysqld sudo rm -rf /etc/my.cnf sudo rm -f /var/log/mysqld.log ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:10:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["MySQL"],"content":"参考 MySQL v5.7 rpm 安装包 MySQL 官网各个版本安装包 ","date":"2024-10-16","objectID":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/:11:0","tags":["MySQL"],"title":"MySQL 集群之主从部署","uri":"/mysql%E9%9B%86%E7%BE%A4%E4%B9%8B%E4%B8%BB%E4%BB%8E%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"背景 为物联网和边缘计算构建的经过认证的Kubernetes发行版，则是Rancher旗下产品 Great for: Edge IoT CI Development ARM Embedding k8s Situations where a PhD in k8s clusterology is infeasible ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:1:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"发展历程 微型kubernetes发行版 CNCF认证的Kubernetes发行版 50MB左右二进制包，500MB左右内存消耗 单一进程包含Kubernetes master,Kubelet,和 containerd 支持SQLite/Mysql/PostgreSQL/DQlite和etcd 同时为x86 64,Arm64,和 Armv7 平台发布 变更 多合一：将基础服务合并到了单一的进程中，如api-server,controller manager、scheduler和cloud-controller-manager合到了单一的进程中,简化了运维，降低了复杂度 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"基本架构 角色 k3s server api-server controller-manager scheduler etcd(option) containerd k3s-agent k3s agent k3s-agent containerd|docker(optional) K3s 架构\r技术亮点 单进程架构简化部署 移除各种非必须代码，减小资源占用 TLS证书管理 内置Containerd 内置自运行rootfs 内置Helm Chart管理机制 内置L4/L7 LB支持 其它 默认使用containerd 默认命名空间 kube-system kube-public kube-node-release k3s 资源文件目录：/var/lib/rancher/k3s ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:3:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"安装部署 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:4:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"server 部署 $ mkdir /root/k3s \u0026\u0026 cd /root/k3s # 1. k3s镜像文件,指定安装版本为 v1.24.9 $ wget https://github.com/k3s-io/k3s/releases/download/v1.24.9-rc1%2Bk3s2/k3s-airgap-images-amd64.tar.gz $ sudo mkdir -p /var/lib/rancher/k3s/agent/images/ $ sudo cp /root/k3s/k3s-airgap-images-amd64.tar /var/lib/rancher/k3s/agent/images/ # 2. k3s安装脚本 $ wget https://rancher-mirror.rancher.cn/k3s/k3s-install.sh # 3. k3s二进制文件 $ wget https://github.com/k3s-io/k3s/releases/download/v1.24.9-rc1%2Bk3s2/k3s $ sudo chmod a+x /root/k3s/k3s /root/k3s/k3s-install.sh $ sudo cp /root/k3s/k3s /usr/local/bin/ # 4. 安装k3s $ INSTALL_K3S_SKIP_DOWNLOAD=true /root/k3s/k3s-install.sh # 5. 验证 $ crictl images IMAGE TAG IMAGE ID SIZE docker.io/rancher/klipper-helm v0.7.4-build20221121 6f2af12f2834b 252MB docker.io/rancher/klipper-lb v0.4.0 3449ea2a2bfa7 9.21MB docker.io/rancher/local-path-provisioner v0.0.23 9621e18c33880 37.7MB docker.io/rancher/mirrored-coredns-coredns 1.9.4 a81c2ec4e946d 49.8MB docker.io/rancher/mirrored-library-busybox 1.34.1 827365c7baf13 5.09MB docker.io/rancher/mirrored-library-traefik 2.9.4 288889429becf 136MB docker.io/rancher/mirrored-metrics-server v0.6.2 25561daa66605 70.2MB docker.io/rancher/mirrored-pause 3.6 6270bb605e12e 686kB $ kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system local-path-provisioner-687d6d7765-h8xvr 1/1 Running 0 2m25s kube-system coredns-7b5bbc6644-nbvnf 1/1 Running 0 2m25s kube-system helm-install-traefik-crd-zfzqv 0/1 Completed 0 2m25s kube-system metrics-server-667586758d-5gvfd 1/1 Running 0 2m25s kube-system svclb-traefik-07d6e08f-gbvwd 2/2 Running 0 107s kube-system traefik-64b96ccbcd-tbcth 1/1 Running 0 107s kube-system helm-install-traefik-tvcc8 0/1 Completed 2 2m25s # server 节点获取k3s token $ mynodetoken=`awk -F ':' '{print $4}' /var/lib/rancher/k3s/server/token` ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:4:1","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"agent 部署 # 1. 准备k3s 二进制 $ mkdir /root/k3s \u0026\u0026 cd /root/k3s $ wget https://github.com/k3s-io/k3s/releases/download/v1.24.9-rc1%2Bk3s2/k3s $ wget https://rancher-mirror.rancher.cn/k3s/k3s-install.sh # 需保证各主机名称不一样进入 /etc/hostname 和 /etc/hosts 修改即可 $ INSTALL_K3S_SKIP_DOWNLOAD=true INSTALL_K3S_VERSION=\"v1.24.9-rc1+k3s2\" K3S_URL=https://192.168.143.101:6443 K3S_TOKEN=7983aa3ee5b8a70afa61a79e2e6476d6 /root/k3s/k3s-install.sh # 在线安装 $ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | INSTALL_K3S_MIRROR=cn \\ K3S_URL=https://192.168.143.101:6443 \\ K3S_TOKEN=7983aa3ee5b8a70afa61a79e2e6476d6 INSTALL_K3S_VERSION=\"v1.24.9-rc1+k3s2\" \\ sh - # 2. 查看状态 $ systemctl status k3s-agent ● k3s-agent.service - Lightweight Kubernetes Loaded: loaded (/etc/systemd/system/k3s-agent.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2024-09-23 17:05:24 CST; 8s ago Docs: https://k3s.io Process: 4125 ExecStartPre=/sbin/modprobe overlay (code=exited, status=0/SUCCESS) Process: 4121 ExecStartPre=/sbin/modprobe br_netfilter (code=exited, status=0/SUCCESS) Process: 4116 ExecStartPre=/bin/sh -xc ! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service 2\u003e/dev/null (code=exited, status=0/SUCCESS) Main PID: 4131 (k3s-agent) Tasks: 24 Memory: 300.4M CGroup: /system.slice/k3s-agent.service ├─4131 /usr/local/bin/k3s agent └─4159 containerd # 3. 切到server 节点 $ kubectl get node NAME STATUS ROLES AGE VERSION master Ready control-plane,master 25m v1.24.9-rc1+k3s2 node1 Ready \u003cnone\u003e 64s v1.24.9-rc1+k3s2 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:4:2","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"K3s 各名字空间介绍 在Kubernetes中，包括K3s，命名空间（Namespaces）提供了一种机制来划分集群资源。它们对于组织和隔离工作负载非常有用，特别是在多用户环境中。以下是K3s中常见的几个命名空间及其作用： default： 这是Kubernetes集群中的默认命名空间。如果你没有特别指定一个命名空间，那么你的Pod、服务等资源将会被创建在这个命名空间中。 kube-system： 该命名空间包含了由Kubernetes系统自身创建的所有组件，如核心DNS服务（CoreDNS）、各种控制器和服务代理（kube-proxy）。此外，如果使用了某些网络插件或存储插件，相关的Pod也会出现在这个命名空间中。 kube-public： 这个命名空间是一个特殊的命名空间，旨在供所有用户（包括那些未认证的用户）读取。它通常用于放置所有人都可以访问的信息，例如集群引导令牌（bootstrap tokens）。在标准配置下，此命名空间内只有一个ConfigMap对象，名为cluster-info，它包含了关于API服务器的信息。 kube-node-lease： 这个命名空间是从Kubernetes 1.16版本开始引入的，用来存放节点租约（Node Lease）对象。这些对象允许节点定期更新其活跃状态，从而优化了节点健康检查的过程，使得集群能够更快地检测到不健康的节点。 其他命名空间： 用户可以根据需要创建更多的命名空间来组织不同的应用或者团队的工作负载。这有助于资源管理和配额控制，并且可以在不同项目之间提供一定程度的隔离。 特定于K3s的命名空间： 在一些情况下，K3s可能会引入额外的命名空间以支持特定的功能或集成。例如，当启用某些附加组件时，可能会自动创建相应的命名空间来托管这些组件。 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:5:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"其他名字空间 catle-system Rancher的\"catle-system\"命名空间通常用于存储Rancher控制平面组件的资源，如Webhook服务(rancher-webhookpod)和与安全相关的配置，比如TLS证书(cattle-webhook-tls)。这些组件是Rancher管理集群自动化部署和操作的核心部分。 当你提到洲除1oca1集群catt1e-system命名空间中的特定Pod和TLS密钥时，这意味着你可能正在清理测试环境或者准备进行维护操作。以 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:5:1","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"生产环境参考 k3s 版本v1.22 数据库：etcd 容器运行时：containerd v1.5.9-k3s1 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:6:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"1. server 服务状态 $ systemctl status k3s ● k3s.service - Lightweight Kubernetes Loaded: loaded (/etc/systemd/system/k3s.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2024-05-13 23:00:30 CST; 8 months 10 days ago Docs: https://k3s.io Main PID: 25364 (k3s-server) Tasks: 84 Memory: 938.8M CGroup: /system.slice/k3s.service ├─17092 /var/lib/rancher/k3s/data/8307e9b398a0ee686ec38e18339d1464f75158a8b948b059b564246f4af3a0a6/bin/containerd-shim-runc-v2 -namespace k8s.io -id f11d857c503976e830b667a4... ├─25364 /usr/local/bin/k3s server ├─25379 containerd $ systemctl status etcd ● etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: active (running) since Fri 2022-05-20 09:50:06 CST; 2 years 8 months ago Main PID: 63172 (etcd) Tasks: 11 Memory: 413.0M CGroup: /system.slice/etcd.service └─63172 /opt/etcd/etcd --config-file=/data/etcd/2379/conf/etcd.conf 版本 $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.6+k3s1\", GitCommit:\"3228d9cb9a4727d48f60de4f1ab472f7c50df904\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T01:27:44Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"22\", GitVersion:\"v1.22.6+k3s1\", GitCommit:\"3228d9cb9a4727d48f60de4f1ab472f7c50df904\", GitTreeState:\"clean\", BuildDate:\"2022-01-25T01:27:44Z\", GoVersion:\"go1.16.10\", Compiler:\"gc\", Platform:\"linux/amd64\"} $ kubectl -n kube-system get all NAME READY STATUS RESTARTS AGE pod/coredns-96cc4f57d-7k4gr 1/1 Running 0 2y247d pod/metrics-server-ff9dbcb6c-vl8hl 1/1 Running 0 2y200d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.43.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 2y248d service/kubelet ClusterIP None \u003cnone\u003e 10250/TCP,10255/TCP,4194/TCP 2y247d service/metrics-server ClusterIP 10.43.99.234 \u003cnone\u003e 443/TCP 2y248d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/coredns 1/1 1 1 2y248d deployment.apps/metrics-server 1/1 1 1 2y248d NAME DESIRED CURRENT READY AGE replicaset.apps/coredns-96cc4f57d 1 1 1 2y248d replicaset.apps/metrics-server-ff9dbcb6c 1 1 1 2y248d 容器运行时 ctr 是 containerd 的客户端命令行工具 $ ctr --help NAME: ctr - __ _____/ /______ / ___/ __/ ___/ / /__/ /_/ / \\___/\\__/_/ containerd CLI USAGE: ctr [global options] command [command options] [arguments...] VERSION: v1.5.9-k3s1 $ ctr version Client: Version: v1.5.9-k3s1 Revision: Go version: go1.16.10 Server: Version: v1.5.9-k3s1 Revision: UUID: 521e28e5-d9b4-4ae9-9dfd-48f742329485 相关镜像 $ ctr images list -q|grep -v sha256|grep -v fluent docker.io/rancher/mirrored-coredns-coredns:1.8.6 docker.io/rancher/mirrored-metrics-server:v0.5.2 docker.io/rancher/mirrored-pause:3.6 docker.io/rancher/pause:3.1 docker.io/rancher/rancher-agent:v2.6.6 systemd 启动配置 $ cat /etc/systemd/system/k3s.service [Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io Wants=network-online.target After=network-online.target [Install] WantedBy=multi-user.target [Service] Type=notify EnvironmentFile=-/etc/default/%N EnvironmentFile=-/etc/sysconfig/%N EnvironmentFile=-/etc/systemd/system/k3s.service.env KillMode=process Delegate=yes # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s ExecStartPre=/bin/sh -xc '! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service' ExecStartPre=-/sbin/modprobe br_netfilter ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/k3s \\ server \\ '--disable' \\ 'traefik' \\ '--disable' \\ 'servicelb' \\ '--disable' \\ 'local-storage' \\ '--datastore-endpoint=http://192.168.47.24:2379,http://192.168.47.25:2379,http://192.168.47.26:2379' \\ '--tls-san' \\ '192.168.47.251' \\ '--token' \\ 'secret' \\ '--kubelet-arg=container-log-max-files=5' \\ '--kubelet-arg=container-log-max-s","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:6:1","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"2. agent $ systemctl status k3s-agent ● k3s-agent.service - Lightweight Kubernetes Loaded: loaded (/etc/systemd/system/k3s-agent.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2024-05-13 23:03:57 CST; 8 months 10 days ago Docs: https://k3s.io Main PID: 68145 (k3s-agent) Tasks: 120 Memory: 1.2G CGroup: /system.slice/k3s-agent.service k3s-agent.service 配置文件信息 # cat /etc/systemd/system/k3s-agent.service [Unit] Description=Lightweight Kubernetes Documentation=https://k3s.io Wants=network-online.target After=network-online.target [Install] WantedBy=multi-user.target [Service] Type=exec EnvironmentFile=-/etc/default/%N EnvironmentFile=-/etc/sysconfig/%N EnvironmentFile=-/etc/systemd/system/k3s-agent.service.env KillMode=process Delegate=yes # Having non-zero Limit*s causes performance problems due to accounting overhead # in the kernel. We recommend using cgroups to do container-local accounting. LimitNOFILE=1048576 LimitNPROC=infinity LimitCORE=infinity TasksMax=infinity TimeoutStartSec=0 Restart=always RestartSec=5s ExecStartPre=/bin/sh -xc '! /usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service' ExecStartPre=-/sbin/modprobe br_netfilter ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/k3s \\ agent \\ '--kubelet-arg' \\ 'container-log-max-files=5' \\ '--kubelet-arg' \\ 'container-log-max-size=100Mi' \\ ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:6:2","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"用户案例 选择K3s的原因 k3s与Kubernetes使用习惯完全一致:上手无代价 与其他同类产品比，无功能绑定 兼容Arm设备 通过导入Rancher进行统一纳管 无厂商锁定，无需绑定硬件 100% 开源，合作方式灵活 出错重启就可以了，不像k8s 出错重启也不一定能解决，哈哈 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:7:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"Q/A 使用k8s 替换k3s 有什么问题？ 如果只是普通的使用一些workload 是没有什么问题的，如果workload 中的一些应用使用了一些诸如自定义的cloud provider 是不适配的，一般应用于iot之类的场景 ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:8:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["K3s"],"content":"参考 k3s官网 Rancher Product K3s Document Kubernetes Document K3s Github 从0到1基础入门 全面了解k3s 一文搞定全场景K3s离线安装 k3s安装指定版本以及离线安装（docker) 11-K3S 安装-仪表盘及卸载K3s ","date":"2024-09-23","objectID":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:9:0","tags":["K3s"],"title":"K3s 快速入门及安装部署","uri":"/k3s%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%8F%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["linux"],"content":"rsync(remote synchronize)是一个远程数据同步工具，可通过 LAN/WAN 快速同步多台主机之间的文件，也可以使用 rsync 同步本地硬盘中的不同目录。rsync 是用于替代rcp的一个工具，rsync 使用所谓的 rsync算法 进行数据同步，这种算法只传送两个文件的不同部分，而不是每次都整份传送，因此速度相当快。 rsync 基于inotify开发,inotify 主要的功能是监测目标目录是否有变化。rsync 用作同步，inotify 用作监测，两者结合使用实现静态资源方案同步的实现。 Rsync有三种模式 本地模式(类似于cp命令) 远程模式(类似于scp命令) 守护进程(socket进程：是rsync的重要功能) ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:0:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"安装 测试机器节点192.168.143.101作为源主机 和 192.168.143.102作为目标主机 rsync 结合inotify 监控源主机的指定目录实时推送到目标主机的指定目录下，实现静态资源的实时同步 安装配置(Red Hat) $ yum install rsync -y # 修改配置文件 $ vim /etc/rsyncd.conf ··· [ftp] path = /data/rsync_test # 启动并设置开机自启 $ systemctl enable rsyncd --now # or $ rsync --daemon # 查看目标主机同步源主机指定目录下的文件 $ rsync --list-only lushuan@192.168.143.101::ftp drwxr-xr-x 19 2025/02/10 10:20:40 . -rw-r--r-- 0 2025/02/10 10:20:40 1.txt # 远程同步1：将本地内容同步到远程主机 rsync -av source/ username@remote_host:destination $ rsync -avz /data/rsync_test/ lushuan@192.168.143.102::ftp/ # 远程同步2: 将远程主机内容同步到本地 $ rsync -avz 192.168.143.101::ftp/ /data/rsync_test/ receiving incremental file list ./ 1.txt 注意 传输的双方都必须安装 rsync。 目标主机需要防火墙需要对源主机开放873 端口权限 目标主机的同步目录权限应为755 ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:1:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"rsyncd 增加安全认证及免密登录 echo \"lushuan:111\" \u003e\u003e /etc/rsyncd.pwd chmod 600 /etc/rsyncd.pwd vim /etc/rsyncd.conf ... auth users = lushuan secrets file=/etc/rsyncd.pwd [ftp] path = /data/rsync_test $ rsync --list-only lushuan@192.168.143.101::ftp Password: (111) drwxr-xr-x 19 2025/02/10 10:20:40 . -rw-r--r-- 0 2025/02/10 10:20:40 1.txt # 目标主机免交互输入密码方式同步文件 $ echo \"111\" \u003e\u003e /etc/rsyncd.pwd.client $ chmod 600 /etc/rsyncd.pwd.client $ rsync --list-only --password-file=/etc/rsyncd.pwd.client lushuan@192.168.143.101::ftp drwxr-xr-x 19 2025/02/10 10:20:40 . -rw-r--r-- 0 2025/02/10 10:20:40 1.txt # 执行增量同步操作到目标主机的目标目录 /data/rsync_test $ rsync -avz --password-file=/etc/rsyncd.pwd.client lushuan@192.168.143.101::ftp /data/rsync_test # 目标主机操作，源主机有文件删除时执行同步操作 --delete $ rsync -avz --delete --password-file=/etc/rsyncd.pwd.client lushuan@192.168.143.101::ftp /data/rsync_test rsync 常用选项 选项 含义 -a, --archive 归档模式，表示以递归方式传输文件，并保持所有文件属性（相当于 -rlptgoD 的组合）。 -r, --recursive 对子目录以递归模式处理，同步目录时包括子目录及其中的文件。 -v, --verbose 详细模式输出，显示更多的关于传输过程的信息。 -z, --compress 在传输过程中压缩文件数据。 -P 相当于 --partial --progress，显示传输进度并保留部分传输的文件（如果传输中断）。 -u, --update 只更新目标端较旧的文件或不存在的文件，跳过比源文件新的目标文件。 -l, --links 保留符号链接，将源目录中的符号链接作为链接直接复制而不是它们指向的实际文件。 -p, --perms 保留文件权限。 -t, --times 保留修改时间。 -g, --group 保留文件所属组信息。 -o, --owner 保留文件所有者信息（仅适用于超级用户）。 -D 相当于 --devices --specials，保留设备文件和特殊文件。 --delete 删除目标位置上存在但在源位置不存在的文件，使目标位置的内容完全与源位置一致。 --exclude=PATTERN 指定排除规则，不传输匹配该模式的文件或目录。 ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:2:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"rsync 推送方案 近时推送(目标主机定时轮询拉取源主机，会有一定的网络资源的浪费) 实时推送 ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:3:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"实时推送源服务器和目标服务器配置 源主机192.168.143.101和目标主机192.168.143.102 rsyncd 配置修改 # 1. 192.168.143.102 目标主机配置 $ echo \"lushuan:111\" \u003e\u003e /etc/rsyncd.pwd $ chmod 600 /etc/rsyncd.pwd $ vim /etc/rsyncd.conf ... uid = root # 创建/data/rsync_test目录的用户拥有的权限 gid = root # 创建/data/rsync_test目录的用户拥有的权限 auth users = lushuan secrets file=/etc/rsyncd.pwd read only = no [ftp] path = /data/rsync_test # 重启服务 $ systemctl restart rsyncd # 2.192.168.143.101 源主机配置 $ echo \"111\" \u003e\u003e /etc/rsyncd.pwd.client $ chmod 600 /etc/rsyncd.pwd.client $ rsync --list-only --password-file=/etc/rsyncd.pwd.client lushuan@192.168.143.102::ftp # 重启服务 $ systemctl restart rsyncd # 实时推送(手动) $ rsync -avz --delete --password-file=/etc/rsyncd.pwd.client /data/rsync_test/ lushuan@192.168.143.102::ftp ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:3:1","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"inotify ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:4:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"inotify 介绍 inotify 是一种用于 Linux 系统的文件系统事件监控机制，它允许应用程序监听文件或目录的变化。通过 inotify，程序可以接收到诸如文件被读取、写入、创建、删除、权限更改等操作的通知。这使得它非常适合需要实时响应文件系统变化的应用场景，比如自动同步文件、日志监控或者作为某些开发工具（如热重载功能）的基础。 以下是 inotify 的一些关键特性： 高效性：与之前的方案（例如 dnotify）相比，inotify 使用了更高效的机制来跟踪文件系统事件，减少了资源消耗。 灵活性：可以监视单个文件或整个目录树，并且可以选择感兴趣的事件类型。 非递归监控：默认情况下，对目录的监控不是递归的；如果要监控子目录，则需要为每个子目录单独设置监控，或者使用第三方工具来实现递归监控。 丰富的事件类型：支持多种类型的事件，包括但不限于访问文件（access）、修改文件（modify）、文件属性或内容的改变（attrib）、打开文件（open）、关闭文件（close）、移动（move）、创建（create）、删除（delete）等。 在实际应用中，可以通过编程接口（如 C 语言提供的 API 或者各种脚本语言的扩展库）直接使用 inotify，也可以利用一些基于 inotify 开发的工具，如 inotify-tools，它提供了命令行工具 inotifywait 和 inotifywatch 来简化监控任务。 由于其强大的功能和灵活性，inotify 已经成为现代 Linux 系统上监控文件系统活动的标准方式之一。不过需要注意的是，inotify 主要适用于 Linux 系统，在其他类 Unix 系统上可能不直接可用。 ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:4:1","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"安装 推送端192.168.143.101安装inotify # 依赖 yum install -y automake # 安装 yum install -y inotify-tools 源主机监控目录 inotifywait -m -r --timefmt '%Y-%m-%d %H:%M:%S' --format '%T %w%f %e' -e close_write,modify,delete,create,attrib,move /data/rsync_test/ 简单的自动化脚本 #!/bin/bash SOURCE=\"/data/rsync_test/\" DEST_USER=\"lushuan\" DEST_HOST=\"192.168.143.102\" DEST_DIR=\"ftp\" # 如果没有设置加密认证可以移除掉 PASSWORD_FILE=\"/etc/rsyncd.pwd.client\" inotifywait -m -r --timefmt '%Y-%m-%d %H:%M:%S' -e close_write,modify,delete,create,attrib,move --format '%w%f' ${SOURCE} | while read NEWFILE do rsync -avz --delete --password-file=${PASSWORD_FILE} ${SOURCE} ${DEST_USER}@${DEST_HOST}::${DEST_DIR} done inotifywait 常用参数及其含义 选项 含义 -m, --monitor 持续监控而不退出，即使在首次事件发生后也继续监听。 -r, --recursive 递归监听所有子目录中的事件。 -e, --event 明确指定要监听的事件类型，如 access, modify, attrib, move, create, delete, close_write 等。可以同时指定多个事件，使用逗号分隔。 --timefmt 当与 --format 一起使用时，设置时间输出格式，例如 '%Y-%m-%d %H:%M:%S'。 --format 定制输出信息的格式，如 '[%T] %w %f %#Xe'，其中 %T 表示时间，%w 监听的目录，%f 发生事件的文件名，%#Xe 表示事件列表。 -q, --quiet 减少或消除非错误消息的输出，用于减少命令行输出的信息量。 -d, --daemon 以守护进程模式运行，通常配合 --outfile 使用来指定日志文件。 --outfile 指定输出重定向到的文件路径，常与 -d 选项一起使用。 ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:4:2","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"最佳实践 在部署rsyncd 时，可能会遇到权限相关的问题，或者是配置相关的问题，下面给出对应的参考，若果有其它的异常报错可以参考rsync故障排除解答,下面是配置参考，根据自己需求来修改，这里是加密认证的配置。 另外如果是在生产环境可能遇到防火墙没有开放的问题，注意放开一下 873 端口，最后就是源主机和目标主机如果是非root 用户注意查看目录的权限，推荐目录权限为766 ... uid = root gid = root auth users = lushuan secrets file=/etc/rsyncd.pwd read only = no [ftp] path = /data/rsync_test ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:5:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["linux"],"content":"参考 rsync官网 rsync 用法教程 rsync故障排除解答 ","date":"2024-08-26","objectID":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/:6:0","tags":["rsync"],"title":"rsync结合inotify实现静态资源实时同步","uri":"/rsync%E7%BB%93%E5%90%88inotify%E5%AE%9E%E7%8E%B0%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E5%AE%9E%E6%97%B6%E5%90%8C%E6%AD%A5/"},{"categories":["Helm"],"content":"前言 通过Helm部署应用，修改configmap后，Deployment 副本控制器不会自动生效，需要手动删除pod 重建一下才会生效， 这里实践通过添加标签的方式在通过只修改configmap也能联动Deployment自动重建 ","date":"2024-07-21","objectID":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/:1:0","tags":["Helm"],"title":"Helm修改configmap后应用自动重启生效","uri":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/"},{"categories":["Helm"],"content":"安装应用 helm repo add bitnami https://charts.bitnami.com/bitnami # 以grafana 应用部署为例 --untar 拉取下来后自动解压 helm pull bitnami/grafana --untar # 这里做创建测试，关闭持久化，不创建pvc helm install grafana ./grafana --set persistence.enabled=false $ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION grafana default 4 2024-07-21 15:28:58.208948145 +0800 CST deployed grafana-11.3.26 11.3.0 $ kubectl get pod NAME READY STATUS RESTARTS AGE grafana-6cc56576dc-97bvw 1/1 Running 0 138m ","date":"2024-07-21","objectID":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/:2:0","tags":["Helm"],"title":"Helm修改configmap后应用自动重启生效","uri":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/"},{"categories":["Helm"],"content":"验证 1、修改配置 vim grafana/templates/deployment.yaml 2、添加label labels: {{- include \"common.labels.standard\" ( dict \"customLabels\" .Values.commonLabels \"context\" $ ) | nindent 4 }} app.kubernetes.io/component: grafana configmap-hash: \"{{ .Values.config | toYaml | sha256sum | trunc 63 }}\" # 添加configmap-hash 3、修改configmap vim grafana/templates/configmap.yaml data 内容添加一行测试数据GF_TEST: /opt/test root@k8s-master01:~/helm# kubectl get cm grafana-envvars -o yaml apiVersion: v1 data: GF_AUTH_LDAP_ALLOW_SIGN_UP: \"false\" GF_AUTH_LDAP_CONFIG_FILE: /opt/bitnami/grafana/conf/ldap.toml GF_AUTH_LDAP_ENABLED: \"false\" GF_INSTALL_PLUGINS: \"\" GF_PATHS_CONFIG: /opt/bitnami/grafana/conf/grafana.ini GF_PATHS_DATA: /opt/bitnami/grafana/data GF_PATHS_LOGS: /opt/bitnami/grafana/logs GF_PATHS_PLUGINS: /opt/bitnami/grafana/data/plugins GF_PATHS_PROVISIONING: /opt/bitnami/grafana/conf/provisioning GF_SECURITY_ADMIN_USER: admin GF_TEST: /opt/test 4、升级 $ helm upgrade grafana ./grafana --set persistence.enabled=false # 可以看到容器发生了重建 $ kubectl get pod NAME READY STATUS RESTARTS AGE grafana-6cc56576dc-97bvw 1/1 Running 0 145m grafana-7b8869fd4f-6jrd5 0/1 Running 0 4s ","date":"2024-07-21","objectID":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/:3:0","tags":["Helm"],"title":"Helm修改configmap后应用自动重启生效","uri":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/"},{"categories":["Helm"],"content":"总结 Helm借助添加标签hash化，动态更新deployment ","date":"2024-07-21","objectID":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/:4:0","tags":["Helm"],"title":"Helm修改configmap后应用自动重启生效","uri":"/helm%E4%BF%AE%E6%94%B9configmap%E5%90%8E%E5%BA%94%E7%94%A8%E8%87%AA%E5%8A%A8%E9%87%8D%E5%90%AF%E7%94%9F%E6%95%88/"},{"categories":["Kubernetes"],"content":"整体流程 部署准备 k8s 集群容器运行时 Containerd 准备 k8s 集群部署 k8s 集群软件 apt 源准备 k8s 集群软件安装 k8s 集群初始化 k8s 集群worker 节点加入 k8s 集群网络插件 Calico 准备 部署应用验证集群可用性 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:1:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"集群环境信息 操作系统Ubuntu22.04 部署 k8s 版本为v1.28 容器运行时为 Containerd 1.7.11 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:2:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"1. 主机准备 虚拟机是通过vmvare 创建的，搭建的k8s 集群是非高可用模式，下方提供部署脚本。 部署过程中由于2024年6月份 docker 镜像仓库不再对大陆提供服务，需要进行离线部署，主要是 calico 这一块会有镜像源缺失,对应章节会有补充说明及解决方案 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:3:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"1.1 操作系统 操作系统 版本 说明 ubuntu ubuntu-22.04.4-live-server-amd64 最小化 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:3:1","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"1.2 主机硬件配置说明 ip CPU 内存 硬盘 角色 主机名 192.168.1.11 2C 4G 30G master k8s-nastert01 192.168.1.7 2C 4G 30G node k8s-node01 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:3:2","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"1.3 主机配置 所有节点执行 1.3.1 主机名配置 #master 节点 hostnamectl set-hostname k8s-nastert01 #ndoe 节点 hostnamectl set-hostname k8s-node01 1.3.2 主机ip 地址配置 vmvare 创建虚拟机网络适配器选择桥接模式(自动)，需要手动配置ip地址，否则当虚拟机重启后ip 地址会发生变化 k8s-master01 节点 sudo cp /etc/netplan/00-installer-config.yaml /etc/netplan/00-installer-config.yaml_before cat \u003c\u003c EOF \u003e /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens33: addresses: - 192.168.1.11/16 nameservers: addresses: - 114.114.114.114 # 国内移动联通 - 8.8.8.8 # 谷歌 search: [] routes: - to: default via: 192.168.1.1 # 电脑本地网关地址 version: 2 EOF # 使配置生效 sudo netplan apply k8s-node01 节点 sudo cp /etc/netplan/00-installer-config.yaml /etc/netplan/00-installer-config.yaml_before cat \u003c\u003c EOF \u003e /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens33: addresses: - 192.168.1.7/16 nameservers: addresses: - 114.114.114.114 # 国内移动联通 - 8.8.8.8 # 谷歌 search: [] routes: - to: default via: 192.168.1.1 version: 2 EOF # 使配置生效 sudo netplan apply 1.3.3 主机名和ip 地址解析 cat \u003e\u003e /etc/hosts \u003c\u003c EOF \u003e 192.168.1.11 k8s-master01 \u003e 192.168.1.7 k8s-node01 \u003e EOF 1.3.4 chrony时间同步配置 k8s-master01 节点执行 $ cp /etc/chrony/chrony.conf /etc/chrony/chrony.conf_bak $ cat \u003e /etc/chrony/chrony.conf \u003c\u003c EOF confdir /etc/chrony/conf.d server ntp.aliyun.com iburst allow 192.168.1.1/24 sourcedir /run/chrony-dhcp sourcedir /etc/chrony/sources.d keyfile /etc/chrony/chrony.keys driftfile /var/lib/chrony/chrony.drift ntsdumpdir /var/lib/chrony logdir /var/log/chrony maxupdateskew 100.0 rtcsync makestep 1 3 leapsectz right/UTC EOF $ date Thu Jun 20 10:10:56 AM CST 2024 # 设置时区 $ timedatectl set-timezone Asia/Shanghai # 部署时间同步服务器，并设置开机自启 $ apt install ntpdate chrony -y $ ntpdate time2.aliyun.com $ systemctl start chrony $ systemctl status chrony $ systemctl enable chrony # 查看 chronyd 时间同步服务的源的统计信息 $ chronyc sourcestats -v k8s-node01 节点执行,修改 server 为 master 节点ip $ cp /etc/chrony/chrony.conf /etc/chrony/chrony.conf_bak $ cat \u003e /etc/chrony/chrony.conf \u003c\u003c EOF confdir /etc/chrony/conf.d server 192.168.1.11 iburst sourcedir /run/chrony-dhcp sourcedir /etc/chrony/sources.d keyfile /etc/chrony/chrony.keys driftfile /var/lib/chrony/chrony.drift ntsdumpdir /var/lib/chrony logdir /var/log/chrony maxupdateskew 100.0 rtcsync makestep 1 3 leapsectz right/UTC EOF $ date Thu Jun 20 10:15:56 AM CST 2024 # 设置时区 $ timedatectl set-timezone Asia/Shanghai # 部署时间同步服务器，并设置开机自启 $ apt install ntpdate chrony -y $ ntpdate time2.aliyun.com $ systemctl start chrony $ systemctl status chrony $ systemctl enable chrony # 查看 chronyd 时间同步服务的源的统计信息 $ chronyc sourcestats -v 1.3.5 配置内核转发及网桥过滤 # 转发IPv4并让iptables看到桥接流量 $ sudo cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF # 手动加载 $ modprobe overlay $ modprobe br_netfilter # 检测是否加载 $ lsmod | egrep \"overlay\" $ lsmod | egrep \"br_netfilter\" # 添加网桥过滤及内核转发配置,设置所需的sysctl参数，参数在重新启动后保持不变 $ sudo cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF #检查sysctl是否成功应用： $ sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward # 手动加载 #sudo sysctl -p /etc/sysctl.d/k8s.conf # or $ sudo sysctl --system 1.3.6 安装ipset和ipvsadm $ apt install ipset ipvsadm -y # 配置ipvsadm 内核模块加载 $ cat \u003c\u003c EOF \u003e /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF # 手动加载模块 $ modprobe -- ip_vs $ modprobe -- ip_vs_rr $ modprobe -- ip_vs_wrr $ modprobe -- ip_vs_sh $ modprobe -- nf_conntrack 1.3.7 关闭SWAP分区 k8s 集群所有节点 # 临时关闭 $ swapoff -a # 永久关闭 $ sed -i '/swap/ s/^\\(.*\\)$/#\\1/g' /etc/fstab # 验证是否禁用成功 swapon -s # 没有输出表示禁用成功 1.3.8 所有设备允许 root 用户 $ cat \u003e\u003e /etc/ssh/sshd_config \u003c\u003c EOF PermitRootLogin yes PasswordAuthentication yes EOF 1.3.9 关闭防火墙 #查看防火墙状态 $ sudo systemctl status ufw #关闭防火墙 $ sudo systemctl stop ufw #禁止防火墙开机启动 $ sudo systemctl disable ufw ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:3:3","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"2. 部署Conatinerd 服务 所有主机都要执行 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:4:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"2.1 下载及安装 # 部署 containerd $ wget https://github.com/containerd/containerd/releases/download/v1.7.11/cri-containerd-static-1.7.11-linux-amd64.tar.gz #tar Czxvf /usr/local/ cri-containerd-static-1.7.11-linux-amd64.tar.gz $ sudo tar -zxvf cri-containerd-cni-1.7.11-linux-amd64.tar.gz -C / 警告\r这里选择的适配k8s 容器运行时的containerd,有一个前缀cri(container runtime interface)，另外根据自己的操作系统选择对应的架构，常用的是amd64\r","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:4:1","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"2.2 配置Containerd服务 $ mkdir -p /etc/containerd $ containerd config default \u003e /etc/containerd/config.toml # 修改为国内代理镜像仓库 $ sed -i 's/registry.k8s.io\\/pause:3.8/registry.cn-hangzhou.aliyuncs.com\\/google_containers\\/pause:3.9/g' /etc/containerd/config.toml # 驱动使用为SystemdCgroup $ sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml $ sed -i 's/disabled_plugins/#disabled_plugins/g' /etc/containerd/config.toml ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:4:2","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"2.3 设置containerd开机自启动 $ systemctl enable --now containerd #验证版本 $ containerd --version ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:4:3","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"3. k8s集群部署 所有节点都执行 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:5:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"3.1 k8s配置集群软件apt源及安装 #下载用于 Kubernetes 软件包仓库的公共签名密钥。所有仓库都使用相同的签名密钥，因此你可以忽略URL中的版本 curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg # 添加 Kubernetes apt 仓库。 请注意，此仓库仅包含适用于 Kubernetes 1.28 的软件包 echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list # 更新 apt 包索引并安装使用 Kubernetes apt 仓库所需要的包 sudo apt-get update sudo apt-get install -y apt-transport-https ca-certificates curl gpg # k8s 集群软件安装 # 3.2.1 更新 apt 包索引，安装 kubelet、kubeadm 和 kubectl，并锁定其版本(防止自动更新)： $ sudo apt-get update $ apt-get install -y kubelet=1.28.11-00 kubeadm=1.28.11-00 kubectl=1.28.11-00 # 查看支持的版本，目前是每年会更新4个版本 $ apt-cache madison kubeadm # 并锁定其版本(防止自动更新) $ sudo apt-mark hold kubelet kubeadm kubectl ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:5:1","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"3.2 查看apt 仓库是否缓存k8s 安装组件 apt-cache policy kubeadm|head - 10 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:5:2","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"3.3 部署k8s 集群 master 节点 修改kubeadm 初始化配置文件 #默认初始化配置文件生成 $ kubeadm config print init-defaults \u003e kube-config.yaml # 修改配置文件 $ sed -i \"s/advertiseAddress: 1.2.3.4/advertiseAddress: 192.168.1.11/g\" kube-config.yaml $ sed -i \"s/name: node/name: k8s-master01/g\" kube-config.yaml $ sed -i \"s/imageRepository: k8s.gcr.io/imageRepository: $ registry.cn-hangzhou.aliyuncs.com\\/google_containers/g\" kube-config.yaml $ sed -i \"s/imageRepository: registry.k8s.io/imageRepository: $ registry.cn-hangzhou.aliyuncs.com\\/google_containers/g\" kube-config.yaml # 追加制定pod ip网段 $ sed -i '/serviceSubnet/a\\ podSubnet: 10.244.0.0/16' kube-config.yaml 添加ipvs 和 kubelet 配置,kube-config.yaml为生成的配置文件 $ cat \u003c\u003c EOF \u003e\u003e kube-config.yaml --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd EOF 提前下载需要的镜像信息 # 查看kubeadm 所需要的镜像列表 $ kubeadm config images list --kubernetes-version=v1.28.11 # 提前下载镜像,使用中国区镜像加速 $ kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers # 查看下载的镜像，镜像保存在k8s.io namesapce 下 $ ctr -n=k8s.io i ls 开始部署，并将log 打印到本地文件中 # 使用部署配置文件初始化k8s 集群 $ kubeadm init --config kube-config.yaml|tee -a kubeadm_init.log #输出内容如下： Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.1.11:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:fb0b8abf6780b7dff4f155edd2ed74a80a085081e25bb5469b906410a7a80398 执行成功后 根据提示还需要再执行以下命令 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config export KUBECONFIG=/etc/kubernetes/admin.conf 技巧\r执行失败查看报错后调整，使用kubeadm reset还原后重新执行\rmaster 节点验证 $ kubectl get node -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master01 Ready control-plane 7d12h v1.28.11 192.168.1.11 \u003cnone\u003e Ubuntu 22.04.4 LTS 5.15.0-94-generic containerd://1.7.11 kube-config.yaml 完成配置文件 apiVersion: kubeadm.k8s.io/v1beta3 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 192.168.1.11 # 修改内容 bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock # 修改内容 imagePullPolicy: IfNotPresent name: k8s-master01 # 修改内容 taints: null --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta3 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns: {} etcd: local: dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers # 修改内容 kind: ClusterConfiguration kubernetesVersion: 1.28.0 networking: dnsDomain: cluster.local serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 # 添加内容 scheduler: {} # 添加内容 --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs --- kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 cgroupDriver: systemd node 节点 join 这里手动指定cri-socket kubeadm join 192.168.1.11:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash \\ --cri-socket=unix:///run/containerd/containerd.sock ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:5:3","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"4. k8s集群部署网络插件 calico 常见的网络插件有多种，比如flannel、Calico和Cilium，本次实验使用Calico使用。 首先访问Calico帮助文档https://docs.tigera.io/calico/latest/getting-started/kubernetes/quickstart ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:6:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"master 节点 calico 部署使用的是operator 的方式，且不再和kube-system 共用一个namespace，另起了一个namesapce calico-system,引入了istio 相关组件服务，详情可以看下calico官网 配置插件需要的环境 wget https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/tigera-operator.yaml kubectl create -f tigera-operator.yaml 下载客户端资源文件并修改pod 网段地址，因为我们的pod的网段和配置文件不一样 # 下载客户端资源文件 curl -LO https://raw.githubusercontent.com/projectcalico/calico/v3.27.2/manifests/custom-resources.yaml # 修改pod的网段地址 podSubnet sed -i 's/cidr: 192.168.0.0/cidr: 10.244.0.0/g' custom-resources.yaml kubectl create -f custom-resources.yaml 查看集群 calico pod 创建过程，大约会耗时三分钟,部署成功后，所有node 节点为Ready 状态 watch kubectl get all -o wide -n calico-system 技巧\r以上方法是在dockerhub 网络仓库正常访问情况下执行的，如果dockerhub 网络仓库访问异常可以使用离线部署的方式 2024.6 月份docker hub 被墙了后，就不能方便的下载 docker 镜像了，这里使用离线的方式进行部署。 calico v3.27 相关镜像列表 docker.io/calico/cni:v3.27.2 docker.io/calico/kube-controllers:v3.27.2 docker.io/calico/node:v3.27.2 docker.io/calico/pod2daemon-flexvol:v3.27.2 docker.io/calico/typha:v3.27.2 # 下面三个镜像 release-v3.27.2 没有包含进来 docker.io/calico/csi:v3.27.2 docker.io/calico/apiserver:v3.27 docker.io/calico/node-driver-registrar:v3.27.2 手动镜像导入 sudo ctr -n k8s.io images ls|grep calico sudo ctr -n k8s.io images import calico-cni.tar sudo ctr -n k8s.io images import calico-dikastes.tar sudo ctr -n k8s.io images import calico-flannel-migration-controller.tar sudo ctr -n k8s.io images import calico-kube-controllers.tar sudo ctr -n k8s.io images import calico-node.tar sudo ctr -n k8s.io images import calico-pod2daemon.tar sudo ctr -n k8s.io images import calico-typha.tar # # 下面三个镜像 release-v3.27.2 没有包含进来,需要手动导入 sudo ctr -n k8s.io images import csi.tar sudo ctr -n k8s.io images import apiserver.tar sudo ctr -n k8s.io images import node-driver-registrar.tar 参考：calico_3.27 quick install 官网 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:6:1","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"5. 部署nginx 验证k8s 集群可用性 nginx-deploy yaml 文件 cat \u003c\u003c EOF \u003e nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-deploy name: nginx-deploy spec: replicas: 1 selector: matchLabels: app: nginx-deploy template: metadata: labels: app: nginx-deploy spec: containers: - image: registry.cn-shenzhen.aliyuncs.com/xiaohh-docker/nginx:1.25.4 name: nginx ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: labels: app: nginx-deploy name: nginx-svc spec: ports: - port: 80 protocol: TCP targetPort: 80 nodePort: 30080 selector: app: nginx-deploy type: NodePort EOF 生效 kubectl apply -f nginx-deploy.yaml # 然后访问你任何一个节点的IP地址加上nodeport 30080就可以访问这个部署的nginx kubectl get service -o wide 浏览器访问http://192.168.1.11:30080 提示 Welcome to nginx! 表示生效成功 ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:7:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"6. 部署crictl(optional) crictl 是Kubelet容器接口（CRI）的CLI和验证工具: 这里部署以下 containerd客户端crictl $ VERSION=\"v1.28.0\" $ wget https://github.com/kubernetes-sigs/cri-tools/releases/download/$VERSION/crictl-$VERSION-linux-amd64.tar.gz $ sudo tar zxvf crictl-$VERSION-linux-amd64.tar.gz -C /usr/local/bin $ rm -f crictl-$VERSION-linux-amd64.tar.gz $ cat \u003e /etc/crictl.yaml \u003c\u003cEOF runtime-endpoint: unix:///var/run/containerd/containerd.sock image-endpoint: unix:///var/run/containerd/containerd.sock timeout: 10 debug: true EOF # 测试 $ crictl pods $ crictl images 内容 docker.io/calico/apiserver v3.27.2 5d4a0194d5324 94MB docker.io/calico/cni v3.27.2 bbf4b051c5078 195MB docker.io/calico/csi v3.27.2 b2c0fe47b0708 17.4MB docker.io/calico/dikastes v3.27.2 236c3ef9745d9 40.5MB docker.io/calico/flannel-migration-controller v3.27.2 99b8d3ea2440b 125MB docker.io/calico/kube-controllers v3.27.2 849ce09815546 75.6MB docker.io/calico/node-driver-registrar v3.27.2 73ddb59b21918 22.6MB docker.io/calico/node v3.27.2 50df0b2eb8ffe 346MB docker.io/calico/pod2daemon-flexvol v3.27.2 ea79f2d96a361 15.4MB docker.io/calico/typha v3.27.2 2ec97bc370c17 68.4MB .... ","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:8:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"完整部署脚本 以作参考,环境不同，相应参数需要手动调整 #！/bin/bash # 本机名称 HOST_NAME=k8s-master01 # ipv4 地址 HOST_IP=192.168.1.11 HOST_NODE_IP=192.168.1.7 # 设置静态主机ip,手动处理 https://cloud.tencent.com/developer/article/1933335 function ch0_set_static_ip() { echo \"main ch0_set_static_ip start\" sudo apt install net-tools -y ifconfig route -n sudo cp /etc/netplan/00-installer-config.yaml /etc/netplan/00-installer-config.yaml_before cat \u003c\u003c EOF \u003e /etc/netplan/00-installer-config.yaml # This is the network config written by 'subiquity' network: ethernets: ens33: addresses: - 192.168.1.11/16 nameservers: addresses: - 114.114.114.114 - 8.8.8.8 search: [] routes: - to: default via: 192.168.1.1 version: 2 EOF # 使配置生效 sudo netplan apply echo \"main ch0_set_static_ip start\" } # 1、k8s 集群主机准备 function ch1_config_hosts(){ echo \"main ch1_config_hosts start\" # 部署文件目录 mkdir ${HOME}/kubernetes_install cd ${HOME}/kubernetes_install # 1.1 设置主机名称 hostnamectl set-hostname ${HOST_NAME} # 1.2 所有设备允许 root 用户 cat \u003e\u003e /etc/ssh/sshd_config \u003c\u003c EOF PermitRootLogin yes PasswordAuthentication yes EOF # 1.3 关闭swap 分区 swapoff -a sed -i '/swap/ s/^\\(.*\\)$/#\\1/g' /etc/fstab # 1.4 主机名和ip 地址解析配置 sed -i \"2a ${HOST_IP} ${HOST_NAME}\" /etc/hosts sed -i \"3a ${HOST_NODE_IP} k8s-node01\" /etc/hosts # 1.5 时间同步配置,时区 cp /etc/chrony/chrony.conf /etc/chrony/chrony.conf_bak cat \u003e /etc/chrony/chrony.conf \u003c\u003c EOF confdir /etc/chrony/conf.d server ntp.aliyun.com iburst allow 192.168.1.1/24 sourcedir /run/chrony-dhcp sourcedir /etc/chrony/sources.d keyfile /etc/chrony/chrony.keys driftfile /var/lib/chrony/chrony.drift ntsdumpdir /var/lib/chrony logdir /var/log/chrony maxupdateskew 100.0 rtcsync makestep 1 3 leapsectz right/UTC EOF timedatectl set-timezone Asia/Shanghai apt install ntpdate chrony -y ntpdate time2.aliyun.com systemctl start chrony systemctl status chrony systemctl enable chrony # 查看 chronyd 时间同步服务的源的统计信息 chronyc sourcestats -v # 1.6 配置内核转发及网桥过滤 # 转发IPv4并让iptables看到桥接流量 sudo cat \u003c\u003cEOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF # 手动加载 modprobe overlay modprobe br_netfilter # 检测是否加载 lsmod | egrep \"overlay\" lsmod | egrep \"br_netfilter\" # 添加网桥过滤及内核转发配置,设置所需的sysctl参数，参数在重新启动后保持不变 sudo cat \u003c\u003cEOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF #检查sysctl是否成功应用： sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward # 手动加载 #sudo sysctl -p /etc/sysctl.d/k8s.conf # or sudo sysctl --system # 1.7 安装ipset 及ipvsadm apt install ipset ipvsadm -y # 配置ipvsadm 内核模块加载 cat \u003c\u003c EOF \u003e /etc/modules-load.d/ipvs.conf ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh nf_conntrack EOF # 手动加载模块 modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack # 1.8 关闭防火墙 #查看防火墙状态 sudo systemctl status ufw #关闭防火墙 sudo systemctl stop ufw #禁止防火墙开机启动 sudo systemctl disable ufw echo \"main ch1_config_hosts end\" } # 2、k8s 集群 containerd 准备 function ch2_containerd_init(){ echo \"main ch2_containerd_init start\" # 部署 containerd wget https://github.com/containerd/containerd/releases/download/v1.7.11/cri-containerd-static-1.7.11-linux-amd64.tar.gz #tar Czxvf /usr/local/ cri-containerd-static-1.7.11-linux-amd64.tar.gz sudo tar -zxvf cri-containerd-cni-1.7.11-linux-amd64.tar.gz -C / mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml sed -i 's/registry.k8s.io\\/pause:3.8/registry.cn-hangzhou.aliyuncs.com\\/google_containers\\/pause:3.9/g' /etc/containerd/config.toml sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml sed -i 's/disabled_plugins/#disabled_plugins/g' /etc/containerd/config.toml # 使用systemd托管containerd # 生成system service文件 cat\u003c\u003cEOF|tee /etc/systemd/system/containerd.service [Unit] Description=containerd container runtime Documentation=https://containerd.io After=network.target local-fs.target [Service] ExecStartPre=-/sbin/modprobe overlay ExecStart=/usr/local/bin/containerd Type=notify Delegate=yes Ki","date":"2024-06-20","objectID":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/:9:0","tags":["Kubernetes"],"title":"Ubuntu-server部署k8s1.28(containerd版本)集群","uri":"/ubuntu-server%E9%83%A8%E7%BD%B2k8s1.28containerd%E7%89%88%E6%9C%AC%E9%9B%86%E7%BE%A4/"},{"categories":["Kubernetes"],"content":"前言 在了解RBAC 之前先了解一下Kubernetes API 对象，RBAC 最终操作的对象还是Kubernetes API 对象，本质还是对Kubernetes API的访问控制，如下RBAC 是在 Api Server 中是通过授权插件默认引入的。 /etc/kubernetes/manifests/kube-apiserver.yaml spec: containers: - command: - kube-apiserver - --advertise-address=192.168.1.11 - --secure-port=6443 # api server 监听的安全端口(https 协议) - --allow-privileged=true - --authorization-mode=Node,RBAC # 启用RBAC 和Node 授权插件 - --client-ca-file=/etc/kubernetes/pki/ca.crt # 启动x509 数字证书验证 - --enable-admission-plugins=NodeRestriction # 额外启用的准入控制器列表 - --enable-bootstrap-token-auth=true # 启用bootstrap 的token 认证 - --requestheader-extra-headers-prefix=X-Remote-Extra- # 代理认证相关的配置 - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key - ... - --service-account-key-file=/etc/kubernetes/pki/sa.pub # 指定用于签署承载令牌的秘钥文件 ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:1:0","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"Kubernetes API 概念 Kubernetes API 是通过 HTTP 提供的基于资源 (RESTful) 的编程接口。 它支持通过标准 HTTP 动词（POST、PUT、PATCH、DELETE、GET）检索、创建、更新和删除主要资源。 对于某些资源，API 包括额外的子资源，允许细粒度授权（例如：将 Pod 的详细信息与检索日志分开）， 为了方便或者提高效率，可以以不同的表示形式接受和服务这些资源。 ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:1:1","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"资源 URI /api/v1/namespaces /api/v1/pods /api/v1/namespaces/my-namespace/pods /apis/apps/v1/deployments /apis/apps/v1/namespaces/my-namespace/deployments /apis/apps/v1/namespaces/my-namespace/deployments/my-deployment kubectl get --raw命令查看 # kubectl get --raw / { \"paths\": [ \"/.well-known/openid-configuration\", \"/api\", \"/api/v1\", \"/apis\", \"/apis/\", \"/apis/admissionregistration.k8s.io\", \"/apis/admissionregistration.k8s.io/v1\", \"/apis/apiextensions.k8s.io\", ... } ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:1:2","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"启动代理 代理一个本地的api server 接口 # kubectl proxy Starting to serve on 127.0.0.1:8001 然后就可以直接访问api server 了，如新开一个会话查看集群中的daemonset 资源信息，这只是一个测试，正常的开发不会通过如下方式进行测试，而是有专门的sdk 如client-go curl http://127.0.0.1:8001/apis/apps/v1/daemonsets 查看kubenetes api 资源类型，资源类型是否namespaced 还是集群范围的 $ kubectl api-resources NAME SHORTNAMES APIVERSION NAMESPACED KIND bindings v1 true Binding componentstatuses cs v1 false ComponentStatus configmaps cm v1 true ConfigMap endpoints ep v1 true Endpoints events ev .... 下面就进入正体RBAC,以上的介绍就是为了说明RBAC 最终的操作还是kubernetes api 对象，当然也不止是kubernetes api 各类资源对象，还有非资源类型的URL. ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:1:3","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"RBAC 授权模型 kubernetes 系统的RBAC 授权插件将角色分为Role和ClusterRole两类，它们都是kubernetes 内置支持的API 资源类型，其中role 作用于名称空间级别，用于承载名称空间内的资源权限集合，而clusterrole 则能够同时承载名称空间和集群级别的资源权限集合。 利用Role 和ClusterRole 两类角色进行赋权时，需要用到另外两种资源RoleBinding和ClusterRoleBinding,它们同样是由Apiserver内置支持的资源类型。 ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:2:0","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"Kubernetes RBAC 是什么？ RBAC 是一个特定的权限管理模型，它把可以施加在\"资源对象\"上的\"动作’‘称为\"许可权限\",这些许可权限能按需组合在一起构建出\"角色\"及其职能，并通过为\"用户账户和组账户\"分配到一个到多个角色完成权限委派。这些能够发出动作的用户在RBAC中称为\"主体(subject)\" RBAC中的用户、角色、权限\r简单来说，RBAC 就是一种访问控制模型，它以角色为中心界定\"谁\"(subject)能够\"操作\"(verb)哪个或哪类\"对象\"(object)。 动作发出者即\"主体\"，通常以\"账号\"为载体，在kubernetes 中，它可以是普通账户(user),也可以是服务账户(service account).“动作\"表示要执行的具体操作，包括创建、删除、修改和查看等行为，对于API Server来说，即PUT、POST、DELETE、GET等请求方法。而\"对象\"则是指管理操作能够施加的目标实体，对 kubernetes API 来说主要指各类资源对象以及非资源类型URL. ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:2:1","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"为什么RBAC很重要，有什么优势? RBAC是基于角色的访问控制，是一种基于个人用户的角色来管理对计算机或网络资源的访问的方法。 相对于其他授权模式，RBAC具有如下优势： 对集群中的资源和非资源权限均有完整的覆盖。 整个RBAC完全由几个API对象完成， 同其他API对象一样， 可以用kubectl或API进行操作。 可以在运行时进行调整，无须重新启动API Server。 kubernetes 访问控制 用户、服务账户、认证、授权和准入控制\r","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:2:2","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"设计 RBAC Design\r","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:2:3","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"RBAC 基础概念术语 术语 Subjects: 主体，谁可以访问kubernetes API. Resources: 哪些 kubernetes API 资源对象可以被访问. Verbs: 可以对这些 Kubernetes API 做哪些操作(CRUD) 可以简单的说RBAC是一种身份和访问管理形式，它涉及一组权限或模板，这些权限或模板决定谁(Subjects)可以执行什么(Verbs)，以及在哪里(命名空间或集群)。 补充： Subject:主体，对应集群中尝试操作的对象，集群中定义了 3 种类型的主体资源: User Account:用户，这是有外部独立服务进行管理的，管理员进行私钥的分配，用户可以使用KeyStone 或者 Goolge 帐号，甚至一个用户名和密码的文件列表也可以。对于用户的管理集群内部没有一个关联的资源对象，所以用户不能通过集群内部的 API 来进行管理 Group:组，这是用来关联多个账户的，集群中有一些默认创建的组，比如 cluster-admin Service Account: 服务帐号 Service Account ServiceAccount 为 Pod 中运行的进程提供了一个身份，Pod 内的进程可以使用其关联服务账号的身份，向集群的 APIServer 进行身份认证。 通过下图可以了解Service Account 的位置和作用 Service Account\rRole 和 ClusterRole Kubernetes 系统的RBAC 授权插件将角色分为Role 和 ClusterRole 两类，它们都是kubernetes 内置支持的API 资源类型，其中Role 用作于空间级别，用于承载名称空间内的资源权限集合，而ClusterRole则能够同时承载名称空间和集群级别的资源权限集合。 Role 无法承载集群级别的资源类型的操作权限，这类的资源包括集群级别的资源(例如Node)、非资源类型的端点(例如/healthz),以及作用于所有名称空间的资源(例如跨名称空间获取任何资源的权限)等。 查看不属于任何特定的命名空间，而是在整个集群范围内管理和配置的API 资源对象 $ kubectl api-resources --namespaced=false Rule 规则，规则是一组属于不同 API Group 资源上的一组操作的集合,在Role 和 ClusterRole资源上定义的rule 也称为PolicyRule,即策略规则，它可以内嵌的字段有如下几个 apiGroups resources resourceNames nonResourceURLs verbs RoleBinding 和 ClusterRoleBinding 利用Role 和 ClusterRole 进行角色赋权时，需要用到另外两种资源RoleBinding和ClusterRoleBinding,它们同样是API Server 内置的支持的资源类型。 RoleBinding 用于将Role 绑定到一个或一组用户之上，它隶属于且仅能作用于其所在的单个名称空间。 RoleBinding 可以引用同一名称空间中的role,也可以引用集群级别的ClusterRole,但是引用ClusterRole的许可权限会降级到仅能在Rolebingding所在的名称空间生效。 ClusterRoleBinding 则用于将ClusterRole 绑定到用户和组，它作用于集群全局，且仅能够引用ClusterRole. 简单来说就是把声明的Subject 和我们的 Role 进行绑定的过程(给某个用户绑定上操作的权限)，二者的区别也是作用范围的区别:RoleBinding 只会影响到当前 namespace 下面的资源操作权限，而ClusterRoleBinding 会影响到所有的 namespace。 四者关系如图： Role、RoleBinding、ClusterRole和ClusterRoleBinding\rService Account 和 token 令牌 注：sa 是service account 的缩写 kubernetes \u003c= 1.20 版本 sa 创建后会自动创建一个secret,该secret 会包含一个token,这个token 就是serviceaccount的令牌，而且是永久有效的 kubernetes \u003e= 1.21 \u0026\u0026 \u003c= 1.23 版本 sa 创建后依然会自动创建一个secret,但是pod 里面使用的sa token不是该secret里面包含的令牌了，而是kubelet 去tokenrequest api 请求的有时间限制的token kubernetes \u003e= 1.24 sa 创建后不会自动创建一个secret 进行关联，然后和前面这个方式一样，通过tokenrequest api 请求token，每个一小时会自动刷新一次令牌，pod 容器内令牌路径为/var/run/secrets/kubernetes.io/serviceaccount/token $ kubectl exec -it nginx-deploy-5947669f9-9h9vg -- cat /var/run/secrets/kubernetes.io/serviceaccount/token eyJhbGciOiJSUzI1NiIsImtpZCI6IjlsalZmUVpQLXEzRjhLSk9hMnpOV0FKWEJMT2RGam15MDIyR1BlMXNkSjAifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzUyMjIxMTQ1LCJpYXQiOjE3MjA2ODUxNDUsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0IiwicG9kIjp7Im5hbWUiOiJuZ2lueC1kZXBsb3ktNTk0NzY2OWY5LTloOXZnIiwidWlkIjoiMThlODJmZTUtNzFjZS00ZTBmLWEwYTItNTNlYWJhYjRjNzgxIn0sInNlcnZpY2VhY2NvdW50Ijp7Im5hbWUiOiJkZWZhdWx0IiwidWlkIjoiNzFlYjM5MzEtMWU5Ny00MGI0LWFmNTItODdiZWYwZjQ2MjVhIn0sIndhcm5hZnRlciI6MTcyMDY4ODc1Mn0sIm5iZiI6MTcyMDY4NTE0NSwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6ZGVmYXVsdCJ9.0VZaQGqg1bCxzBIaRMXQ3doYWsu6g4PYsnTPoIDjCuF4IufFq-eG9YppkhjR21cfg-SJpV4h1maW1axB3yevpf9PYFxuEDRrLY4_qpkJW5UR-r_nz0D99iiQCisb2znrDSjChGFpUnStQ1TMgQfkejPgRh08RqKLNJE1QhQ6dncebALBmWeXEdi2vWkXUOT9KlvZxP7Bk2gYaQH39PhxiHb_b2nLSPwfiVNvK0Lpxe54oCQE4WH3qSWTg0GQ0nLn74E0fgSkyegBJhmPzyPcJ-iR8Je5qL6SKyNjHftu1m49q40RrNkeh0ogLBcD0qobpMWWw-XvtiK-bcwQt1qgLA $ 通过jwt令牌解析查看 Service Account 和 Token\r最新版本的令牌是一小时刷新一次，并且exp 是一年后到期，如果创建的ServiceAcccount 设置不自动挂载可以通过如下设置 apiVersion: v1 kind: ServiceAccount metadata: name: no-access-sa automountServiceAccountToken: false for pod apiVersion: v1 kind: Pod metadata: name: test-pod spec: serviceAccountName: no-access-sa automountServiceAccountToken: false ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:2:4","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"Kubernetes RBAC 实例 Create a Service Account $ kubectl create sa sa-demo serviceaccount/sa-demo created $ kubectl create sa sa2 serviceaccount/sa2 created 如果创建一个Pod 资源，不绑定serviceaccount 则使用namesapce命名空间下的默认的serviceaccount 也就是default $ kubectl get pod nginx-deploy-5947669f9-9h9vg -o yaml|grep -i serviceAccount - mountPath: /var/run/secrets/kubernetes.io/serviceaccount serviceAccount: default serviceAccountName: default Role and ClusterRole Role In RBAC 在default 名称空间中定义一个名称为pod-reader的Role 资源，且只能访问default名字空间下的secrets资源，对该资源只能做\"get”, “watch\"和\"list” 操作 pod-reader_role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: default name: pod-reader rules: - apigroups: [\"\"] # 表示核心API 群组 resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] # 也可以使用['*'] read_secret_role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: read-secret namespace: default rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] Cluster Role In RBAC Roles in RBAC 是用来访问指定某一个命名空间下的资源，如果想要提升到集群级别那么就需要使用ClusterRole secret-reader-clusterole.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\"] RoleBinding and ClusterRoleBinding Role Binding In RBAC Role Binding In RBAC\rread_pod_rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-pods namespace: default subjects: - kind: ServiceAccount name: sa-demo namespace: default roleRef: # \"roleRef\" 指定关联是 Role / ClusterRole kind: Role # 这里 必须指定 Role or ClusterRole name: pod-reader # 这里绑定你想匹配的 Role or ClusterRole 的名称 apiGroup: rbac.authorization.k8s.io read_secret_rolebinding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: read-secret namespace: default subjects: - kind: ServiceAccount name: sa2 namespace: default roleRef: kind: Role name: read-secret apiGroup: rbac.authorization.k8s.io Cluster Role Binding In RBAC Cluster Role Binding in RBAC 用来赋予subject 集群级别的权限，对所有的名字空间都有访问的权限 Cluster Role Binding In RBAC\rsecret-reader-ClusterRoleBinding.yaml apiVersion: rbac.authorization.k8s.io/v1 # 允许任何人在\"manager\"组中都可以访问任何namespace 下的secrets api 对象 kind: ClusterRoleBinding metadata: name: read-secrets-global subjects: - kind: ServiceAccount name: sa2 namespace: default roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io 权限检查 检查用户或者服务账户是否有对应的权限，可以使用命令kubectl auth can-i 例如： 检查 service account sa-demo 在namespace “default” 是否有查看secrets 的权限 $ kubectl auth can-i get secrets -n default --as=system:serviceaccount:default:sa-demo yes 名字空间下检查两个service account 的权限 $ kubectl auth can-i get pods -n default --as=system:serviceaccount:default:sa-demo yes $ kubectl auth can-i get secrets -n default --as=system:serviceaccount:default:sa-demo no $ kubectl auth can-i get pods -n default --as=system:serviceaccount:default:sa2 no $ kubectl auth can-i get secrets -n default --as=system:serviceaccount:default:sa2 yes 集群级别检查service account sa 的权限 $ kubectl auth can-i get secrets -n default --as=system:serviceaccount:default:sa2 $ kubectl auth can-i get secrets -n kube-system --as=system:serviceaccount:default:sa2 yes $ kubectl auth can-i get daemonsets -n kube-system --as=system:serviceaccount:default:sa2 no 只能访问某个 namespace 的普通用户 创建一个User Account，只能访问 kube-system 这个命名空间，对应的用户信息如下所示： username: lushuan group: ikubernetes 创建用户凭证 我们前面已经提到过，Kubernetes 没有 User Account 的 API 对象，不过要创建一个用户帐号的话也是挺简单的，利用管理员分配给你的一个私钥就可以创建了，这个我们可以参考官方文档中的方法，这里我们来使用 0penssL 证书来创建一个 User，当然我们也可以使用更简单的 cfssl 工具来创建:给用户 lushuan 创建一个私钥，命名成 lushuan.key: $ openssl genrsa -out lushuan.key 2048 使用我们刚刚创建的私钥创建一个证书签名请求文件:lushuan.csr，要注意需要确保在-subj参数中指定用户名和组(CN 表示用户名，0表示组): $ openssl req -new -key lushuan.key -out lushuan.csr -subj \"/CN=lushuan/O=ikubernetes\" 然后找到我们的 Kubernetes 集群的 CA 证书，我们使用的是 kubeadm 安装的集群，CA 相关证书位于/etc/kubernetes/pki/ 目录下面，如果你是二进制方式搭建的，你应该在最开始搭建集群的时候就已经指定好了 CA 的目录，我们会利用该目录下面的ca.crt和 ca.key两个文件来批","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:2:5","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"总结 Role 和 RoleBinding 用于同一个namespace RoleBindings 可以存在于不同的命名空间，以授予服务账户权限。 RoleBinding 可以引用同一名称空间中的role,也可以引用集群级别的ClusterRole,但是引用ClusterRole的许可权限会降级到仅能在Rolebingding所在的名称空间生效。 ClusterRoleBinding 则用于将ClusterRole 绑定到用户和组，它作用于集群全局，可以访问集群所有资源，但是仅能够引用ClusterRole. ClusterRoleBinding 不可以引用Role ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:3:0","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["Kubernetes"],"content":"参考 https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/ https://kubernetes.io/zh-cn/docs/reference/using-api/api-concepts/ https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/certificates/#openssl 什么是基于角色的访问控制（RBAC）？ RBAC权限系统分析、设计与实现 ","date":"2024-04-27","objectID":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/:4:0","tags":["Kubernetes"],"title":"一文了解 Kubernetes RBAC 机制和实践","uri":"/%E4%B8%80%E6%96%87%E4%BA%86%E8%A7%A3kubernetes-rbac/"},{"categories":["DevOps","English Article"],"content":"\rdevops\rIn recent years, DevOps has emerged as a critical discipline that merges software development (Dev) with IT operations (Ops). It aims to shorten the development lifecycle and provide continuous delivery with high software quality. As the demand for faster development cycles and more reliable software increases, professionals in the field constantly seek tools and practices to enhance their efficiency and effectiveness. Enter the realm of free and open-source tools – a goldmine for DevOps practitioners looking to stay ahead of the curve. This article is designed for DevOps professionals, whether you’re just starting or looking to refine your craft. We’ve curated a list of ten essential free and open-source tools that have proven themselves and stand out for their effectiveness and ability to streamline the DevOps process. These tools cover a range of needs, from continuous integration and delivery (CI/CD) to infrastructure as code (IaC), monitoring, and more, ensuring you have the resources to tackle various challenges head-on. Additionally, these tools have become essential for every DevOps engineer to know and use. Getting the hang of them can boost your career in the field. So, based on our hands-on industry experience, we’ve compiled this list for you. But before we go further, let’s address a fundamental question. Devops Periodic Table\r","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:0:0","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"What is DevOps? DevOps is a set of practices and methodologies that brings together the development (those who create software) and operations (those who deploy and maintain software) teams. Instead of working in separate silos, these teams collaborate closely throughout the entire software lifecycle, from design through the development process to production support. But what exactly does it mean, and why is it so important? Let’s break it down in an easy-to-understand way. Imagine you’re part of a team creating a puzzle. The developers are those who design and make the puzzle pieces, while the operations team is responsible for putting the puzzle together and making sure it looks right when completed. In traditional settings, these teams might work separately, leading to miscommunication, delays, and a final product that doesn’t fit perfectly. DevOps ensures everyone works together from the start, shares responsibilities, and communicates continuously to solve problems faster and more effectively. It’s the bridge that connects the creation and operation of software into a cohesive, efficient, and productive workflow. In other words, DevOps is the practice that ensures both teams are working in tandem and using the same playbook. The end game is to improve software quality and reliability and speed up the time it takes to deliver software to the end users. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:1:0","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"Key Components of DevOps Continuous Integration (CI): This practice involves developers frequently merging their code changes into a central repository, where automated builds and tests are run. The idea is to catch and fix integration errors quickly. Continuous Delivery (CD): Following CI, continuous delivery automates the delivery of applications to selected infrastructure environments. This ensures that the software can be deployed at any time with minimal manual intervention. Automation: Automation is at the heart of DevOps. It applies to testing, deployment, and even infrastructure provisioning, helping reduce manual work, minimize errors, and speed up processes. Monitoring and Feedback: Constant application and infrastructure performance monitoring is crucial. It helps quickly identify and address issues. Feedback loops allow for continuous improvement based on real user experiences. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:2:0","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"DevOps Life Cycle Grasping the various stages of the DevOps life cycle is key to fully comprehending the essence of the DevOps methodology. So, if you’re just entering this field, we’ve broken them down below to clarify things. DevOps Life Cycle\rPlan: In this initial stage, the team decides on the software’s features and capabilities. It’s like laying out a blueprint for what needs to be built. Code: Developers write code to create the software once the plan is in place. This phase involves turning ideas into tangible products using programming languages and tools. Build: After coding, the next step is to compile the code into a runnable application, which involves converting source code into an executable program. Test: Testing is crucial for ensuring the quality and reliability of the software. In this phase, automated tests are run to find and fix bugs or issues before the software is released to users. Deploy \u0026 Run: Once the software passes all tests, it’s time to release it and run it into the production environment where users can access it. Deployment should be automated for frequent and reliable releases with minimal human intervention. Monitor: Monitoring involves collecting, analyzing, and using data about the software’s performance and usage to identify issues, trends, or areas for improvement. Improve: The final stage closes the loop, where feedback from monitoring and end-user experiences is used to make informed decisions on future improvements or changes. However, to make this happen, we need specific software tools. The good news is that the top ones in the DevOps ecosystem are open-source! Here they are. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:3:0","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"Linux: The DevOps’ Backbone Linux\rWe’ve left Linux out of the numbered list below of the key DevOps tools, not because it’s unimportant, but because calling it just a ‘tool’ doesn’t do it justice. Linux is the backbone of all DevOps activities, making everything possible. In simple terms, DevOps as we know it wouldn’t exist without Linux. It’s like the stage where all the DevOps magic happens. We’re highlighting this to underline how crucial it is to have Linux skills if you’re planning to dive into the DevOps world. Understanding the basics of how the Linux operating system works is fundamental. Without this knowledge, achieving high expertise and success in DevOps can be quite challenging. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:0","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"1. Docker Docker\rDocker and container technology have become foundational to the DevOps methodology. They have revolutionized how developers build, ship, and run applications, unprecedentedly bridging the gap between code and deployment. Containers allow a developer to package an application with all of its needed parts, such as libraries and other dependencies, and ship it as one package. This consistency significantly reduces the “it works on my machine” syndrome, streamlining the development lifecycle and enhancing productivity. At the same time, the Docker containers can be started and stopped in seconds, making it easier to manage peak loads. This flexibility is crucial in today’s agile development processes and continuous delivery cycles, allowing teams to push updates to production faster and more reliably. Let’s not forget that containers also provide isolation between applications, ensuring that each application and its runtime environment can be secured separately. This helps minimize conflict between running applications and enhances security by limiting the surface area for potential attacks. Even though containers were around before Docker arrived, it made them popular and set them up as a key standard widely used in the IT industry. These days, Docker remains the top choice for working with containers, making it an essential skill for all DevOps professionals. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:1","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"2. Kubernetes Kubernetes\rWe’ve already discussed containers. Now, let’s discuss the main tool for managing them, known as the ‘orchestrator’ in the DevOps ecosystem. While there are other widely used alternatives in the container world, such as Podman, LXC, etc., when we talk about container orchestration, one name stands out as the ultimate solution – Kubernetes. As a powerful, open-source platform for automating the deployment, scaling, and management of containerized applications, Kubernetes has fundamentally changed how development and operations teams work together to deliver applications quickly and efficiently by automating the distribution of applications across a cluster of machines. It also enables seamless application scaling in response to fluctuating demand, ensuring optimal resource utilization and performance. Abstracting the complexity of managing infrastructure, Kubernetes allows developers to focus on writing code and operations teams to concentrate on governance and automation. Moreover, Kubernetes integrates well with CI/CD pipelines, automating the process from code check-in to deployment, enabling teams to release new features and fixes rapidly and reliably. In simple terms, knowing how to use Kubernetes is essential for every professional in the DevOps field. If you’re in this industry, learning Kubernetes is a must. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:2","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"3. Python Python\rAt the heart of DevOps is the need for automation. Python‘s straightforward syntax and extensive library ecosystem allow DevOps engineers to write scripts that automate deployments, manage configurations, and streamline the software development lifecycle. Its wide adoption and support have led to the development of numerous modules and tools specifically designed to facilitate DevOps processes. Whether it’s Ansible for configuration management, Docker for containerization, or Jenkins for continuous integration, Python serves as the glue that integrates these tools into a cohesive workflow, enabling seamless operations across different platforms and environments. Moreover, it is pivotal in the IaC (Infrastructure as Code) paradigm, allowing teams to define and provision infrastructure through code. Libraries such as Terraform and CloudFormation are often used with Python scripts to automate the setup and management of servers, networks, and other cloud resources. We can continue by saying that Python’s data analysis and visualization capabilities are invaluable for monitoring performance, analyzing logs, and identifying bottlenecks. Tools like Prometheus and Grafana, often integrated with Python, enable DevOps teams to maintain high availability and performance. Even though many other programming languages, such as Golang, Java, Ruby, and more, are popular in the DevOps world, Python is still the top choice in the industry. This is supported by the fact that, according to GitHub, the biggest worldwide code repository, Python has been the most used language over the past year. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:3","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"4. Git Git\rGit, a distributed version control system, has become indispensable for DevOps professionals. First and foremost, it enables team collaboration by allowing multiple developers to work on the same project simultaneously without stepping on each other’s toes. It provides a comprehensive history of project changes, making it easier to track progress, revert errors, and understand the evolution of a codebase. This capability is crucial for maintaining the speed and quality of development that DevOps aims for. Moreover, Git integrates seamlessly with continuous integration/continuous deployment (CI/CD) pipelines, a core component of DevOps practices. Understanding Git also empowers DevOps professionals to implement and manage code branching strategies effectively, such as the popular Git flow. A lot of what DevOps teams do begins with a simple Git command. It kicks off a series of steps in the CI/CD process, ultimately leading to a completed software product, a functioning service, or a settled IT infrastructure. In conclusion, for industry professionals, commands like “pull,” “push,” “commit,” and so on are the DevOps alphabet. Therefore, getting better and succeeding in this field depends on being skilled with Git. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:4","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"5. Ansible Ansible\rAnsible is at the heart of many DevOps practices, an open-source automation tool that plays a pivotal role in infrastructure as code, configuration management, and application deployment. Mastery of Ansible skills has become increasingly important for professionals in the DevOps field, and here’s why. Ansible allows teams to automate software provisioning, configuration management, and application deployment processes. This automation reduces the potential for human error and significantly increases efficiency, allowing teams to focus on more strategic tasks rather than repetitive manual work. One of Ansible’s greatest strengths is its simplicity. Its use of YAML for playbook writing makes it accessible to those who may not have a strong background in scripting or programming, bridging the gap between development and operations teams. In addition, what sets Ansible apart is that it’s agentless. This means there’s no need to install additional software on the nodes or servers it manages, reducing overhead and complexity. Instead, Ansible uses SSH for communication, further simplifying the setup and execution of tasks. It also boasts a vast ecosystem of modules and plugins, making it compatible with a wide range of operating systems, cloud platforms, and software applications. This versatility ensures that DevOps professionals can manage complex, heterogeneous environments efficiently. While it’s not mandatory for a DevOps engineer, like containers, Kubernetes, and Git are, you’ll hardly find a job listing for a DevOps role that doesn’t ask for skills to some level in Ansible or any of the other alternative automation tools, such as Chief, Puppet, etc. That’s why proficiency is more than advisable. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:5","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"6. Jenkins Jenkins\rJenkins is an open-source automation server that facilitates continuous integration and continuous delivery (CI/CD) practices, allowing teams to build, test, and deploy applications more quickly and reliably. It works by monitoring a version control system for changes, automatically running tests on new code, and facilitating the deployment of passing builds to production environments. Due to these qualities, just as Kubernetes is the go-to choice for container orchestration, Jenkins has become the go-to tool for CI/CD processes, automating the repetitive tasks involved in the software development lifecycle, such as building code, running tests, and deploying to production. By integrating with a multitude of development, testing, and deployment tools, Jenkins serves as the backbone of a streamlined CI/CD pipeline. It enables developers to integrate changes to the project and makes it easier for teams to detect issues early on. Proficiency in Jenkins is highly sought after in the DevOps field. As organizations increasingly adopt DevOps practices, the demand for professionals skilled in Jenkins and similar technologies continues to rise. Mastery of it can open doors to numerous opportunities, from roles focused on CI/CD pipeline construction and maintenance to DevOps engineering positions. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:6","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"7. Terraform / OpenTofu Terraform\rIn recent years, Terraform has emerged as a cornerstone for DevOps professionals. But what exactly is Terraform? Simply put, it’s a tool created by HashiCorp that allows you to define and provision infrastructure through code. It allows developers and IT professionals to define their infrastructure using a high-level configuration language, enabling them to script the setup and provisioning of servers, databases, networks, and other IT resources. By doing so, Terraform introduces automation, repeatability, and consistency into the often complex infrastructure management process. This approach, called Infrastructure as Code (IaC), allows infrastructure management to be automated and integrated into the development process, making it more reliable, scalable, and transparent. With Terraform, DevOps professionals can seamlessly manage multiple cloud services and providers, deploying entire infrastructures with a single command. This capability is crucial in today’s multi-cloud environments, as it ensures flexibility, avoids vendor lock-in, and saves time and resources. Moreover, it integrates very well with distributed version control systems such as Git, allowing teams to track and review changes to the infrastructure in the same way they manage application code. However, HashiCorp recently updated Terraform’s licensing, meaning it’s no longer open source. The good news is that the Linux Foundation has introduced OpenTofu, a Terraform fork that’s fully compatible and ready for production use. We highly recommend putting your time and effort into OpenTofu. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:7","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"8. Argo CD Argo\rArgo CD emerges as a beacon for modern DevOps practices. At its core, it’s a declarative GitOps continuous delivery tool for Kubernetes, where Git repositories serve as the source of truth for defining applications and their environments. When developers push changes to a repository, Argo CD automatically detects these updates and synchronizes the changes to the specified environments, ensuring that the actual state in the cluster matches the desired state stored in Git, significantly reducing the potential for human error. Mastery of Argo CD equips professionals with the ability to manage complex deployments efficiently at scale. This proficiency leads to several key benefits, the main one being enhanced automation. By tying deployments to the version-controlled configuration in Git, Argo CD ensures consistency across environments. Moreover, it automates the deployment process, reducing manual errors and freeing up valuable time for DevOps teams to focus on more strategic tasks. Furthermore, as applications and infrastructure grow, Argo CD’s capabilities allow teams to manage deployments across multiple Kubernetes clusters easily, supporting scalable operations without compromising control or security. For professionals in the DevOps field, mastering Argo CD means being at the forefront of the industry’s move towards more automated, reliable, and efficient deployment processes. And last but not least, investing efforts in mastering Argo CD can significantly boost your career trajectory. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:8","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"9. Prometheus Prometheus\rLet’s start with a short explanation of what Prometheus is and why it is crucial for DevOps professionals. Prometheus is an open-source monitoring and alerting toolkit that has gained widespread adoption for its powerful and dynamic service monitoring capabilities. At its core, it collects and stores metrics in real-time as time series data, allowing users to query this data using its PromQL language. This capability enables DevOps teams to track everything from CPU usage and memory to custom metrics that can be defined by the user, providing insights into the health and performance of their systems. How does it work? Prometheus works by scraping metrics from configured targets at specified intervals, evaluating rule expressions, displaying the results, and triggering alerts if certain conditions are met. This design makes it exceptionally well-suited for environments with complex requirements for monitoring and alerting. Overall, Prometheus is a critical skill for anyone in the DevOps field. Its ability to provide detailed, real-time insights into system performance and health makes it indispensable for modern, dynamic infrastructure management. As systems become complex, the demand for skilled professionals who can leverage Prometheus effectively will only increase, making it a key competency for any DevOps career path. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:9","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"10. Grafana Grafana\rGrafana allows teams to visualize and analyze metrics from various sources, such as Prometheus, Elasticsearch, Loki, and many others, in comprehensive, easy-to-understand dashboards. By transforming this numerical data into visually compelling graphs and charts, Grafana enables teams to monitor their IT infrastructure and services, providing real-time insights into application performance, system health, and more. But why are Grafana’s skills so crucial in the DevOps field? Above all, they empower DevOps professionals to keep a vigilant eye on the system, identifying and addressing issues before they escalate, ensuring smoother operations and better service reliability. Moreover, with Grafana, data from various sources can be aggregated and visualized in a single dashboard, making it a central monitoring location for all your systems. On top of that, Grafana’s extensive customization options allow DevOps professionals to tailor dashboards to their specific needs. This flexibility is crucial in a field where requirements can vary greatly from one project to another. In conclusion, mastering Grafana equips DevOps professionals with the skills to monitor, analyze, and optimize their systems effectively. Because of this, the ability to harness Grafana’s power will continue to be a valuable asset in any DevOps professional’s toolkit. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:4:10","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["DevOps","English Article"],"content":"Conclusion This list, reflects the industry’s best practices and highlights the essential tools that every DevOps engineer should be familiar with to achieve professional success in today’s dynamic technological landscape. Whether you are a newcomer or an experienced professional looking to enhance your skills, mastering these tools can significantly boost the trajectory of your DevOps career. ","date":"2024-02-17","objectID":"/10-must-know-free-devops-tools-for-professional-success/:5:0","tags":["DevOps","English Article"],"title":"10 Must-Know Free DevOps Tools for Professional Success","uri":"/10-must-know-free-devops-tools-for-professional-success/"},{"categories":["Kubernetes"],"content":"K8S 之连接异常——【集群故障】 Pod问题思考思路 Kubernetes集群中Pod一般哪些方面容易出现问题呢？ Kubernetes资源配置错误：例如在部署Deployment和Statefulset时，资源清单书写有问题，导致Pod无法正常创建 代码问题：应用程序代码在容器启动后失败，那就需要通过排查代码查找错误 网络问题：网络插件部署有问题，导致Pod启动之后无法相互通信 存储问题：Pod挂载存储，但是需要共享的存储连接不上导致Pod启动异常 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:0","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod 状态异常排查问题集 Running Pending Waiting ContainerCreating ImagePullBackOff CrashLoopBackOff Error Terminating Unknown Evicted 如何查看pod状态： kubectl get pods \u003cpod name\u003e #查看pod运行状态 kubectl describe pods \u003cpod name\u003e #查看pod详细信息 Pod状态异常排查思路 一般来说，无论 Pod 处于什么异常状态，都可以执行以下命令来查看 Pod 的状态： 查看Pod的配置是否正确 kubectl get pod \u003cpod-name\u003e -o yaml 查看 Pod 的详细信息 kubectl describe pod \u003cpod-name\u003e 查看Pod里对应容器的日志 kubectl logs \u003cpod-name\u003e [-c \u003ccontainer-name\u003e] ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:1","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"部分节点无法启动Pod：ContainerCreating状态 诊断方法 查看pod kubectl get pods -o wide | grep tomcat NAME READY STATUS RESTARTS AGE IP NODE tomcat 0/1 ContainerCreating 0 106s 10.244.1.4 k8s-node1 查看pod 详情信息 kubectl describe pods tomcat 显示 error: code = Unknown desc = failed to set up sandbox container \"3f25ce8c6599a8b7824f8f9ba9cf2c078d9496dd840fb2305bf415929879f107\" network for pod “tomcat\": NetworkPlugin cni failed to set up pod “tomcat\" network: failed to set bridge addr: \"cni0\" already has an IP address different from 10.244.1.4/24 总结：pod 异常需要通过 describe 查看具体报错信息，根据具体报错信息再进行处理，以上是遇到同类问题的一种排查思路，问题各种各样，排查问题的思路确是可以有迹可循的 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:2","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod状态异常排查问题集-pending状态排查思路 Pending是挂起状态：表示创建的Pod找不到可以运行它的物理节点，不能调度到相应的节点上运行，那么这种情况如何去排查呢？可以从两个层面分析问题： 1. 物理节点层面分析 查看节点资源使用情况：如free -m查看内存、top查看CPU使用率、df –h查看磁盘使用情况，这样就可以快 速定位节点资源情况，判断是不是节点有问题 查看节点污点：kubectl describe nodes master1（控制节点名字），如果节点定义了污点，那么Pod不能容 忍污点，就会导致调度失败，如下： Taints: node-role.kubernetes.io/master:NoSchedule 这个看到的是控制节点的污点，表示不允许Pod调度（如果Pod定义容忍度，则可以实现调度） **1. Pod 本身分析 ** 在定义pod时，如果指定了nodeName是不存在的，那也会调度失败 在定义Pod时，如果定义的资源请求比较大，导致物理节点资源不够也是不会调度的 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:3","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod状态异常排查问题集- ImagePullBackOff状态 问题分析 拉取镜像时间长导致超时 配置的镜像有错误：如镜像名字不存在等 配置的镜像无法访问：如网络限制无法访问hub上的镜像 配置的私有镜像仓库参数错误：如imagePullSecret没有配置或者配置错误 dockerfile打包的镜像不可用 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:4","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod状态异常排查问题集- CrashLoopBackOff状态 K8s中的pod正常运行，但是里面的容器可能退出或者多次重启或者准备删除，导致出现这个状态，这个状态具有偶发性，可能上一秒还是running状态，但是突然就变成CrashLoopBackOff状态了。 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:5","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod状态异常排查问题集- Error状态 这种状态主要是Pod启动过程会出现： 原因分析： 1）依赖的ConfigMap、Secret、PV、StorageClass等不存在； 2）请求的资源超过了管理员设置的限制，比如超过了limits等； 3）无法操作集群内的资源，比如开启RBAC后，需要为ServiceAccount配置权限。 排查思路： # 1. describe #查看pod详细信息 kubectl describe pods \u003cpod\u003e # 2. logs #查看pod日志 kubectl logs \u003cpod\u003e -c \u003ccontainer\u003e # 3. journalctl #查看kubelet日志 journalctl -f -u kubelet # 4. tail –f #查看主机日志 tailf /var/log/messages ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:6","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod状态异常排查问题集- Terminating或Unknown状态 原因：Terminated是因为容器里的应用挂了。 从k8s v1.5开始，Kubernetes不会因为Node失联而删除其上正在运行的Pod，而是将其标记为Terminating 或 Unknown 状态。想要删除这些状态的Pod有 3 种方法： 手动删除node: 手动删除kubectl delete nodes node节点名字 恢复正常可选择性删除: 若node恢复正常，kubelet会重新跟kube-apiserver通信 确认这些Pod的期待状态，进而再决定删除或者继续运行这些Pod，如果删除可以通过kubectl delete pods pod-name –grace-period=0 –force强制删除 使用参数重建容器 Pod行为异常，Pod没有按预期的行为执行，比如没有运行podSpec里面设置的命令行参数，这一般是podSpec yaml文件内容有误，可以尝试使用 validate 参数重建容器， 比如（kubectl delete pod mypod 和 kubectl create – validate -f mypod.yaml）；也可以查看创建后的 podSpec 是否是对的，比如（kubectl get pod mypod -o yaml） Unknown是pod状态不能通过kubelet汇报给apiserver，这很有可能是主从节点（Master和Kubelet）间的通信出现了问题。 解决思路: 查看kubectl 状态 查看节点状态 查看apiserver是否异常 查看 kubelet 日志 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:7","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"Pod健康检查：存活性探测\u0026就绪性探测 为什么需要探针？ 如果没有探针，k8s无法知道应用是否还活着，只要pod还在运行，k8s则认为容器是健康的。但实际上，pod虽然运行了，但里面容器中的应用可能还没提供服务，那就需要做健康检查，健康检查就需要探针，常见的探针有如下几种： httGet 对容器的ip地址(指定的端口和路径)执行http get请求 如果探测器收到响应，并且响应码是2xx,则认为探测成功。如果服务器没有响应或者返回错误响 应码则说明探测失败，容器将重启。 tcpSock 探针与容器指定端口建立tcp连接，如果连接建立则探测成功，否则探测失败容器重启。 exec 在容器内执行任意命令，并检查命令退出状态码，如果状态码为0，则探测成功，否则探测失败 容器重启 LivenessProbe和ReadinessProbe区别： LivenessProbe决定是否重启容器 ReadinessProbe主要来确定容器是否已经就绪： 只有当Pod里的容器部署的应用都处于就绪状态，也就是pod的Ready为true（1/1）时，才会将请求转发给容器。 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:8","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"coredns或者kube-dns经常重启和报错 问题描述： 在k8s中部署服务，服务之间无法通过dns解析，coredns一直处于CrashLoopBackOff状态 查看日志，报错如下： plugin/loop: Loop (127.0.0.1:44222 -\u003e :53) detected for zone 解决思路： coredns主要和resolv.conf打交道，会读取宿主机的/etc/resolv.conf中的nameserver内容，查询 resolv.conf文件，如果里面存在本地回环如127.0.0.1或者127.0.0.53那么就容易造成死循环 解决步骤： 修改resolv.conf文件： 将nameserver临时修改为114.114.114.114之后：kubectl edit deployment coredns -n kube-system 将replicates改为0，从而停止已经启动的coredns pod 再将replicates改为2， 这样会触发coredns重新读取系统配置，此时服务的状态为Running ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:9","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"kubectl执行异常：无权限问题定位 描述 $ kubectl get node \u003e The connection to the server localhost:8080 was refused - did you specify the right host or port? 这是因为安装k8s之后，默认是没有权限操作k8s资源的，可用如下方法解决 mkdir -p $HOME/.kube sudo cp -ifr /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:1:10","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"K8S 之统信异常——【网络故障】 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:2:0","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"网络故障原因 Kubernetes中的网络是非常重要的，整个k8s集群的通信调度都是通过网络插件实现的，kubernetes之上的网路通信主要有如下几种： 容器间的通信: 同一个pod内的多个容器间的通信，lo pod 通信： 从一个pod Ip到另一个pod Ip Pod与service通信： pod ip 到 cluster ip 外部通信： Service与集群外部客户端的通信 通信需要依赖网络插件，如calico、flannel、canel等，通信过程最常见的故障如下 Pod 自身网络故障 Pod 间通信故障 Pod 和物理节点通信故障 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:2:1","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"K8S 之内部异常——【节点故障】 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:3:0","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"节点异常1: 集群节点NotReady 一般是该节点上的kubelet 挂掉了，查看kubelet 状态 systemctl status kubelet 如果挂掉需要重启一下kubelet 扩展：为什么要重启kubelet？ 监视分配给该Node节点的pods 挂载pod所需要的volumes 下载pod的secret 通过docker/rkt来运行pod中的容器 周期的执行pod中为容器定义的liveness探针 上报pod的状态给系统的其他组件 上报Node的状态 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:3:1","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"节点异常2: pod调度失败故障排查 在k8s集群中，pod是最小单元，需要调度到具体的物理节点上运行，那么为了更好排查问题，了解pod的调度流程是重中之重，接下来介绍下pod工作和调度流程： 创建pod过程： 1、用户创建pod的信息通过API Server存储到etcd中，etcd记录pod的元信息并将结果返回API Server 2、API Server告知调度器请求资源调度分配，调度器通过计算，将优先级高的node与pod绑定并告知API Server 3、API Server将此信息写入etcd，得到etcd回复后调用kubelet创建pod 4、kubelet使用docker run创建pod内的容器，得到反馈信息后将容器信息告知API Server 5、API Server将收到的信息写入etcd并得到回馈 6、此时使用kubectl get pod就可以查看到信息了 调度方式： 1、当我们发送请求创建Pod，调度器就会把pod调度到合适的物理节点，大致分为以下过程： Scheduler根据预选策略和优选函数，选额合适的noed节点调度 2、可以通过定义nodeName和nodeSelector进行调度 3、可以通过节点亲和性进行调度 Pod在调度的时候会找到一个合适的节点，所以如果节点资源不足；pod定义了指定的节点，但是指定的节点出现故障，或者pod定义了亲和性，但是节点没有满足的条件，都会导致调度失败。 排查思路 describe pod 查看是否节点加了污点，pod 无法容忍，当然还要根据报错信息进行定位 查看目标节点 kubelet 服务的状态 pod调度失败内置污点 当某些条件为真时，节点控制器会自动为节点添加污点： node.kubernetes.io/ NoSchedule：如果一个pod没有声明容忍这个Taint，则系统不会把该Pod调度到有这个Taint的node上 node.kubernetes.io/NoExecute：定义pod的驱逐行为，以应对节点故障。 node.kubernetes.io/not-ready：节点尚未准备好。这对应于NodeConditionReady为False。 node.kubernetes.io/unreachable：无法从节点控制器访问节点。这对应于NodeConditionReady为Unknown。 node.kubernetes.io/out-of-disk：节点磁盘不足。 node.kubernetes.io/memory-pressure：节点有内存压力。 node.kubernetes.io/disk-pressure：节点有磁盘压力。 node.kubernetes.io/network-unavailable：节点的网络不可用。 node.kubernetes.io/unschedulable：节点不可调度。 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:3:2","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"节点资源不足：Pod超过节点资源限制 Pod超过节点资源限制分两种情况： 情况一：Pod数量太多超过物理节点限制，较少见 情况二：Pod资源超过物理节点资源限制，相对频繁一些，requests 资源申请过多 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:3:3","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"K8S之内部异常 【节点故障】 排查思路总结 节点状态查询：kubectl get nodes 常见异常现象：1、节点状态NotReady 2、调度到该节点的pod显示Unkonwn、Pending等 常见故障：kubelet进程异常、未安装cni网络插件、docker异常、磁盘空间不足、内存不足、cpu不足 排查排查： kubectl describe nodes 查看node节点信息 kubectl describe pods \u003cpod 名字\u003e 查看pod节点信息 systemctl status 查看kubelet状态 Journalctl查看系统日志 free查看内存 top查看cpu ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:3:4","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"K8S 之应用故障——【应用故障】 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:4:0","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"故障排查：Service访问异常 Service是什么？为什么容易出现问题？ 在kubernetes中，Pod是有生命周期的，如果Pod重启IP很有可能会发生变化。如果我们的服务都是将Pod的IP地址写死，Pod的挂掉或者重启，和刚才重启的pod相关联的其他服务将会找不到它所关联的Pod，为了解决这个问题，在kubernetes中定义了service资源对象，Service 定义了一个服务访问的入口，客户端通过这个入口即可访问服务背后的应用集群实例，Service是一组Pod的逻辑集合，这一组 Pod 能够被 Service 访问到，通常是通过 Label Selector实现的。 service label selector\r情况一： service 和 pod labels 没有匹配 情况二：service 和 pod 正确关联，kube-proxy 异常 kube-proxy 导致Service 访问异常 kube-proxy 是什么？ service 为一组相同的 pod 提供了统一的入口，起到负载均衡的效果。service 具有这样的功能，正是 kube-proxy 的功劳，kube-proxy是一个代理，安装在每一个k8s节点，当我们暴露一个 service 的时候，kube-proxy 会在iptables中追加一些规则，为我们实现路由与负载均衡的功能。 背景：查看 Service、pod、endpoint 是否正常关联 $kubectl describe svc myapp-v1-service Name: myapp-v1-service Namespace: default Selector: app=myapp,version=v1 Type: NodePort IP: 10.98.1.137 Port: \u003cunset\u003e 80/TCP TargetPort: 80/TCP NodePort: \u003cunset\u003e 30080/TCP Endpoints: 10.244.1.233:80,10.244.1.234:80 查看iptables 是否正常创建规则 iptables -t nat -L|grep 10.98.1.137 KUBE-MARK-MASQ tcp -- !10.244.0.0/16 10.98.1.137 /* default/myapp-v1-service cluster IP */ tcp dpt:http KUBE-SVC-J2PKJ4NHOU6XHVIG tcp -- anywhere 10.98.1.137 /* default/myapp-v1-service cluster IP */ tcp dpt:http 通过上面可以看到之前创建的service，会通过kube-proxy在iptables中生成一个规则，来实现流量路由。 通过上面可以看到，有一系列目标为 KUBE-SVC-xxx 链的规则，每条规则都会匹配某个目标 ip 与端口。也就是说访问某个 ip:port 的请求会由 KUBE-SVC-xxx 链来处理。这个目标 IP 其实就是service ip。 $ iptables -t nat -L|grep KUBE-SVC-J2PKJ4NHOU6XHVIG KUBE-SVC-J2PKJ4NHOU6XHVIG tcp -- anywhere anywhere /* default/myapp-v1-service */ tcp dpt:30080 KUBE-SVC-J2PKJ4NHOU6XHVIG tcp -- anywhere 10.98.1.137 /* default/myapp-v1-service cluster IP */ tcp dpt:http $ iptables -t nat -L|grep KUBE-MARK-MASQ | grep 10.244.1.233 KUBE-MARK-MASQ all -- 10.244.1.233 anywhere /* default/myapp-v1-service */ $ iptables -t nat -L|grep KUBE-MARK-MASQ | grep 10.244.1.234 KUBE-MARK-MASQ all -- 10.244.1.234 anywhere /* default/myapp-v1-service */ 通过上面分析可以知道，kube-proxy在service请求中起到流量路由转发的作用： kube-proxy 会为我们暴露的service追加iptables规则，具体流程如下： 所有进出请求都会经过 KUBE-SERVICES 链 KUBE-SERVICES 链有我们发布的每一个服务，每个服务的链会对应到 DNAT 到 service 的 endpoint KUBE-SERVICES 链最后的是 KUBE-NODEPORTS 链，会匹配请求本主机的一些端口，这就是我们通过 NODEPORT 类型发布服务能在集群外部访问的原因。 经过上面的分析，service和pod正确关联，pod也处于运行状态，那请求service还是访问不到pod，90%就是kube-proxy引起的。查看kube-proxy是否在机器上正常运行。 kube-proxy如果在节点上运行，下一步，确认它有没有出现其他异常，比如连接主节点失败。要做到这一点，必须查看日志：如 /var/log/messages或者/var/log/kube-proxy.log(二进制安装的时候可以指定的)，也使用 journalctl 访问日志、kube-proxy如果是在k8s中以pod形式运行的，可以通过kubectl logs 查看日志。 根据日志报错信息再进行进一步的排查 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:4:1","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"故障排查：Pod删除失败 故障描述：在k8s中，可能会产生很多垃圾pod，也就是有些pod虽然是running状态，可是其所调度到的k8s node节点已经从k8s集群删除了，但是pod还是在这个node上，没有被驱逐，针对这种情况就需要把pod删除。 删除pod的几种方法： 删除pod所在的namespace 如果pod是通过控制器管理的，可以删除控制器资源 如果pod是自主式管理，直接删除pod资源就可以了 但是我们常常遇见这样一种情况，我们在删除pod资源的时候，pod会一直terminate状态，很长时间无法删除 解决方案： 可以通过如下方法强制删除： 加参数 --force --grace-period=0，grace-period表示过渡存活期，默认 30s，在删除POD之前允许POD慢慢终止其上的容器进程，从而优雅退出，0表示立即终止POD kubectl delete pod \u003cpod名字\u003e -n --force --grace-period=0 ","date":"2024-01-13","objectID":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/:4:2","tags":["Kubernetes排障"],"title":"k8s 常见故障和解决思路及方法","uri":"/k8s-%E5%B8%B8%E8%A7%81%E6%95%85%E9%9A%9C%E5%92%8C%E8%A7%A3%E5%86%B3%E6%80%9D%E8%B7%AF%E5%8F%8A%E6%96%B9%E6%B3%95/"},{"categories":["Kubernetes"],"content":"\rKubernetes 技能图谱\r","date":"2023-10-13","objectID":"/kubernetes-%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/:0:0","tags":["Kubernetes"],"title":"Kubernetes 技能图谱","uri":"/kubernetes-%E6%8A%80%E8%83%BD%E5%9B%BE%E8%B0%B1/"},{"categories":["Kubernetes"],"content":"问题描述 问题\r在高可用的k8s集群中，当Node节点挂掉，kubelet无法提供工作的时候，pod将会自动调度到其他的节点上去， 而调度到节点上的时间需要我们慎重考量，因为它决定了生产的稳定性、可靠性，更快的迁移可以减少我们业务的影响性， 但是有可能会对集群造成一定的压力，从而造成集群崩溃。\r","date":"2023-09-09","objectID":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/:1:0","tags":["Kubernetes"],"title":"调整节点pod驱逐时间","uri":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/"},{"categories":["Kubernetes"],"content":"Kubelet 状态更新的基本流程 kubelet 自身会定期更新状态到 apiserver，通过参数–node-status-update-frequency指定上报频率，默认是 10s 上报一次。 kube-controller-manager 会每隔–node-monitor-period时间去检查 kubelet 的状态，默认是 5s。 当 node 失联一段时间后，kubernetes 判定 node 为 notready 状态，这段时长通过–node-monitor-grace-period参数配置，默认 40s。 当 node 失联一段时间后，kubernetes 判定 node 为 unhealthy 状态，这段时长通过–node-startup-grace-period参数配置，默认 1m0s。 当 node 失联一段时间后，kubernetes 开始删除原 node 上的 pod，这段时长是通过–pod-eviction-timeout参数配置，默认 5m0s。 kube-controller-manager 和 kubelet 是异步工作的，这意味着延迟可能包括任何的网络延迟、apiserver 的延迟、etcd 延迟，一个节点上的负载引起的延迟等等。因此，如果–node-status-update-frequency设置为5s，那么实际上 etcd 中的数据变化会需要 6-7s，甚至更长时间。 如果业务生产认为默认的驱逐时间不能满足业务的及时性，可以手动修改驱逐时间。 在master节点操作： 修改/etc/systemd/system/kube-controller-manager.service文件 增加一行–pod-eviction-timeout=1m （这里是调整为1分钟，具体数值可以根据业务重要实际情况修改） ","date":"2023-09-09","objectID":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/:2:0","tags":["Kubernetes"],"title":"调整节点pod驱逐时间","uri":"/%E8%B0%83%E6%95%B4%E8%8A%82%E7%82%B9pod%E9%A9%B1%E9%80%90%E6%97%B6%E9%97%B4/"},{"categories":["docker"],"content":"CGroups与Namespaces 摘要\r了解下容器背后的两大核心技术：CGroups 和 Namespace。\r","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:0:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"CGroups 概述 CGroups 全称为 Linux Control Group，其作用是限制一组进程使用的资源（CPU、内存等）上限，CGroups 也是 Containerd 容器技术的核心实现原理之一，首先我们需要先了解几个 CGroups 的基本概念： Task: 在 cgroup 中，task 可以理解为一个进程，但这里的进程和一般意义上的操作系统进程不太一样，实际上是进程 ID 和线程 ID 列表。 CGroup: 即控制组，一个控制组就是一组按照某种标准划分的 Tasks，可以理解为资源限制是以进程组为单位实现的，一个进程加入到某个控制组后，就会受到相应配置的资源限制。 Hierarchy: cgroup 的层级组织关系，cgroup 以树形层级组织，每个 cgroup 子节点默认继承其父 cgroup 节点的配置属性，这样每个 Hierarchy 在初始化会有 root cgroup。 Subsystem: 即子系统，子系统表示具体的资源配置，如 CPU 使用，内存占用等，Subsystem 附加到 Hierarchy 上后可用 CGroups 支持的子系统包含以下几类，即为每种可以控制的资源定义了一个子系统: cpuset: 为 cgroup 中的进程分配单独的 CPU 节点，即可以绑定到特定的 CPU cpu: 限制 cgroup 中进程的 CPU 使用份额 cpuacct: 统计 cgroup 中进程的 CPU 使用情况 memory: 限制 cgroup 中进程的内存使用,并能报告内存使用情况 devices: 控制 cgroup 中进程能访问哪些文件设备(设备文件的创建、读写) freezer: 挂起或恢复 cgroup 中的 task net_cls: 可以标记 cgroups 中进程的网络数据包，然后可以使用 tc 模块(traffic contro)对数据包进行控制 blkio: 限制 cgroup 中进程的块设备 IO perf_event: 监控 cgroup 中进程的 perf 时间，可用于性能调优 hugetlb: hugetlb 的资源控制功能 pids: 限制 cgroup 中可以创建的进程数 net_prio: 允许管理员动态的通过各种应用程序设置网络传输的优先级 通过上面的各个子系统，可以看出使用 CGroups 可以控制的资源有: CPU、内存、网络、IO、文件设备等。CGroups 具有以下几个特点： CGroups 的 API 以一个伪文件系统（/sys/fs/cgroup/）的实现方式，用户的程序可以通过文件系统实现 CGroups 的组件管理 CGroups 的组件管理操作单元可以细粒度到线程级别，用户可以创建和销毁 CGroups，从而实现资源载分配和再利用 所有资源管理的功能都以子系统（cpu、cpuset 这些）的方式实现，接口统一子任务创建之初与其父任务处于同一个 CGroups 的控制组 我们可以通过查看 /proc/cgroups 文件来查找当前系统支持的 CGroups 子系统: $ cat /proc/cgroups #subsys_name hierarchy num_cgroups enabled cpuset 5 153 1 cpu 8 478 1 cpuacct 8 478 1 memory 3 478 1 devices 9 478 1 freezer 4 153 1 net_cls 6 153 1 blkio 10 478 1 perf_event 2 153 1 hugetlb 7 153 1 pids 11 478 1 net_prio 6 153 在使用 CGroups 时需要先挂载，我们可以使用 df -h | grep cgroup 命令进行查看: $ df -h | grep cgroup tmpfs 7.8G 0 7.8G 0% /sys/fs/cgroup 可以看到被挂载到了 /sys/fs/cgroup，cgroup 其实是一种文件系统类型，所有的操作都是通过文件来完成的，我们可以使用 mount –type cgroup命令查看当前系统挂载了哪些 cgroup： $ mount --type cgroup cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_prio,net_cls) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct,cpu) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids) /sys/fs/cgroup 目录下的每个子目录就对应着一个子系统，cgroup 是以目录形式组织的，/ 是 cgroup 的根目录，但是这个根目录可以被挂载到任意目录，例如 CGroups 的 memory 子系统的挂载点是 /sys/fs/cgroup/memory，那么 /sys/fs/cgroup/memory/ 对应 memory 子系统的根目录，我们可以列出该目录下面的文件： $ ll /sys/fs/cgroup/memory/ total 0 -rw-r--r-- 1 root root 0 Aug 8 2022 cgroup.clone_children --w--w--w- 1 root root 0 Aug 8 2022 cgroup.event_control -rw-r--r-- 1 root root 0 Aug 8 2022 cgroup.procs -r--r--r-- 1 root root 0 Aug 8 2022 cgroup.sane_behavior drwxr-xr-x 5 root root 0 Aug 28 14:38 docker drwxr-xr-x 5 root root 0 Sep 6 14:01 kubepods -rw-r--r-- 1 root root 0 Aug 8 2022 memory.failcnt --w------- 1 root root 0 Aug 8 2022 memory.force_empty -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.failcnt -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.limit_in_bytes -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.max_usage_in_bytes -r--r--r-- 1 root root 0 Aug 8 2022 memory.kmem.slabinfo -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.failcnt -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.limit_in_bytes -rw-r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.max_usage_in_bytes -r--r--r-- 1 root root 0 Aug 8 2022 memory.kmem.tcp.usage_in_bytes -r--r--r-- 1 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:1:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"CGroup 测试 接下来我们来尝试手动设置下 cgroup，以 CPU 这个子系统为例进行说明，首先我们在 /sys/fs/cgroup/cpu 目录下面创建一个名为 ydzs.test 的目录： $ mkdir -p /sys/fs/cgroup/cpu/ydzs.test $ ls /sys/fs/cgroup/cpu/ydzs.test/ cgroup.clone_children cpuacct.stat cpu.cfs_period_us cpu.rt_runtime_us notify_on_release cgroup.event_control cpuacct.usage cpu.cfs_quota_us cpu.shares tasks cgroup.procs cpuacct.usage_percpu cpu.rt_period_us cpu.stat 我们可以看到目录创建完成后，下面就会已经自动创建 cgroup 的相关文件，这里我们重点关注 cpu.cfs_period_us 和 cpu.cfs_quota_us 这两个文件，前面一个是用来配置 CPU 时间周期长度的，默认为 100000us，后者用来设置在此时间周期长度内所能使用的 CPU 时间数，默认值为-1，表示不受时间限制。 $ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_period_us 100000 $ cat /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us -1 现在我们写一个简单的 Python 脚本来消耗 CPU： # cgroup.py while True: pass 直接执行这个死循环脚本即可： $ python cgroup.py \u0026 [1] 2113 使用 top 命令可以看到进程号 2113 的 CPU 使用率达到了 100% 现在我们将这个进程 ID 写入到 /sys/fs/cgroup/cpu/ydzs.test/tasks 文件下面去，然后设置 /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us 为 10000us，因为 cpu.cfs_period_us 默认值为 100000us，所以这表示我们要限制 CPU 使用率为 10%： $ echo 2113 \u003e /sys/fs/cgroup/cpu/ydzs.test/tasks $ echo 10000 \u003e /sys/fs/cgroup/cpu/ydzs.test/cpu.cfs_quota_us 设置完过后上面我们的测试进程 CPU 就会被限制在 10% 左右了，再次使用 top 命令查看该进程可以验证。 如果要限制内存等其他资源的话，同样去对应的子系统下面设置资源，并将进程 ID 加入 tasks 中即可。如果要删除这个 cgroup，直接删除文件夹是不行的，需要使用 libcgroup 工具： $ yum install libcgroup libcgroup-tools $ cgdelete cpu:ydzs.test $ ls /sys/fs/cgroup/cpu/ydzs.test ls: cannot access /sys/fs/cgroup/cpu/ydzs.test: No such file or directory ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:2:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"在容器中使用 CGroups 上面我们测试了一个普通应用如何配置 cgroup，接下来我们在 Containerd 的容器中来使用 cgroup，比如使用 nerdctl 启动一个 nginx 容器，并限制其使用内存为 50M: $ nerdctl run -d -m 50m --name nginx nginx:alpine 8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c $ nerdctl ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8690c7dba4ff docker.io/library/nginx:alpine \"/docker-entrypoint.…\" 53 seconds ago Up nginx 在使用 nerdctl run 启动容器的时候可以使用 -m 或 –memory 参数来限制内存，启动完成后该容器的 cgroup 会出现在 名为 default 的目录下面，比如查看内存子系统的目录： $ ll /sys/fs/cgroup/memory/default/ total 0 drwxr-xr-x 2 root root 0 Oct 21 15:01 8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children --w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs ...... 上面我们启动的 nginx 容器 ID 的目录会出现在 /sys/fs/cgroup/memory/default/ 下面，该文件夹下面有很多和内存相关的 cgroup 配置文件，要进行相关的配置就需要在该目录下对应的文件中去操作： $ ll /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c total 0 -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.clone_children --w--w--w- 1 root root 0 Oct 21 15:01 cgroup.event_control -rw-r--r-- 1 root root 0 Oct 21 15:01 cgroup.procs -rw-r--r-- 1 root root 0 Oct 21 15:01 memory.failcnt --w------- 1 root root 0 Oct 21 15:01 memory.force_empty -rw-r--r-- 1 root root 0 Oct 21 15:01 memory.kmem.failcnt 我们这里需要关心的是 memory.limit_in_bytes 文件，该文件就是用来设置内存大小的，正常应该是 50M 的内存限制： $ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/memory.limit_in_bytes 52428800 同样我们的 nginx 容器进程 ID 也会出现在上面的 tasks 文件中： $ cat /sys/fs/cgroup/memory/default/8690c7dba4ffe03d63983555c594e2784c146b5f9939de1195a9626339c9129c/tasks 2686 2815 2816 2817 2818 我们可以通过如下命令过滤该进程号，可以看出第一行的 2686 就是 nginx 进程在主机上的进程 ID，下面几个是这个进程下的线程： $ ps -ef | grep 2686 root 2686 2656 0 15:01 ? 00:00:00 nginx: master process nginx -g daemon off; 101 2815 2686 0 15:01 ? 00:00:00 nginx: worker process 101 2816 2686 0 15:01 ? 00:00:00 nginx: worker process 101 2817 2686 0 15:01 ? 00:00:00 nginx: worker process 101 2818 2686 0 15:01 ? 00:00:00 nginx: worker process root 2950 1976 0 15:36 pts/0 00:00:00 grep --color=auto 2686 我们删除这个容器后，/sys/fs/cgroup/memory/default/ 目录下的容器 ID 文件夹也会自动删除。 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:3:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"Namespaces namespace 也称命名空间，是 Linux 为我们提供的用于隔离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用个人 PC 时，我们并没有运行多个完全分离的服务器的需求，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这是我们不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。而我们这里的容器其实就通过 Linux 的 Namespaces 技术来实现的对不同的容器进行隔离。 linux 共有 6(7)种命名空间: ipc namespace: 管理对 IPC 资源（进程间通信（信号量、消息队列和共享内存）的访问 net namespace: 网络设备、网络栈、端口等隔离 mnt namespace: 文件系统挂载点隔离 pid namespace: 用于进程隔离 user namespace: 用户和用户组隔离（3.8 以后的内核才支持） uts namespace: 主机和域名隔离 cgroup namespace：用于 cgroup 根目录隔离（4.6 以后版本的内核才支持） 我们可以通过 lsns 命令查看当前系统已经创建的命名空间： # lsns|more NS TYPE NPROCS PID USER COMMAND 4026531836 pid 600 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531837 user 946 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531838 uts 629 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531839 ipc 600 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531840 mnt 591 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026531856 mnt 1 88 root kdevtmpfs 4026531956 net 643 1 root /usr/lib/systemd/systemd --switched-root --system --deserialize 22 4026532450 mnt 1 6635 chrony /usr/sbin/chronyd 4026532451 mnt 1 6660 root /usr/sbin/NetworkManager --no-daemon 要查看一个进程所属的命名空间信息，可以到 /proc//ns 目录下查看： # ps -ef|grep chronyd chrony 6635 1 0 May11 ? 00:00:50 /usr/sbin/chronyd root 17096 25474 0 17:38 pts/1 00:00:00 grep --color=auto chronyd # ll /proc/6635/ns total 0 lrwxrwxrwx 1 root root 0 Sep 11 17:23 ipc -\u003e ipc:[4026531839] lrwxrwxrwx 1 root root 0 Sep 11 17:23 mnt -\u003e mnt:[4026532450] lrwxrwxrwx 1 root root 0 Sep 11 17:23 net -\u003e net:[4026531956] lrwxrwxrwx 1 root root 0 Sep 11 17:23 pid -\u003e pid:[4026531836] lrwxrwxrwx 1 root root 0 Sep 11 17:23 user -\u003e user:[4026531837] lrwxrwxrwx 1 root root 0 Sep 11 17:23 uts -\u003e uts:[4026531838] 这些 namespace 都是链接文件, 格式为 namespaceType:[inode number]，inode number 用来标识一个 namespace，可以理解为 namespace id，如果两个进程的某个命名空间的链接文件指向同一个，那么其相关资源在同一个命名空间中，也就没有隔离了。比如同样针对上面运行的 nginx 容器，我们查看其命名空间： # lsns |grep nginx 4026533147 mnt 17 25233 10000 nginx: master process nginx -g daemon off 4026533148 uts 17 25233 10000 nginx: master process nginx -g daemon off 4026533149 ipc 17 25233 10000 nginx: master process nginx -g daemon off 4026533150 pid 17 25233 10000 nginx: master process nginx -g daemon off 4026533152 net 17 25233 10000 nginx: master process nginx -g daemon off 4026533313 mnt 17 25604 10000 nginx: master process nginx -g daemon off 4026533314 uts 17 25604 10000 nginx: master process nginx -g daemon off 4026533315 ipc 17 25604 10000 nginx: master process nginx -g daemon off 4026533316 pid 17 25604 10000 nginx: master process nginx -g daemon off 4026533318 net 17 25604 10000 nginx: master process nginx -g daemon off 可以看出 nginx 容器启动后，已经为该容器自动创建了单独的 mtn、uts、ipc、pid、net 命名空间，也就是这个容器在这些方面是独立隔离的，其他容器想要和该容器共享某一个命名空间，那么就需要指向同一个命名空间。 ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:4:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"参考 https://kubernetes.io/zh-cn/docs/setup/production-environment/container-runtimes/ https://www.infoq.cn/article/docker-resource-management-cgroups/ ","date":"2023-06-26","objectID":"/cgroups%E4%B8%8Enamespaces/:5:0","tags":["docker"],"title":"CGroups与Namespaces","uri":"/cgroups%E4%B8%8Enamespaces/"},{"categories":["docker"],"content":"介绍 Dockerfile 是一个文本文件，其内包含了一条条的 指令(Instruction)，每一条指令 构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 一个简单Dockerfile文件 # VERSION 0.0.1 FROM ubuntu MAINTAINER James Turnbull \"james@example.com\" RUN echo \"deb http://archive.ubuntu.com/ubuntu precise main ↩ universe\" \u003e /etc/apt/sources.list RUN apt-get update RUN apt-get install -y openssh-server RUN mkdir /var/run/sshd RUN echo \"root:password\" | chpasswd EXPOSE 22 可以看的出来Dockerfile 包含一系列命令并附上参数 每个命令都是大写开头，后面是跟着参数 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:0:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"Dockerfile 命令 命令 解释 FROM 指定基础镜像 RUN Dockerfile中的每个指令都会创建一个新的镜像层 CMD 在容器中执行的命令 EXPOSE 暴露端口 ENV 设置环境变量 COPY 拷贝本地文件和目录至镜像 ADD 功能更丰富的添加拷贝指令，COPY优先于ADD ENTRYPOINT ENTRYPOINT指令并不是必须的，ENTRYPOINT是一个脚本 VOLUME 定义镜像中的某个目录为容器卷，会随机生成一个容器卷名 WORKDIR 指定工作目录 ARG 定义了可以通过docker build –build-arg命令传递并在Dockerfile中使用的变量。 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:1:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"Dockerfile 小建议 要使用 tag，但不要 latest。 Debian 挺好的，不要总 Ubuntu。 apt-get update 在前，rm -rf /var/lib/apt/lists/* 在后。 yum install，不忘 yum clean。 多 RUN 要合并，来减少层数。 无用的软件，不要乱安装。 COPY 放最后，缓存很开心。 善用 dockerignore，不浪费传输。 不忘 MAINTAINER，这都是我的。 容器只运行一个应用 摘要\r小结： （FROM）选择合适的基础镜像(alpine版本最好) (RUN)在安装更新时勿忘删除缓存和安装包， 多个命令聚合在一起减少构建时的层数，用不到的软件不要安装 (ADD和COPY)选COPY （LABEL）添加镜像元数据， (MAINTAINER)让我知道是谁维护我 (ENV)设置默认的环境变量 （EXPOSE）说一下暴露的映射端口\r","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:2:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"规约 各团队尽量使用统一的基础镜像。我们建立并维护公司统一的基础镜像列表，请参见: java应用使用的基础镜像 减少Dockerfile的行数，使用\u0026\u0026连接多个命令，因为每一行命令都会生成一个层 将增加文件和清理文件的动作放到一行里面，比如yum install和yum clean all，如果分为两行，第二条清理动作就无法真正删除文件 只复制需要的文件，如果整个目录复制，一定要仔细检查目录下是否有隐藏文件、临时文件等不需要的内容 容器镜像自身有压缩机制，因此把文件压缩成压缩文件然后打入容器，容器启动时解压的方法并不会有什么效果 避免向生产镜像打入一些不必要的工具，比如有的团队打入了sshd，不应该使用这种方案，增加安全风险 尽量精简安装的内容，比如只安装工具的运行时，无须带上帮助文档、源码、样例等等 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:3:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"示例 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:4:0","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"nginx FROM centos MAINTAINER xianchao RUN yum install wget -y RUN yum install nginx -y # COPY index.html /usr/share/nginx/html/ EXPOSE 80 ENTRYPOINT [\"/usr/sbin/nginx\",\"-g\",\"daemon off;\"] ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:4:1","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"tomcat FROM centos MAINTAINER xianchao RUN yum install wget -y ADD jdk-8u45-linux-x64.rpm /usr/local/ ADD apache-tomcat-8.0.26.tar.gz /usr/local/ RUN cd /usr/local \u0026\u0026 rpm -ivh jdk-8u45-linux-x64.rpm RUN mv /usr/local/apache-tomcat-8.0.26 /usr/local/tomcat8 ENTRYPOINT /usr/local/tomcat8/bin/startup.sh \u0026\u0026 tail -F /usr/local/tomcat8/logs/catalina.out EXPOSE 8080 ","date":"2023-06-26","objectID":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/:4:2","tags":["docker"],"title":"Dockerfile介绍及优化","uri":"/dockerfile%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BC%98%E5%8C%96/"},{"categories":["docker"],"content":"平时较多使用的是docker项目将容器运行时迁移至了containerd，这里整理下相关的操作命令 ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:0:0","tags":["docker","containerd"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["docker"],"content":"常用指令对比 ctr是containerd自带的工具，有命名空间的概念 crictl是k8s社区的专用CLI工具，所有操作都在命名空间k8s.io 所以使用ctr操作时注意指定-n k8s.io Docker vs Containerd\r","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:1:0","tags":["docker","containerd"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["docker"],"content":"清理未被使用的镜像，一般用于释放本地空间 docker 场景 docker image prune -a containerd场景（需要高版本crictl） crictl rmi --prune ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:2:0","tags":["docker","containerd"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["docker"],"content":"本地文件和容器内文件间拷贝 docker 场景 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|- docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH 使用kubectl cp Copy files and directories to and from containers. Examples: # !!!Important Note!!! # Requires that the 'tar' binary is present in your container # image. If 'tar' is not present, 'kubectl cp' will fail. # Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace kubectl cp /tmp/foo_dir \u003csome-pod\u003e:/tmp/bar_dir # Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container kubectl cp /tmp/foo \u003csome-pod\u003e:/tmp/bar -c \u003cspecific-container\u003e # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace \u003csome-namespace\u003e kubectl cp /tmp/foo \u003csome-namespace\u003e/\u003csome-pod\u003e:/tmp/bar # Copy /tmp/foo from a remote pod to /tmp/bar locally kubectl cp \u003csome-namespace\u003e/\u003csome-pod\u003e:/tmp/foo /tmp/bar Options: -c, --container='': Container name. If omitted, the first container in the pod will be chosen --no-preserve=false: The copied file/directory's ownership and permissions will not be preserved in the container Usage: kubectl cp \u003cfile-spec-src\u003e \u003cfile-spec-dest\u003e [options] Use \"kubectl options\" for a list of global command-line options (applies to all commands). ","date":"2023-06-26","objectID":"/%E4%BB%8Edocker%E8%87%B3containerd/:3:0","tags":["docker","containerd"],"title":"从Docker至Containerd","uri":"/%E4%BB%8Edocker%E8%87%B3containerd/"},{"categories":["Kubernetes"],"content":"k8s namespace 无法删除记录 名字空间yaml 文件 kind: Namespace apiVersion: v1 metadata: name: kube-logging 删除卡住 $ sudo kubectl delete -f kube-logging.yaml --wait=true namespace \"kube-logging\" deleted # 长久的等待 强制删除（无效果） sudo kubectl delete ns kube-logging --grace-period=0 --force 排查思路 查看命名空间下的所有资源 kubectl api-resources -o name --verbs=list --namespaced | xargs -n 1 kubectl get --show-kind --ignore-not-found -n kube-logging 输出结果该命名空间下无关联的相关资源 ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:0","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"解决方式 一行命令解决，注意替换两处待删命名空间字样 kubectl get namespace \"待删命名空间\" -o json \\ | tr -d \"\\n\" | sed \"s/\\\"finalizers\\\": \\[[^]]\\+\\]/\\\"finalizers\\\": []/\" \\ | kubectl replace --raw /api/v1/namespaces/待删命名空间/finalize -f - ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:1","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"总结 创建namesapce时注入finalizers会导致namespace无法 可以通过edit ns 的方式将finalizers 置空或者上面的一行命令解决的方式，本质是一致的 ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:2","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"补充 K8s Finalizers Finalizers 字段属于 Kubernetes GC 垃圾收集器，是一种删除拦截机制，能够让控制器实现异步的删除前（Pre-delete）回调。其存在于任何一个资源对象的 Meta[1] 中，在 k8s 源码中声明为 []string，该 Slice 的内容为需要执行的拦截器名称。 对带有 Finalizer 的对象的第一个删除请求会为其 metadata.deletionTimestamp 设置一个值，但不会真的删除对象。一旦此值被设置，finalizers 列表中的值就只能被移除。 当 metadata.deletionTimestamp 字段被设置时，负责监测该对象的各个控制器会通过轮询对该对象的更新请求来执行它们所要处理的所有 Finalizer。当所有 Finalizer 都被执行过，资源被删除。 metadata.deletionGracePeriodSeconds 的取值控制对更新的轮询周期 当 finalizers 字段为空时，k8s 认为删除已完成。 ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:0:3","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"参考 k8s 如何让 ns 无法删除 kubernetes_Namespace无法删除的解决方法 K8S从懵圈到熟练 - 我们为什么会删除不了集群的命名空间？ k8s问题解决 - 删除命名空间长时间处于terminating状态 熟悉又陌生的 k8s 字段：finalizers ","date":"2023-05-27","objectID":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/:1:0","tags":["Kubernetes排障"],"title":"K8s删除Namesapce一直处于Terminating状态处理","uri":"/k8s%E5%88%A0%E9%99%A4namesapce%E4%B8%80%E7%9B%B4%E5%A4%84%E4%BA%8Eterminating%E7%8A%B6%E6%80%81%E5%A4%84%E7%90%86/"},{"categories":["Kubernetes"],"content":"故障描述 警告\rPVC 显示创建不成功：kubectl get pvc -n efk 显示 Pending，这是由于版本太高导致的。k8sv1.20 以上版本默认禁止使用 selfLink。 (selfLink：通过 API 访问资源自身的 URL，例如一个 Pod 的 link 可能是 /api/v1/namespaces/ns36aa8455/pods/sc-cluster-test-1-6bc58d44d6-r8hld)。\r","date":"2023-05-23","objectID":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes v1.20创建PVC报错","uri":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/"},{"categories":["Kubernetes"],"content":"故障解决 $ vi /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 ··· - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --feature-gates=RemoveSelfLink=false # 添加这个配置 重启下kube-apiserver.yaml # 如果是二进制安装的 k8s，执行 systemctl restart kube-apiserver # 如果是 kubeadm 安装的 k8s $ ps aux|grep kube-apiserver $ kill -9 [Pid] $ kubectl apply -f /etc/kubernetes/manifests/kube-apiserver.yaml ... $ kubectl get pvc # 查看 pvc 显示 Bound NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE my-pvc Bound pvc-ae9f6d4b-fc4c-4e19-8854-7bfa259a3a04 1Gi RWX example-nfs 13m ","date":"2023-05-23","objectID":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes v1.20创建PVC报错","uri":"/k8s_v1.20.x%E5%88%9B%E5%BB%BApvc%E6%8A%A5%E9%94%99/"},{"categories":["Jenkins"],"content":"在了解Jenkins 之前先了解一下CI\u0026CD ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:0:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"cicd 介绍 持续集成(Continous Intergrations) 和持续部署(Continuous Deployment)的区别 持续集成(Continous Intergrations) 和持续部署(Continuous Deployment)是DevOps中域软件交付和 部署相关的两个重要概念 尽管他们的目标是类似的，但在具体含义上存在一些区别 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:1:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"持续集成CI 持续集成(Continous Intergrations) 是一种开发实践，通过将每次的代码改动频繁地集成到 共享代码库中，并进行自动化构建和测试，以确保团队开发的代码能够快速且无误地集成在一起 持续集成的目标是通过频繁的集成代码来快速发现和解决集成问题，减少冲突和错误，提供开发效率和 代码质量 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:1:1","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"持续部署CD 持续部署(Continuous Deployment)是在持续集成的基础上更进一步，指的是将经过 持续集成和测试的代码自动部署到生成环境中，使其可以立即提供用户使用 持续部署强调自动化的部署过程，通过自动化测试和验证确保代码的稳定性和可靠性， 使新功能、修复或者改进能够实时交付给用户 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:1:2","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"Jenkins 介绍 Jenkins是一款开源 CI\u0026CD 软件，用于自动化各种任务，包括构建、测试和部署软件。 Jenkins 支持各种运行方式，可通过系统包、Docker 或者通过一个独立的 Java 程序。 说明 Jenkins 是CI\u0026CD 实现方式的一种，还有其它很多种类似的工具,不过使用于特定的场景，只不过在业界 Jenkins 是使用最广泛的一种 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:2:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"Jenkins 同类CICD工具了解 GitLab CI/CD: GitLab 的集成 CI/CD 功能，可以与 GitLab 代码仓库紧密集成，支持自动化构建、测试和部署流程。它具有易用性和配置简单的优点。 CircleCI: CircleCI 是一个托管的持续集成和部署服务，支持多种语言和框架。它通过配置文件（YAML）定义构建流水线，并提供丰富的插件和集成支持。 Travis CI: Travis CI 是一个面向开源项目的持续集成服务，支持 GitHub 和 GitLab 等代码托管平台。它使用 .travis.yml 文件定义构建步骤和环境设置。 TeamCity: JetBrains 的 TeamCity 是一个功能强大的持续集成和部署服务器，支持多种项目类型和复杂的构建流程配置。它具有良好的可扩展性和集成能力。 GitHub Actions: GitHub Actions 是 GitHub 提供的一种基于事件的自动化工作流服务，可以实现代码检查、测试和部署等操作。它与 GitHub 仓库紧密集成，支持自定义的工作流程定义。 Bamboo: Atlassian 的 Bamboo 是一个企业级的持续集成和部署工具，支持构建和自动化测试、部署到多种环境等。它与其他 Atlassian 产品（如 Jira 和 Bitbucket）集成良好。 Drone: Drone 是一个现代化的持续集成和交付平台，支持 Docker 容器化构建，并提供易用的配置文件和插件系统。它适合于基于容器的项目和微服务架构。 Buildkite: Buildkite 是一个分布式的持续集成和交付平台，注重于灵活性和可扩展性。它通过代理机制实现构建作业的并行执行，适用于大规模和复杂的构建流程。 每个工具都有其独特的优势和适用场景，选择合适的工具通常取决于项目的具体需求、团队的技术栈和预算等因素。 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:2:1","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"安装 在官网的用户手册上提供了多种部署Jenkins的方式，下面提供一下基于Kubernetes 部署的Jenkins jenkins-deploy.yaml --- apiVersion: apps/v1 kind: Deployment metadata: name: jenkins namespace: kube-ops spec: selector: matchLabels: app: jenkins template: metadata: labels: app: jenkins spec: #hostNetwork: true # 解决 updates.jenkins.io 异常问题 terminationGracePeriodSeconds: 10 serviceAccount: jenkins initContainers: - name: fix-permissions image: docker.io/library/busybox:latest # busybox:1.35.0 imagePullPolicy: IfNotPresent command: ['sh', '-c', 'chown -R 1000:1000 /var/jenkins_home'] securityContext: privileged: true volumeMounts: - name: jenkinshome mountPath: /var/jenkins_home containers: - name: jenkins image: localhost/jenkins/jenkins:2.452.2 #v1 -\u003e v2.297 #jenkins/jenkins:lts 生成使用 lts 稳定版本 imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP resources: limits: cpu: 1000m memory: 1Gi requests: cpu: 500m memory: 512Mi livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 volumeMounts: - name: jenkinshome subPath: jenkins mountPath: /var/jenkins_home securityContext: fsGroup: 1000 volumes: - name: jenkinshome hostPath: path: /opt/jenkins #volumes: #- name: jenkinshome # persistentVolumeClaim: # claimName: opspvc --- apiVersion: v1 kind: Service metadata: name: jenkins namespace: kube-ops labels: app: jenkins spec: selector: app: jenkins type: NodePort ports: - name: web port: 8080 targetPort: web nodePort: 30002 - name: agent port: 50000 targetPort: agent jenkins-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: jenkins namespace: kube-ops --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: jenkins rules: - apiGroups: [\"extensions\", \"apps\"] resources: [\"deployments\"] verbs: [\"create\", \"delete\", \"get\", \"list\", \"watch\", \"patch\", \"update\"] - apiGroups: [\"\"] resources: [\"services\"] verbs: [\"create\", \"delete\", \"get\", \"list\", \"watch\", \"patch\", \"update\"] - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/exec\"] verbs: [\"create\",\"delete\",\"get\",\"list\",\"patch\",\"update\",\"watch\"] - apiGroups: [\"\"] resources: [\"pods/log\"] verbs: [\"get\",\"list\",\"watch\"] - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: jenkins namespace: kube-ops roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: jenkins subjects: - kind: ServiceAccount name: jenkins namespace: kube-ops 在k8s 中部署的命令 mkdir /opt/jenkins sudo chown -R 1000:1000 /opt/jenkins sudo chmod -R 755 /opt/jenkins kubectl create namespace kube-ops kubectl -n kube-ops create -f rbac.yaml kubectl -n kube-ops create -f jenkins-deploy.yaml kubectl -n kube-ops get pod 注意\r这种是通过直接挂载目录的形式实现的，生产环境是多master 节点，建议使用 nfs 的 pvc 的方式进行挂载\r其它方式安装Jenkins-官网 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:3:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"修改镜像源 由于国内墙的原因，下载一些插件会遇到网络问题，需要修改一下镜像源 系统管控-\u003e 插件管理-\u003e 高级配置 代理镜像源信息： https://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json Jenkins\r对应服务器上配置文件 root@k8s-master01:/opt/jenkins/jenkins# cat hudson.model.UpdateCenter.xml \u003c?xml version='1.1' encoding='UTF-8'?\u003e \u003csites\u003e \u003csite\u003e \u003cid\u003edefault\u003c/id\u003e \u003curl\u003ehttps://mirrors.tuna.tsinghua.edu.cn/jenkins/updates/update-center.json\u003c/url\u003e \u003c/site\u003e ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:4:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"修改 Jenkins 配置 为了方便的使用Jenkins，需要安装一写常用的插件，以下列的有点多，根据需要安装常用的插件，以下基本上都会用得到的，不了解的插件可以到插件市场上 搜索一下看下介绍。 Locale Localization: Chinese Git Git Parameter Git Pipeline for blue Ocean Gitlab Kubernetes Kubernetes CLI Kubernetes Credentials Image Tag Parameter Active Choices Credentials Credentials Binding Blue Ocean (Jenkins 外部UI的页面) Blue Ocean Pineline Editor Blue Ocean Core Js Pipeline SCM API for Blue Ocean Dashboard for Blue Ocean Build with Parameters Dynamic Extended Choice Parameter Extended Choice Parameter List Git Branches Parameter Pipeline Pipelinie: Declarative Publish Over SSH SonarQube Scanner Localization: Chinese(Simplified) Dingtalk Gitlab Plugin (jenkins和Gitlab 建立安全认证) Timestamper AnsiColor SSH SSH Pipeline Steps Jenkins 插件官网 ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:5:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Jenkins"],"content":"参考 https://www.jenkins.io/zh/doc/ ","date":"2023-04-18","objectID":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/:6:0","tags":["Jenkins"],"title":"Jenkins 入门介绍","uri":"/jenkins-%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D/"},{"categories":["Helm"],"content":"Helm 使用一种名为 charts 的包格式，一个 chart 是描述一组相关的 Kubernetes 资源的文件集合，单个 chart 可能用于部署简单的应用，比如 memcached pod，或者复杂的应用，比如一个带有 HTTP 服务、数据库、缓存等等功能的完整 web 应用程序。 Charts 是创建在特定目录下面的文件集合，然后可以将它们打包到一个版本化的存档中来部署。接下来我们就来看看使用 Helm 构建 charts 的一些基本方法。 ","date":"2023-04-17","objectID":"/helm_charts-%E4%BB%8B%E7%BB%8D/:0:0","tags":["Helm"],"title":"Helm Charts 介绍","uri":"/helm_charts-%E4%BB%8B%E7%BB%8D/"},{"categories":["Helm"],"content":"文件结构 chart 被组织为一个目录中的文件集合，目录名称就是 chart 的名称（不包含版本信息），下面是一个 WordPress 的 chart，会被存储在 wordpress/ 目录下面，基本结构如下所示： wordpress/ Chart.yaml # 包含当前 chart 信息的 YAML 文件 LICENSE # 可选：包含 chart 的 license 的文本文件 README.md # 可选：一个可读性高的 README 文件 values.yaml # 当前 chart 的默认配置 values values.schema.json # 可选: 一个作用在 values.yaml 文件上的 JSON 模式 charts/ # 包含该 chart 依赖的所有 chart 的目录 crds/ # Custom Resource Definitions templates/ # 模板目录，与 values 结合使用时，将渲染生成 Kubernetes 资源清单文件 templates/NOTES.txt # 可选: 包含简短使用使用的文本文件 ","date":"2023-04-17","objectID":"/helm_charts-%E4%BB%8B%E7%BB%8D/:1:0","tags":["Helm"],"title":"Helm Charts 介绍","uri":"/helm_charts-%E4%BB%8B%E7%BB%8D/"},{"categories":["Helm"],"content":"Chart.yaml 文件 对于一个 chart 包来说 Chart.yaml 文件是必须的，它包含下面的这些字段： apiVersion: chart API 版本 (必须) name: chart 名 (必须) version: SemVer 2版本 (必须) kubeVersion: 兼容的 Kubernetes 版本 (可选) description: 一句话描述 (可选) type: chart 类型 (可选) keywords: - 当前项目关键字集合 (可选) home: 当前项目的 URL (可选) sources: - 当前项目源码 URL (可选) dependencies: # chart 依赖列表 (可选) - name: chart 名称 (nginx) version: chart 版本 (\"1.2.3\") repository: 仓库地址 (\"https://example.com/charts\") maintainers: # (可选) - name: 维护者名字 (对每个 maintainer 是必须的) email: 维护者的 email (可选) url: 维护者 URL (可选) icon: chart 的 SVG 或者 PNG 图标 URL (可选). appVersion: 包含的应用程序版本 (可选). 不需要 SemVer 版本 deprecated: chart 是否已被弃用 (可选, boolean) ","date":"2023-04-17","objectID":"/helm_charts-%E4%BB%8B%E7%BB%8D/:2:0","tags":["Helm"],"title":"Helm Charts 介绍","uri":"/helm_charts-%E4%BB%8B%E7%BB%8D/"},{"categories":["Helm"],"content":"TEMPLATES 和 VALUES Helm Chart 模板是用 Go template 语言 进行编写的，另外还额外增加了(【Sprig】](https://github.com/Masterminds/sprig)库中的50个左右的附加模板函数和一些其他专用函数。 所有模板文件都存储在 chart 的 templates/ 目录下面，当 Helm 渲染 charts 的时候，它将通过模板引擎传递该目录中的每个文件。模板的 Values 可以通过两种方式提供： Chart 开发人员可以在 chart 内部提供一个名为 values.yaml 的文件，该文件可以包含默认的 values 值内容。 Chart 用户可以提供包含 values 值的 YAML 文件，可以在命令行中通过 helm install 来指定该文件 当用户提供自定义 values 值的时候，这些值将覆盖 chart 中 values.yaml 文件中的相应的值。 ","date":"2023-04-17","objectID":"/helm_charts-%E4%BB%8B%E7%BB%8D/:3:0","tags":["Helm"],"title":"Helm Charts 介绍","uri":"/helm_charts-%E4%BB%8B%E7%BB%8D/"},{"categories":["Kubernetes"],"content":"背景 k8s 容器运行时是docker和containerd结合使用，且生产环境是离线环境将需要的containerd和docker版本相关的rpm包通过yum 进行升级，升级过程涉及到docker的重启，docker容器将会导致所有的容器进行重启，故升级前需要将k8s该节点先设置成维护状态，待升级结束再解除，本文对离线运行时升级进行一个记录，至于为什么需要将Docker和containerd进行结合使用参考下面补充介绍 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:1:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级前提和影响 docker 版本低于 19.03.xx containerd 版本低于 1.3.9 升级后的版本 $ sudo docker version Client: Docker Engine - Community Version: 19.03.11 API version: 1.40 Go version: go1.13.10 Git commit: 42e35e61f3 Built: Mon Jun 1 09:13:48 2020 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.11 API version: 1.40 (minimum version 1.12) Go version: go1.13.10 Git commit: 42e35e61f3 Built: Mon Jun 1 09:12:26 2020 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.3.9 GitCommit: ea765aba0d05254012b0b9e595e995c09186427f runc: Version: 1.0.0-rc10 GitCommit: dc9208a3303feef5b3839f4323d9beb36df0a9dd docker-init: Version: 0.18.0 GitCommit: fec3683 $ containerd --version containerd containerd.io 1.3.9 ea765aba0d05254012b0b9e595e995c09186427f ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:2:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级操作 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"获取升级前集群所有节点状态 $ kubectl get nodes \u003e before-update-node-status.txt ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:1","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级过程 升级原则，升级 1 个，检查 OK 后，再升级另外 1 个节点 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:2","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级前操作 将准备的升级介质上传至yum源 # rpm 包指定文件夹 /repos/zcm-custom/centos/7/x86_64/ mv docker-ce-19.03.11.tar.gz /repos/zcm-custom/centos/7/x86_64/ cd /repos/zcm-custom/centos/7/x86_64/ tar zxvf docker-ce-19.03.11.tar.gz cd /repos/zcm-custom/centos/7/ createrepo --update x86_64/ # 更新yum源仓库 yum makecache # #重新生成缓存 yum list docker-ce 检查 $ sudo yum list docker-ce Loaded plugins: product-id, search-disabled-repos Repodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast Installed Packages docker-ce.x86_64 3:19.03.11-3.el7 @zcm-custom 能看到 19.03.11 说明 yum 源更新成功 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:3","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级步骤 将被升级节点置为非调度状态；对被升级节点执行驱逐操作，将该节点上的业务容器飘到其它节点 $ /usr/local/bin/kubectl cordon 10.45.80.44 $ /usr/local/bin/kubectl drain 10.45.80.44 --ignore-daemonsets 停止 kubelet 服务、停止 kube-proxy 服务 $ sudo systemctl stop kube-proxy $ sudo systemctl stop kubelet 停止 docker/containerd 服务 $ sudo systemctl stop docker $ sudo systemctl stop containerd $ sudo ps aux | grep docker # 检查该机器上所有与 docker 相关的进程 # 如果存在，则 kill 掉 $ sudo ps -ef|grep docker|grep -v dockerd|awk '{print $2}'|xargs kill -9 containerd 升级 $ yum install -y containerd 启动升级后的 docker/containerd 服务 $ sudo systemctl start containerd $ sudo systemctl start docker $ sudo systemctl enable docker $ sudo systemctl enable containerd 启动 kubelet 服务、启动 kube-proxy 服务 $ sudo systemctl start kube-proxy $ sudo systemctl start kubelet 登录 master 节点将升级节点状态设置为允许调度状态 $ sudo kubectl get nodes $ sudo kubectl uncordon 10.45.80.44 被升级节点的状态检查 检查被升级节点的状态 在 k8s master 节点上，执行 kubectl get nodes |grep 被升级节点的 IP 看看是否处于 Ready 状态。 检查被升级节点的容器网络，在升级完后的节点上 ping 一下其它机器上的容器 IP，ping 通的话，容器网络正常 该节点升级完成 依次升级其它节点，按前面的升级步骤位次升级后续的节点，在升级后续节点的过程中，业务容器会在前面升级完成后的节点上调度 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:3:4","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"升级后的节点状态检查 $ kubectl get nodes ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:4:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"补充-为什么要将Docker和containerd进行结合使用？ 更轻量级：containerd是一个更轻量级且专注于容器操作的运行时工具，相比于Docker具有更小的内存占用和更快的启动时间。 更稳定和可靠：containerd是由Docker开源社区开发和维护的，因此可以受益于Docker社区的经验和反馈，从而获得更稳定和可靠的容器技术。 更灵活的集成：containerd被设计为模块化的容器运行时，可以与其他容器生态系统中的工具和服务集成，例如Kubernetes、CRI-O等。 更高性能：由于containerd比Docker更轻量级，因此可以获得更高的性能。它可以快速创建和销毁容器，并提供更低的延迟和更高的吞吐量。 兼容性：通过使用Docker作为容器镜像格式，containerd能够与Docker生态系统中的工具和服务无缝集成，而无需进行任何修改。 综上所述，使用Docker和containerd结合可以提供更轻量级、更稳定可靠、更灵活集成、更高性能和更好的兼容性。这使得它成为一个强大的容器运行时组合，适用于各种不同的容器化场景。 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:5:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Kubernetes"],"content":"参考 Docker 与 Containerd 并用配置 手把手教你搭建Linux离线YUM源环境 ","date":"2023-04-17","objectID":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/:6:0","tags":["Kubernetes"],"title":"k8s容器运行时 containerd 版本升级","uri":"/k8s%E5%AE%B9%E5%99%A8%E8%BF%90%E8%A1%8C%E6%97%B6containerd%E5%8D%87%E7%BA%A7/"},{"categories":["Jenkins"],"content":"要实现在 Jenkins 中的构建工作，可以有多种方式，我们这里采用比较常用的 Pipeline 这种方式。 Pipeline，简单来说，就是一套运行在 Jenkins 上的工作流框架，将原来独立运行于单个或者多个节点的任务连接起来，实现单个任务难以完成的复杂流程编排和可视化的工作。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:0:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline是什么？ Pipeline是Jenkins的核心功能，提供一组可扩展的工具。 通过Pipeline 的DSL语法可以完成从简单到复杂的交付流水线实现。 jenkins的Pipeline是通过Jenkinsfile（文本文件）来实现的。 这个文件可以定义Jenkins的执行步骤，例如检出代码。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:1:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Jenkins file Jenkinsfile使用两种语法进行编写，分别是声明式和脚本式。 声明式和脚本式的流水线从根本上是不同的。 声明式是jenkins流水线更友好的特性。 脚本式的流水线语法，提供更丰富的语法特性。 声明式流水线使编写和读取流水线代码更容易设计。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:2:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"为什么要使用Pipeline? 本质上，jenkins是一个自动化引擎，它支持许多自动模式。流水线向Jenkins添加了一组强大的工具，支持用例、简单的持续集成到全面的持续交付流水线。 通过对一系列的发布任务建立标准的模板，用户可以利用更多流水线的特性，比如： 代码化: 流水线是在代码中实现的，通常会存放到源代码控制，使团队具有编辑、审查和更新他们项目的交付流水线的能力。 耐用性：流水线可以从Jenkins的master节点重启后继续运行。 可暂停的：流水线可以由人功输入或批准继续执行流水线。 解决复杂发布： 支持复杂的交付流程。例如循环、并行执行。 可扩展性： 支持扩展DSL和其他插件集成。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:3:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Jenkins Pipeline 几个核心概念 Node：节点，一个 Node 就是一个 Jenkins 节点，Master 或者 Agent，是执行 Step 的具体运行环境 Stage：阶段，一个 Pipeline 可以划分为若干个 Stage，每个 Stage 代表一组操作，比如：Build、Test、Deploy，Stage 是一个逻辑分组的概念，可以跨多个 Node Step：步骤，Step 是最基本的操作单元，可以是打印一句话，也可以是构建一个 Docker 镜像，由各类 Jenkins 插件提供，比如命令：sh ‘make’，就相当于我们平时 shell 终端中执行 make 命令一样。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:4:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"创建Jenkins Pipeline Pipeline 脚本是由 Groovy 语言实现的，但是我们没必要单独去学习 Groovy，当然你会的话最好 Pipeline 支持两种语法：Declarative(声明式)和 Scripted Pipeline(脚本式)语法 Pipeline 也有两种创建方法：可以直接在 Jenkins 的 Web UI 界面中输入脚本；也可以通过创建一个 Jenkinsfile 脚本文件放入项目源码库中 Jenkins\r一般我们都推荐在 Jenkins 中直接从源代码控制(SCMD)中直接载入 Jenkinsfile Pipeline 这种方法，正常项目中会将 Jenkinsfile 流水线文件放到项目目录下，可以选择Pipeline script from SCM Jenkins\r参考项目 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:5:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline 模板 可以根据以下模板进行动态的填充和修改 // 所有的脚步命令都放在pipeline 中 pipeline{ // 在任一节点都可以使用 agent any // parameters { // string{name:'DEPLOY_ENV',defaultValue:'staging',description:''}, // booleanParam{name:'DEBUG_BUID',defaultValue:true,description:''} // } // 定义全局变量 environment{ key = 'value' } options { //timestamps() // 日志会有时间，依赖Timestamper 插件 // skipDefaultCheckout() // 删除隐式checkout scm 语句 disableConcurrentBuilds() // 禁止并行 timeout(time: 1,unit:'HOURS') // 流水线超时设置1h } stages{ // Example stage('Input 交换的方式'){ steps { timeout(time:5,unit:\"MINUTES\"){ script{ echo \"nice to meet you\" //input id: 'Id', message: 'message', ok: '是否继续', parameters: [choice(choices: ['a', 'b', 'c'], name: 'abc')], submitter: 'lushuan,admin' println(\"hello\") } } } } // 下载代码 stage('拉取git 仓库代码'){ steps { timeout(time:5,unit:\"MINUTES\"){ echo '拉取代码成功-SUCCESS' } } } stage('通过maven构建项目'){ steps { timeout(time:20,unit:\"MINUTES\"){ sh \"/var/jenkins_home/maven/bin/mvn clean package -DskipTests\" } } } stage('通过SnoarQube做代码质量检测'){ steps { timeout(time:30,unit:\"MINUTES\"){ echo '通过SnoarQube做代码质量检测-SUCCESSS' } } } stage('通过Docker制作自定义镜像'){ steps { timeout(time:10,unit:\"MINUTES\"){ //sh \"docker build -t my-spring-boot-app:latest .\" sshPublisher(publishers: [sshPublisherDesc(configName: 'mytest', transfers: [sshTransfer(cleanRemote: false, excludes: '', execCommand: '''cd /opt/jenkins/jenkins/workspace/mytest docker build -t my-spring-boot-app:latest . docker stop mytest docker rm mytest docker run -it -d --name mytest -p 8080:8080 my-spring-boot-app:latest''', execTimeout: 120000, flatten: false, makeEmptyDirs: false, noDefaultExcludes: false, patternSeparator: '[, ]+', remoteDirectory: '', remoteDirectorySDF: false, removePrefix: '', sourceFiles: '')], usePromotionTimestamp: false, useWorkspaceInPromotion: false, verbose: false)]) echo '通过Docker制作自定义镜像-SUCCESSS' } } } stage('将自定义镜像推送至Harbor'){ steps { timeout(time:10,unit:\"MINUTES\"){ echo '将自定义镜像推送至Harbor-SUCCESSS' } } } stage('通过Publish Over SSH 通知目标服务器'){ steps { timeout(time:10,unit:\"MINUTES\"){ echo '通过Publish Over SSH 通知目标服务器-SUCCESSS' } } } } // 构建后操作 post { // 无论成功失败总是执行 always{ script{ println(\"always\") } } success { script{ currentBuild.description += \"\\n 构建成功!\" } // 需要安装 dingtalk 插件 // dingtalk( // robot: \"钉钉自定义名称\", // type: 'MARKDOWN', // title: \"success: ${JOB_NAME}\" // text: [\"- 成功构建:${JOB_NAME}! \\n-版本:${tag} \\n-持续时间:${currentBuild.durationString}\"] // ) } failure { script{ currentBuild.description += \"\\n 构建失败!\" } // dingtalk( // robot: \"钉钉自定义名称\", // type: 'MARKDOWN', // title: \"failure: ${JOB_NAME}\" // text: [\"- 失败构建:${JOB_NAME}! \\n-版本:${tag} \\n-持续时间:${currentBuild.durationString}\"] // ) } aborted { script{ currentBuild.description += \"\\n 构建取消!\" } } } } ","date":"2023-04-16","objectID":"/jenkins-pipeline/:6:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"流水线基础语法 以下是简要的对流水线语法进行粗浅的人生，更详细的请登录官网流水线语法参考进行查看 简介pipeline 基础语法，声明式脚步式，节点步骤 安装声明式插件 Pipeline: Declarative Jenkinsfile 组成 指定node 节点/workspace 指定运行选项 指定stages 阶段 指定构建后操作 Pipeline 的语法分为两种，一种是声明式的，另外一种是脚步式的，声明式编写更加友好，声明式内可以嵌入脚本式 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline定义- post 指定构建后操作 介绍 always{}: 总是执行脚本片段 success{}: 成功后执行 failure{}: 失败后执行 aborted{}: 取消后执行 changed 只有当流水线或者阶段完成状态与之前不同时 unstable 只有当流水线或者阶段状态为\"unstable运行\"，例如测试失败 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:1","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- agent agent 指定了流水线的执行节点 参数： - any 在任何可用的节点上执行pipeling - none 没有指定agent 的时候默认 - lable 在指定的标签上的节点上运行Pipeline - node 允许额外的选项 两种是一样的\ragent { node {label 'labelname'}}\ragent { label 'labelname'}\r","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:2","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- option buildDiscarder:为最近的流水线运行的特定数量保存组件和控制台输出。 disableConcurrentBuilds:不允许同时执行流水线。可被用来防止同时访问共享资源等 overridelndexTriggers: 允许覆盖分支索引触发器的默认处理。 skipDefaultCheckout: 在agent 指令中,跳过从源代码控制中检出代码的默认情况。 skipStagesAfterUnstable:一旦构建状态变得UNSTABLE,跳过该阶段。 checkoutToSubdirectory: 在工作空间的子目录中自动地执行源代码控制检出。 timeout: 设置流水线运行的超时时间,在此之后,Jenkins将中止流水线。 retry:在失败时,重新尝试整个流水线的指定次数。 timestamps 预测所有由流水线生成的控制台输出,与该流水线发出的时间一致。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:3","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- parameters 添加参数时，可以在jenkins 脚本中获取到 parameters { string{name:'DEPLOY_ENV',defaultValue:'staging',description:''}, booleanParam{name:'DEBUG_BUID',defaultValue:true,description:''} } ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:4","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- trigger 触发器 triggers { cron('H */4 * * 1-5') } ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:5","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- tool 主要是用来指定全局工具 tools { maven 'apache-maven-3.0.1' } 如在pipeline 中集成maven时使用 stage(\"build\"){ mvHome=\"/usr/local/apache-maven-3.5.0/bin\" sh \"${mvHome}/mvn clean package\" } // 或者通过 tool 引用 stage(\"build\"){ mvHome = tool 'M3' // M3: /usr/local/apache-maven-3.5.0/bin sh \"${mvHome}/mvn clean package\" } ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:6","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- input input 用户在执行各个阶段的时候，由人工确认是否继续执行 message 呈现给用户的提示信息。 id 可选,默认为stage名称。 ok 默认表单上的ok文本。 submitter 可选的,以逗号分隔的用户列表或允许提交的外部组名。默认允许任何用户。 submitterParameter环境变量的可选名称。如果存在,用submitter 名称设置。 parameters 提示提交者提供的一个可选的参数列表。 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:7","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- when when 指令允许流水线根据给定的条件决定是否应该执行阶段。when 指令必须包含至少一个条件。如果when 指令包含多个条件，所有的条件必须返回True，阶段才能执行。 语法和在stage 平级 内置条件 branch: 当正在构建的分支与模式给定的分支匹配是，执行这个阶段，这只适用于多分支流水线例如： when { branch 'master'} environment:当指定的环境变量是给定的值时，执行这个步骤，例如： when { environment name:'DEPLOY_TO',value: 'production'} not 当嵌套的条件错误时，执行这个阶段，必须包含一个条件，例如 when { not {branch 'master'}} anyof when { anyof {branch 'master';branch 'staging'}} ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:8","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"Pipeline语法- parallel 并行 声明式流水线的阶段可以在他们内部声明多隔嵌套阶段, 它们将并行执行。 另外, 通过添加 failFast true 到包含parallel的 stage中， 当其中一个进程失败时，可以强制所有的 parallel 阶段都被终止。 pipeline { agent any stages { stage('Non-Parallel Stage') { steps { echo 'This stage will be executed first.' } } stage('Parallel Stage') { when { branch 'master' } failFast true parallel { stage('Branch A') { agent { label \"for-branch-a\" } steps { echo \"On Branch A\" } } stage('Branch B') { agent { label \"for-branch-b\" } steps { echo \"On Branch B\" } } } } } } ","date":"2023-04-16","objectID":"/jenkins-pipeline/:7:9","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"参考 Jenkins 流水线 ","date":"2023-04-16","objectID":"/jenkins-pipeline/:8:0","tags":["Jenkins"],"title":"Jenkins Pipeline","uri":"/jenkins-pipeline/"},{"categories":["Jenkins"],"content":"什么是 Blue Ocean? Blue Ocean 是Jenkins的开源子项目，在保证原有强大的功能不变的基础下，对持续交付(CD)Pipeline过程的可视化方面相较于Jenkins 之前的经典界面有了很大的提升。 Blue Ocean 重新思考Jenkins的用户体验，从头开始设计Jenkins Pipeline, 但仍然与自由式作业兼容，Blue Ocean减少了混乱而且进一步明确了团队中每个成员 Blue Ocean 的主要特性包括： 持续交付(CD)Pipeline的 复杂可视化 ，可以让您快速直观地理解管道状态。 Pipeline 编辑器 - 引导用户通过直观的、可视化的过程来创建Pipeline，从而使Pipeline的创建变得平易近人。 个性化 以适应团队中每个成员不同角色的需求。 在需要干预和/或出现问题时 精确定位 。 Blue Ocean 展示 Pipeline中需要关注的地方， 简化异常处理，提高生产力 本地集成分支和合并请求, 在与GitHub 和 Bitbucket中的其他人协作编码时实现最大程度的开发人员生产力。 ","date":"2023-04-14","objectID":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/:1:0","tags":["Jenkins"],"title":"使用Blue Ocean创建一个简单的流水线","uri":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/"},{"categories":["Jenkins"],"content":"访问Blue Ocean 在系统管理\u003e插件管理\u003e可选插件中搜索 Blue Ocean \u003e安装,安装成功之后，就可以在页面上看到Blue Ocean的图标,打开后注册git 仓库地址 认证后就可以创建流水线了。 Blue Ocean\r","date":"2023-04-14","objectID":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/:2:0","tags":["Jenkins"],"title":"使用Blue Ocean创建一个简单的流水线","uri":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/"},{"categories":["Jenkins"],"content":"创建流水线 创建流水线前需要先安装对应的插件 Blue Ocean 这边使用的git 仓库是gitee，使用gitlab 也是一样的方式，有两种认证的方式 通过账号口令 通过生成 ssh 公钥在gitlab 或者gitee 仓库上进行注册 Blue Ocean\r首先先进行证书认证，然后再创建流水线，如果项目中没有 Jenkinsfile 文件，会自动生成一个文件并默认推到 master 分支。 ","date":"2023-04-14","objectID":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/:3:0","tags":["Jenkins"],"title":"使用Blue Ocean创建一个简单的流水线","uri":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/"},{"categories":["Jenkins"],"content":"在Blue Ocean 查看任务进度视图 点击对应的工作节点，可以查询任务运行过程中的日志详情 Blue Ocean\rblue ocean 反向生成的pipeline 代码 pipeline { agent any stages { stage('gitlab pull code') { parallel { stage('gitlab pull code') { steps { sh 'echo \\'gitlab pull code\\'' } } stage('code test') { steps { sh 'echo \\'code test\\'' } } } } stage('docker build') { parallel { stage('docker build') { steps { sh 'echo \\'docker build\\'' } } stage('docker test') { steps { sh 'echo \\'docker test\\'' } } } } stage('deployment') { steps { sh 'echo \\'deployment\\'' } } stage('sanity test') { steps { sh 'echo \\'sanity test\\'' } } } } ","date":"2023-04-14","objectID":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/:3:1","tags":["Jenkins"],"title":"使用Blue Ocean创建一个简单的流水线","uri":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/"},{"categories":["Jenkins"],"content":"参考 https://www.jenkins.io/zh/doc/book/blueocean/ ","date":"2023-04-14","objectID":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/:4:0","tags":["Jenkins"],"title":"使用Blue Ocean创建一个简单的流水线","uri":"/%E4%BD%BF%E7%94%A8blue-ocean%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E6%B5%81%E6%B0%B4%E7%BA%BF/"},{"categories":["Kubernetes"],"content":"现象 k8s 某台主机的/var/lib/kubelet目录存储使用率超过80% 使用了40多G存储 ","date":"2023-04-04","objectID":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:1:0","tags":["Kubernetes"],"title":"kubelet目录满问题记录","uri":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes"],"content":"分析 申请root权限 cd /var/lib/kubelet 执行du -h -d 1 发现 pod目录占据大量存储 cd pod 继续执行du -h -d 1 定位到pod 2753dcf9-4113-4e65-a944-278bcbd8d6d8 按上面方法继续执行 发现最终路径为 /var/lib/kubelet/pods/2753dcf9-4113-4e65-a944-278bcbd8d6d8/volumes/kubernetes.io~empty-dir/storage-volume docker ps|grep 2753dcf9-4113-4e65-a944-278bcbd8d6d8 定位容器为prometheus docker inspect 122ec2fbbd1e kubelet\r该容器使用了empty-dir挂载内部的/data ","date":"2023-04-04","objectID":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:2:0","tags":["Kubernetes"],"title":"kubelet目录满问题记录","uri":"/kubelet%E7%9B%AE%E5%BD%95%E6%BB%A1%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Kubernetes"],"content":"kube-dns切换为coredns k8s版本及其dns适配版本 k8s版本 coredns版本 镜像 镜像yaml 1.17 1.6.7 coredns-1.6.7.tar.gz coredns-1.6.7.yaml 1.22 1.8.4 coredns-1.8.4.tar.gz coredns-1.8.4.yaml coredns/coredns:1.6.7/1.8.4 ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:1:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"操作 根据版本选择对应的dns镜像以及yaml文件上传到其中一台master主机，以1.8.4 版本为例 如果是内网则将coredns-1.6.7/1.8.4.tar.gz 压缩包进行解压并上传至现场的harbor仓库，如果可以连接外网则直接修改 coredns-1.8.4.yaml 文件，将站位符{{ image_address }} 进行修改 ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:2:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"将原有的 kube-dns 停止 $ kubectl scale deploy kube-dns --replicas=0 -n kube-system $ kubectl scale deploy kube-dns-autoscaler --replicas=0 -n kube-system ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:3:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"部署coredns $ kubectl apply -f coredns.yaml ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:4:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"补充: kube-dns vs CoreDNS CoreDNS和kube-dns都是用于Kubernetes集群中的DNS解析服务，它们在不同方面有一些区别，适用于不同的场景。 另外还有一个方面是早期版本的 kube-dns 在 Kubernetes 中主要支持 IPv4。IPv6 的支持不如 CoreDNS 完善，并且在实际应用中可能会遇到限制。 CoreDNS： CoreDNS是一个开源的、轻量级的、灵活的DNS服务器，它是一个通用的DNS服务器，可以用于各种不同的环境和场景，不仅限于Kubernetes集群。 CoreDNS是一个插件驱动的DNS服务器，可以通过插件的方式支持各种功能，例如反向代理、负载均衡、缓存、DNSSEC等。 CoreDNS在Kubernetes集群中可以作为集群的默认DNS插件，用于提供内部服务发现和外部域名解析。它可以动态地解析Kubernetes集群内部的服务和Pod，并提供DNS记录。 kube-dns： kube-dns是Kubernetes集群中的默认DNS解析服务，是一种基于SkyDNS的插件，与Kubernetes紧密集成。 kube-dns使用了一个集群内部的DNS服务器和一个外部的DNS服务器配合工作。集群内部的DNS服务器负责解析集群内的服务和Pod，并提供DNS记录。外部的DNS服务器负责解析集群外的域名，并提供DNS记录。 kube-dns的优点是它是Kubernetes的默认解析器，易于配置和使用，可以满足大多数基本的服务发现和域名解析需求。 综上所述，CoreDNS适用于更通用的环境和场景，可以满足更多的功能需求，而kube-dns是Kubernetes集群中的默认解析器，适用于基本的服务发现和域名解析需求。具体使用哪个取决于特定的需求和场景。 ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:5:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"参考 【kubernetes】部署-CoreDNS-服务 ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:6:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"coredns-1.6.7.yaml apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors health { lameduck 5s } ready kubernetes cluster.local in-addr.arpa ip6.arpa { fallthrough in-addr.arpa ip6.arpa } template ANY AAAA { rcode NXDOMAIN } prometheus :9153 forward . /etc/resolv.conf cache 30 loop reload loadbalance } --- apiVersion: apps/v1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/name: \"CoreDNS\" spec: # replicas: not specified here: # 1. Default is 1. # 2. Will be tuned in real time if DNS horizontal auto-scaling is turned on. strategy: type: RollingUpdate rollingUpdate: maxUnavailable: 1 selector: matchLabels: k8s-app: kube-dns template: metadata: labels: k8s-app: kube-dns spec: priorityClassName: system-cluster-critical serviceAccountName: coredns tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" - key: node.kubernetes.io/unschedulable effect: NoSchedule nodeSelector: kubernetes.io/role: master affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: k8s-app operator: In values: [\"kube-dns\"] topologyKey: kubernetes.io/hostname containers: - name: coredns image: {{ image_address }}/coredns/coredns:1.6.7 imagePullPolicy: IfNotPresent resources: limits: memory: 170Mi requests: cpu: 100m memory: 70Mi args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns readOnly: true ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP - containerPort: 9153 name: metrics protocol: TCP securityContext: allowPrivilegeEscalation: false capabilities: add: - NET_BIND_SERVICE drop: - all readOnlyRootFilesystem: true livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 readinessProbe: httpGet: path: /ready port: 8181 scheme: HTTP dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system annotations: prometheus.io/port: \"9153\" prometheus.io/scrape: \"true\" labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP - name: metrics port: 9153 protocol: TCP ","date":"2023-03-29","objectID":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/:7:0","tags":["Kubernetes"],"title":"kube-dns切换为coredns实践","uri":"/kube-dns%E5%88%87%E6%8D%A2%E4%B8%BAcoredns%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":" 当Kubernetes创建Pod时发生了什么-全景图 https://github.com/jamiehannaford/what-happens-when-k8s/tree/master/zh-cn what-happens-when-k8s\r","date":"2023-03-26","objectID":"/%E5%BD%93-kubernetes-%E5%88%9B%E5%BB%BA-pod-%E6%97%B6%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/:0:0","tags":["Kubernetes"],"title":"当 Kubernetes 创建 Pod 时发生了什么","uri":"/%E5%BD%93-kubernetes-%E5%88%9B%E5%BB%BA-pod-%E6%97%B6%E5%8F%91%E7%94%9F%E4%BA%86%E4%BB%80%E4%B9%88/"},{"categories":["Helm"],"content":"预置条件 适用于Kubernetes v1.12.1及以上，Helm v3.4.2及以上。 以test-cmdb应用为例进行说明,test-cmdb为部署在k8s中的web 应用 ","date":"2023-03-20","objectID":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/:1:0","tags":["Helm"],"title":"Helm charts 编写模板说明","uri":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/"},{"categories":["Helm"],"content":"目录结构 test-cmdb ├── Chart.yaml ├── files │ └── application.properties ├── templates │ ├── configmap.yaml │ ├── deployment.yaml │ ├── _helpers.tpl │ ├── ingress.yaml │ ├── NOTES.txt │ └── service.yaml └── values.yaml ​ 2 directories, 9 files 目录及文件描述： 类型 名称 描述 D test-cmdb 应用名称，根据实际业务应用进行命名。 F Chart.yaml 文件包含了该chart的描述。 D files 配置文件目录 F application.properties 配置文件 D templates 目录包括了模板文件。当Helm评估chart时，会通过模板渲染引擎将所有文件发送到templates/目录中。 然后收集模板的结果并发送给Kubernetes。 F configmap.yaml configmap模板 F deployment.yaml deployment模板 F _helpers.tpl 放置可以通过chart复用的模板辅助对象 F ingress.yaml ingress模板。 F NOTES.txt chart的\"帮助文本\"。会在用户执行helm install时展示。 F service.yaml service模板。 F values.yaml chart的默认值。会在用户执行helm install 或 helm upgrade时被覆盖。 其中，类型：D，目录；F，文件。 ","date":"2023-03-20","objectID":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/:2:0","tags":["Helm"],"title":"Helm charts 编写模板说明","uri":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/"},{"categories":["Helm"],"content":"配置资源 一个完整的应用部署通常由应用名称、镜像名称及tag、环境变量、资源限额、端口监听、端口映射、目录/文件挂载、配置文件、网关规则等组成。通过helm将应用模板化，仅需要简单配置些许地方即可。 应用名称 文件名 key Chart.yaml name values.yaml fullnameOverride 镜像名称及tag 文件名 key values.yaml image.repository values.yaml image.tag 当前的模板默认是以镜像名称与应用名称保持一致为前提的，如镜像10.45.80.1/iplatform/test-cmdb:C_20220214144619的名称为test-cmdb与应用名称test-cmdb保持一致。假如实际的应用名与镜像名称不一致，请自行修改_helpers.tpl中的fullimage部分。 修改前 {{- define \"zcm-tpl.fullimage\" -}} {{- .Values.global.repository }}/{{ .Values.image.repository }}:{{ index .Values .Chart.Name \"image\" \"tag\" | default .Chart.AppVersion }} {{- end }} 修改后 {{- define \"zcm-tpl.fullimage\" -}} {{- .Values.global.repository }}/{{ .Values.image.repository }}:{{ index .Values \"name-template\" \"image\" \"tag\" | default .Chart.AppVersion }} {{- end }} 其中，name-template为实际的镜像名称，该镜像信息配置在$HELM_PATH/charts/values.yaml中，如果该文件中没有匹配的信息，当在部署时会提示异常。 环境变量 文件名 key values.yaml env.name values.yaml env.value values.yaml env.valueFrom.configMapKeyRef.name values.yaml env.valueFrom.configMapKeyRef.key 资源限额 文件名 key values.yaml resources.requests.cpu values.yaml resources.requests.memory values.yaml resources.limits.cpu values.yaml resources.limits.memory 端口监听 文件名 key values.yaml ports.containerPort values.yaml ports.protocol values.yaml ports.name 端口映射 文件名 key values.yaml service.ports.name values.yaml service.ports.port values.yaml service.ports.targetPort 目录/文件挂载 文件名 key values.yaml volumeMounts.name values.yaml volumeMounts.mountPath values.yaml volumeMounts.readOnly values.yaml volumeMounts.subPath values.yaml volumes.name values.yaml volumes.hostpath.path 配置文件 文件名 key values.yaml cm.name values.yaml cm.configmap.name 网关规则 文件名 key values.yaml ingress.annotations values.yaml ingress.tls values.yaml ingress.hosts.paths.path values.yaml ingress.hosts.paths.pathType values.yaml ingress.hosts.paths.port ","date":"2023-03-20","objectID":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/:3:0","tags":["Helm"],"title":"Helm charts 编写模板说明","uri":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/"},{"categories":["Helm"],"content":"部署测试 将准备好的chart放到ZCM部署主机$HELM_PATH/charts目录下，镜像推送到对应环境的镜像仓库中，并在$HELM_PATH/charts/values.yaml中配置镜像信息， 如镜像10.45.80.1/iplatform/zcm-crms:T_20220411015401，需配置对应的信息如下： zcm-crms: image: tag: T_20220411015401 执行部署操作 cd $HELM_PATH/charts helm install zcm-crms ./zcm-crms -f values.yaml -n backend 也可执行其他相关操作 升级 helm upgrade zcm-crms ./zcm-crms -f values.yaml -n backend 卸载 helm uninstall zcm-crms -n backend ","date":"2023-03-20","objectID":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/:4:0","tags":["Helm"],"title":"Helm charts 编写模板说明","uri":"/helm_charts%E7%BC%96%E5%86%99%E6%A8%A1%E6%9D%BF%E8%AF%B4%E6%98%8E/"},{"categories":["Kubernetes"],"content":"记录一下kuberntes 遇到的问题场景及排查方向 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:0:0","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"DNS 解析异常 5 秒延时 如果DNS查询经常延时5秒才返回，通常是遇到内核 conntrack 冲突导致的丢包 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"解析超时 如果容器内报 DNS 解析超时，先检查下集群 DNS 服务 (kube-dns/coredns) 的 Pod 是否 Ready，如果不是，查看日志信息。如果运行正常，再具体看下超时现象。 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:1:1","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"Service 无法解析 集群 DNS 没有正常运行(kube-dns或CoreDNS) 检查集群 DNS 是否运行正常: kubelet 启动参数 –cluster-dns 可以看到 dns 服务的 cluster ip: $ ps -ef | grep kubelet ... /usr/bin/kubelet --cluster-dns=172.16.14.217 ... 或者放置配置文件中 $ cat /var/lib/kubelet/config.yaml|grep clusterDNS -A 2 clusterDNS: - 172.16.0.10 clusterDomain: cluster.local 找到 dns 的 service: $ kubectl get svc -n kube-system | grep 172.16.14.217 kube-dns ClusterIP 172.16.14.217 \u003cnone\u003e 53/TCP,53/UDP 47d 看是否存在 endpoint: $ kubectl -n kube-system describe svc kube-dns | grep -i endpoints Endpoints: 172.16.0.156:53,172.16.0.167:53 Endpoints: 172.16.0.156:53,172.16.0.167:53 检查 endpoint 的 对应 pod 是否正常: $ kubectl -n kube-system get pod -o wide | grep 172.16.0.156 kube-dns-898dbbfc6-hvwlr 3/3 Running 0 8d 172.16.0.156 10.0.0.3 ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["Kubernetes"],"content":"Pod 与 DNS 服务之间网络不通 检查下 pod 是否连不上 dns 服务，可以在 pod 里 telnet 一下 dns 的 53 端口: # 连 dns service 的 cluster ip $ telnet 172.16.14.217 53 如果检查到是网络不通，就需要排查下网络设置: 检查节点的安全组设置，需要放开集群的容器网段 检查是否还有防火墙规则，检查 iptables ","date":"2022-09-28","objectID":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/:2:1","tags":["Kubernetes排障"],"title":"Kubernetes 网络排障","uri":"/kubernetes-%E7%BD%91%E7%BB%9C%E6%8E%92%E9%94%99/"},{"categories":["prometheus"],"content":"\rPromQL\r","date":"2022-08-29","objectID":"/promql/:0:0","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"介绍 PromQL 是 Prometheus 内置的数据查询语言，其提供对时间序列数据丰富的查询，聚合以及逻辑运算能力的支持。 并且被广泛应用在 Prometheus 的日常应用当中，包括对数据查询、可视化、告警处理。可以这么说，PromQL 是 Prometheus 所有应用场景的基础，理解和掌握 PromQL 是我们使用 Prometheus 必备的技能。 ","date":"2022-08-29","objectID":"/promql/:1:0","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"PromQL 简介 Prometheus Query Language (PromQL) 是一个专为Prometheus监控系统设计的强大查询语言， 它允许用户对收集的时间序列数据进行高效、灵活的查询和分析。PromQL的设计哲学在于提供简洁而强大的语法，以支持复杂的数据检索和实时监控场景 Prometheus和PromQL的关系 Prometheus是一个开源的系统监控和警报工具包，广泛用于云原生环境中。它通过收集和存储时间序列数据，支持实时监控和警报。 PromQL作为Prometheus的核心组件，允许用户通过强大的查询语言对这些数据进行检索和分析。无论是简单的数据查看还是复杂的性能分析， PromQL都能够提供必要的工具来满足用户的需求。 PromQL的设计哲学 PromQL的设计哲学围绕着几个关键点：灵活性、表现力和性能。它旨在提供足够的灵活性，以支持从简单到复杂的各种查询需求， 同时保持查询表达式的简洁性。此外，PromQL经过优化以支持高效的数据处理和检索，这对于实时监控系统来说至关重要。 ","date":"2022-08-29","objectID":"/promql/:1:1","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"PromQL基础概念 ","date":"2022-08-29","objectID":"/promql/:2:0","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"数据类型和结构 在PromQL中，主要操作以下几种数据类型： 即时向量（Instant Vector） 即时向量是一个时间点上的一组时间序列，每个时间序列具有一个唯一的标签集合和一个数值。它通常用于表示某一瞬间的系统状态。 假设我们有一个监控系统的CPU使用率的时间序列，其查询表达式可能如下： cpu_usage{host=\"server01\"} 该查询返回“server01”主机上最新的CPU使用率数据。 区间向量（Range Vector） 区间向量是在一段时间范围内的一组时间序列，它可以用来分析时间序列的变化趋势或计算时间序列的移动平均等。 要查询过去5分钟内server01主机的CPU使用率数据： cpu_usage{host=\"server01\"}[5m] 标量（Scalar） 标量是一个简单的数值类型，它不带有时间戳，通常用于数学计算或与时间序列数据的比较。 假设我们想要将server01主机的CPU使用率与一个固定阈值进行比较： cpu_usage{host=\"server01\"} \u003e 80 这里“80”就是一个标量值。 字符串（String） 字符串类型在PromQL中用得较少，主要用于标签值的展示。 ","date":"2022-08-29","objectID":"/promql/:2:1","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"时间序列（Time Series）和指标（Metrics） 时间序列 在时间序列中的每一个点称为一个样本（sample），样本由以下三部分组成： 指标(metric)：metric name 和描述当前样本特征的 labelsets 时间戳(timestamp)：一个精确到毫秒的时间戳 样本值(value)： 一个 float64 的浮点型数据表示当前样本的值 示例 \u003c--------------- metric ---------------------\u003e\u003c-timestamp -\u003e\u003c-value-\u003e http_request_total{status=\"200\", method=\"GET\"}@1434417560938 =\u003e 94355 http_request_total{status=\"200\", method=\"GET\"}@1434417561287 =\u003e 94334 http_request_total{status=\"404\", method=\"GET\"}@1434417560938 =\u003e 38473 http_request_total{status=\"404\", method=\"GET\"}@1434417561287 =\u003e 38544 http_request_total{status=\"200\", method=\"POST\"}@1434417560938 =\u003e 4748 http_request_total{status=\"200\", method=\"POST\"}@1434417561287 =\u003e 4785 指标类型 从存储上来讲所有的监控指标 metric 都是相同的，但是在不同的场景下这些 metric 又有一些细微的差异。 例如，在 Node Exporter 返回的样本中指标 node_load1 反应的是当前系统的负载状态，随着时间的变化这个指标返回的样本数据是在不断变化的。 而指标 node_cpu_seconds_total 所获取到的样本数据却不同，它是一个持续增大的值，因为其反应的是 CPU 的累计使用时间，从理论上讲只要系统不关机，这个值是会一直变大。 为了能够帮助用户理解和区分这些不同监控指标之间的差异，Prometheus 定义了 4 种不同的指标类型： Counter（计数器） Gauge（仪表盘） Histogram（直方图） Summary（摘要） 在 node-exporter 返回的样本数据中，其注释中也包含了该样本的类型。例如： # HELP node_cpu_seconds_total Seconds the cpus spent in each mode. # TYPE node_cpu_seconds_total counter node_cpu_seconds_total{cpu=\"cpu0\",mode=\"idle\"} 362812.7890625 Counter Counter (只增不减的计数器) 类型的指标其工作方式和计数器一样，只增不减。常见的监控指标，如 http_requests_total、node_cpu_seconds_total 都是 Counter 类型的监控指标。 Counter 是一个简单但又强大的工具，例如我们可以在应用程序中记录某些事件发生的次数，通过以时间序列的形式存储这些数据，我们可以轻松的了解该事件产生的速率变化。PromQL 内置的聚合操作和函数可以让用户对这些数据进行进一步的分析，例如，通过 rate() 函数获取 HTTP 请求量的增长率： rate(prometheus_http_requests_total[5m]) or rate(http_requests_total[5m]) Gauge 与 Counter 不同，Gauge（可增可减的仪表盘）类型的指标侧重于反应系统的当前状态。因此这类指标的样本数据可增可减。常见指标如：node_memory_MemFree_bytes（主机当前空闲的内存大小）、node_memory_MemAvailable_bytes（可用内存大小）都是 Gauge 类型的监控指标。 通过 Gauge 指标，用户可以直接查看系统的当前状态： node_memory_MemFree_bytes Histogram 和 Summary 除了 Counter 和 Gauge 类型的监控指标以外，Prometheus 还定义了 Histogram 和 Summary 的指标类型。Histogram 和 Summary 主用用于统计和分析样本的分布情况。 在大多数情况下人们都倾向于使用某些量化指标的平均值，例如 CPU 的平均使用率、页面的平均响应时间，这种方式也有很明显的问题，以系统 API 调用的平均响应时间为例：如果大多数 API 请求都维持在 100ms 的响应时间范围内，而个别请求的响应时间需要 5s，那么就会导致某些 WEB 页面的响应时间落到中位数上，而这种现象被称为长尾问题。 为了区分是平均的慢还是长尾的慢，最简单的方式就是按照请求延迟的范围进行分组。例如，统计延迟在 0~10ms 之间的请求数有多少而 10~20ms 之间的请求数又有多少。通过这种方式可以快速分析系统慢的原因。Histogram 和 Summary 都是为了能够解决这样的问题存在的，通过 Histogram 和Summary 类型的监控指标，我们可以快速了解监控样本的分布情况。 例如，指标 prometheus_tsdb_wal_fsync_duration_seconds 的指标类型为 Summary。它记录了 Prometheus Server 中 wal_fsync 的处理时间，通过访问 Prometheus Server 的 /metrics 地址，可以获取到以下监控样本数据： # HELP prometheus_tsdb_wal_fsync_duration_seconds Duration of WAL fsync. # TYPE prometheus_tsdb_wal_fsync_duration_seconds summary prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.5\"} 0.012352463 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.9\"} 0.014458005 prometheus_tsdb_wal_fsync_duration_seconds{quantile=\"0.99\"} 0.017316173 prometheus_tsdb_wal_fsync_duration_seconds_sum 2.888716127000002 prometheus_tsdb_wal_fsync_duration_seconds_count 216 从上面的样本中可以得知当前 Prometheus Server 进行 wal_fsync 操作的总次数为 216 次，耗时 2.888716127000002s。其中中位数（quantile=0.5）的耗时为 0.012352463，9 分位数（quantile=0.9）的耗时为 0.014458005s。 在 Prometheus Server 自身返回的样本数据中，我们还能找到类型为 Histogram 的监控指标prometheus_tsdb_compaction_chunk_range_seconds_bucket： # HELP prometheus_tsdb_compaction_chunk_range_seconds Final time range of chunks on their first compaction # TYPE prometheus_tsdb_compaction_chunk_range_seconds histogram prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"100\"} 71 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"400\"} 71 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"1600\"} 71 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"6400\"} 71 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"25600\"} 405 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"102400\"} 25690 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"409600\"} 71863 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"1.6384e+06\"} 115928 prometheus_tsdb_compaction_chunk_range_seconds_bucket{le=\"6.5536e+06\"} 2.5687892e+07 promet","date":"2022-08-29","objectID":"/promql/:2:2","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"查询 ","date":"2022-08-29","objectID":"/promql/:3:0","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"简单查询和表达式 PromQL Web UI 的 Graph 选项卡提供了简单的用于查询数据的入口 输入 up，然后点击 Execute，就能查到监控正常的 Target 通过标签选择器过滤出 job 为 kubernetes-pods 的监控 up{job=\"kubernetes-pods\"} 注意此时是 up{job=“node-exporter”}属于绝对匹配，PromQL 也支持如下表达式： !=：不等于； =~：表示等于符合正则表达式的指标； !~：和=~类似，=~表示正则匹配，!~表示正则不匹配。 如果想要查看主机监控的指标有哪些，可以输入 node，会提示所有主机监控的指标： 查询\r假 如 想 要 查 询 Kubernetes 集 群 中 每 个 宿 主 机 的 磁 盘 总 量 ， 可 以 使 用node_filesystem_size_bytes 查询指定分区大小 node_filesystem_size_bytes{mountpoint=\"/\"}： 或者是查询分区不是/boot，且磁盘是/dev/开头的分区大小（结果不再展示）： node_filesystem_size_bytes{device=~\"/dev/.*\", mountpoint!=\"/boot\"} 查询主机 192.168.1.11 在最近 5 分钟可用的磁盘空间变化： node_filesystem_size_bytes{mountpoint=\"/\",instance=\"192.168.1.11:9100\"}[5m] 目前支持的范围单位如下： s：秒 m：分钟 h：小时 d：天 w：周 y：年 查询 10 分钟之前磁盘可用空间，只需要指定 offset 参数即可 node_filesystem_avail_bytes{instance=\"192.168.1.11:9100\", mountpoint=\"/\", device=\"/dev/mapper/centos-root\"} offset 10m 查询 10 分钟之前，5 分钟区间的磁盘可用空间的变化 node_filesystem_avail_bytes{instance=\"192.168.1.11:9100\", mountpoint=\"/\", device=\"/dev/mapper/centos-root\"}[5m] offset 10m ","date":"2022-08-29","objectID":"/promql/:3:1","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"使用标签选择器（Label Selectors）过滤数据 多个标签通过逗号进行分隔 kubelet_http_requests_total{method=\"GET\",path =~ \"metrics.*\"} 查看主机过去5分钟已使用内存 node_memory_Active_bytes[5m] 查询结果 Label Selectors\r查看10.10.192.220主机一小时前的使用内存 node_memory_Active_bytes{instance=\"10.10.192.220:9100\"} offset 1h ","date":"2022-08-29","objectID":"/promql/:3:2","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["prometheus"],"content":"高级查询 使用PromQL除了能够方便的按照查询和过滤时间序列以外，PromQL还支持丰富的操作符，用户可以使用这些操作符对进一步的对事件序列进行二次加工。这些操作符包括：数学运算符，逻辑运算符，布尔运算符等等。 数学运算 例如，我们可以通过指标node_memory_MemFree_bytes获取当前主机可用的内存空间大小，其样本单位为Bytes。这是如果客户端要求使用MB作为单位响应数据，那只需要将查询到的时间序列的样本值进行单位换算即可 node_memory_MemFree_bytes / 1024 / 1024 PromQL支持的所有数学运算符如下所示： 使用布尔运算过滤时间序列 查看free 内存值大于512M的主机 (node_memory_MemFree_bytes / 1024 / 1024) \u003e 512 目前，Prometheus支持以下布尔运算符如下： != (不相等) \u003e (大于) \u003c (小于) \u003e= (大于等于) \u003c= (小于等于) 使用集合运算符 使用瞬时向量表达式能够获取到一个包含多个时间序列的集合，我们称为瞬时向量。 通过集合运算，可以在两个瞬时向量与瞬时向量之间进行相应的集合操作。 如时钟未同步告警 min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds \u003e= 16 目前，Prometheus支持以下集合运算符： and (并且) or (或者) unless (排除) 操作符优先级 对于复杂类型的表达式，需要了解运算操作的运行优先级 例如，查询主机的CPU使用率，可以使用表达式： 100-(avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) by(instance,job)* 100) \u003e 80 其中irate是PromQL中的内置函数，用于计算区间向量中时间序列每秒的即时增长率。 内置函数 Prometheus还提供了下列内置的聚合操作符，这些操作符作用域瞬时向量。可以将瞬时表达式返回的样本数据进行聚合，形成一个新的时间序列。 sum (求和) min (最小值) max (最大值) avg (平均值) stddev (标准差) stdvar (标准差异) count (计数) count_values (对value进行计数) bottomk (后n条时序) topk (前n条时序) quantile (分布统计) 使用聚合操作的语法如下： \u003caggr-op\u003e([parameter,] \u003cvector expression\u003e) [without|by (\u003clabel list\u003e)] 其中只有count_values, quantile, topk, bottomk支持参数(parameter) without用于从计算结果中移除列举的标签，而保留其它标签。 by则正好相反，结果向量中只保留列出的标签，其余标签则移除。通过without和by可以按照样本的问题对数据进行聚合。 例如： sum(http_requests_total) without (instance) 等价于 sum(http_requests_total) by (code,handler,job,method) 如果只需要计算整个应用的HTTP请求总量，可以直接使用表达式： sum(http_requests_total) count_values用于时间序列中每一个样本值出现的次数。count_values会为每一个唯一的样本值输出一个时间序列，并且每一个时间序列包含一个额外的标签。 例如： count_values(\"count\", http_requests_total) topk和bottomk则用于对样本值进行排序，返回当前样本值前n位，或者后n位的时间序列。 获取HTTP请求数前5位的时序样本数据，可以使用表达式： topk(5, http_requests_total) quantile用于计算当前样本数据值的分布情况quantile(φ, express)其中0 ≤ φ ≤ 1。 例如，当φ为0.5时，即表示找到当前样本数据中的中位数： quantile(0.5, http_requests_total) ","date":"2022-08-29","objectID":"/promql/:3:3","tags":["prometheus"],"title":"PromQL","uri":"/promql/"},{"categories":["Kubernetes"],"content":"使用 kubernetes 经常会用到命令行，这里对命令行进行分类整理 集群管理 kubectl cluster-info kubectl config kubectl version kubectl api-versions 查看当前集群支持的api版本 kubectl get node kubectl get pod kubectl get service kubectl cordon 命令：用于标记某个节点不可调度 kubectl uncordon 命令：用于标签节点可以调度 kubectl drain 命令： 用于在维护期间排除节点。 kubectl taint 命令：用于给某个Node节点设置污点 Pod 管理 kubectl create pod kubectl get pod kubectl get pod pod_name –show-labels kubectl describe pod kubectl logs kubectl exec kubectl delete pod 资源监测 kubectl top node kubectl top pods kubectl get quota kubectl describe Service管理 kubectl create service kubectl get service kubectl expose kubectl describe service kubectl delete service kubectl port-forwad 配置和加密 kubectl create configmap kubectl get configmap kubectl create secret kubectl get secret kubectl describe configmap kubectl describe secret Deployment管理 kubectl create deployment kubectl get deployment kubectl scale deployment sudo kubectl scale deployment deploy-name –replicas=0 不删除应用暂停服务 kubectl autoscale deployment kubectl rollout status kubectl rollout history kubectl delete deployment 名字空间管理 kubectl create namespace kubectl get namespace kubectl describe namespace kubectl delete namespace kubectl apply -n kubectl switch -n kubeadm管理 kubeadm init 启动引导一个 Kubernetes 主节点 kubeadm join 启动引导一个 Kubernetes 工作节点并且将其加入到集群 kubeadm upgrade 更新 Kubernetes 集群到新版本 kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 kubeadm upgrade 命令 kubeadm token 使用 kubeadm join 来管理令牌 kubeadm reset 还原之前使用 kubeadm init 或者 kubeadm join 对节点所作改变 kubeadm version 打印出 kubeadm 版本 kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈 kubeadm token create –print-join-command 创建node加入口令 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:0:0","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"命令说明 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:0","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"1. kubectl expose 和 kubectl port-forwad 的区别 kubectl expose 和 kubectl port-forward 是两个不同的命令，它们在 Kubernetes 中有不同的作用和用法。 kubectl expose 命令用于创建一个新的 Service 对象来暴露 Deployment、ReplicationController 或 ReplicaSet 等其他资源。它会为指定的资源创建一个新的 ClusterIP 类型的 Service，在集群内部通过 Service IP 和端口号来访问暴露的资源。这个 Service 对象可以是临时的，也可以是永久的，它根据指定的参数设置来控制其行为。 kubectl port-forward 命令用于在本地机器和 Kubernetes 集群之间建立一个临时的协议转发，将本地的端口与 Kubernetes Pod 或 Service 的端口进行绑定。这样可以直接通过本地机器上的端口来访问 Kubernetes 集群中的服务或应用。该命令通常用于开发和调试，比如在本地机器上直接访问运行在集群中的应用程序或服务的日志。 总结来说，kubectl expose 用于创建一个新的 Service 对象来暴露 Kubernetes 资源，而 kubectl port-forward 则用于建立本地与集群之间的临时端口转发，方便本地机器访问集群中的服务或应用程序。 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:1","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"2. kubectl apply 和 kubectl switch 的区别 kubectl apply 用于在 Kubernetes 集群上部署或更新资源对象，可以通过文件或目录进行声明。它会读取文件中的资源定义，并在集群中创建或更新相应的资源对象。kubectl switch 是一个非官方的插件，用于切换当前上下文的集群。Kubernetes 集群允许存在多个上下文，每个上下文对应一个集群。 kubectl switch 可以方便地在多个集群之间切换，目的是为了方便用户在不同的集群之间进行操作。因此，kubectl apply 用于部署和更新资源对象，而 kubectl switch 用于切换当前操作的集群。它们是两个不同的命令，用途和功能不同。 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:2","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"3. kubectl scale 和 kubectl autoscale的区别 kubectl scale和kubectl autoscale是用于扩容或缩容Kubernetes Deployment或ReplicaSet的命令，但它们有一些区别。 kubectl scale命令允许通过手动指定副本数量来更新Deployment或ReplicaSet的副本数量。例如，可以使用以下命令将副本数量设置为3： kubectl scale deployment/my-deployment --replicas=3 kubectl autoscale命令则是根据指定的CPU使用率来自动扩容或缩容Deployment。它会创建一个HorizontalPodAutoscaler(HPA)对象，该对象会根据CPU的使用情况动态地增加或减少Pod的副本数量以满足指定的CPU目标使用率。例如，可以使用以下命令将Pod的副本数量自动调整为至少2个，当CPU使用率超过50%时： kubectl autoscale deployment/my-deployment --min=2 --max=10 --cpu-percent=50 总结起来，kubectl scale用于手动指定副本数量来更新Deployment或ReplicaSet的副本数量，而kubectl autoscale用于根据CPU使用率自动扩容或缩容Pod的副本数量。 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:3","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"4. kubectl exec和kubectl attach 的区别 kubectl exec和kubectl attach是用于与正在运行的Pod进行交互的命令，但它们在使用方式和功能上有一些区别。 kubectl exec命令用于在正在运行的Pod中执行命令。它可以连接到正在运行的容器，并在容器中执行指定的命令。例如，可以使用以下命令在名为my-pod的Pod中执行echo命令： kubectl exec -it my-pod -- echo \"Hello, World!\" kubectl attach命令用于将本地终端连接到正在运行的Pod中的某个容器的标准输入、输出和错误流。它可以与容器建立交互式会话。例如，可以使用以下命令将本地终端连接到名为my-pod的Pod中的第一个容器： kubectl attach -it my-pod 总结起来，kubectl attach命令类似于kubectl exec，但它附加到容器中运行的主进程，而不是运行额外的进程。 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:4","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Kubernetes"],"content":"5. kubectl patch和kubectl replace的区别 kubectl patch和kubectl replace是用于部分更新和替换更新Kubernetes资源的命令，但它们在更新方式和行为上有一些区别。 kubectl patch命令用于部分更新Kubernetes资源的特定字段。它可以通过提供一个JSON或YAML文件、一个JSON或YAML字符串，或者使用特定的操作类型（如add、remove、replace等）来更新资源的字段。例如，可以使用以下命令将名为my-deployment的Deployment资源的labels字段添加一个新的标签： kubectl patch deployment/my-deployment -p '{\"spec\":{\"template\":{\"metadata\":{\"labels\":{\"new-label\":\"value\"}}}}}' kubectl replace命令用于替换更新整个Kubernetes资源。它会基于提供的配置文件完全替换现有资源的内容。例如，可以使用以下命令将名为my-deployment的Deployment资源替换为一个新的配置文件： kubectl replace -f new-deployment.yaml 总结起来，kubectl patch用于部分更新Kubernetes资源的特定字段，而kubectl replace用于完全替换更新整个Kubernetes资源。 ","date":"2022-08-20","objectID":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/:1:5","tags":["Kubernetes"],"title":"kubernetes常用命令整理","uri":"/kubernetes%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/"},{"categories":["Grafana"],"content":"Grafana简介 Grafana 是一个监控仪表系统，它是由 Grafana Labs 公司开源的的一个系统监测 (System Monitoring) 工具。它可以大大帮助你简化监控的复杂度，你只需要提供你需要监控的数据，它就可以帮你生成各种可视化仪表。同时它还有报警功能，可以在系统出现问题时通知你。 Grafana 本身是非常轻量级的，不会占用大量资源，此外 Grafana 需要一个数据库来存储其配置数据，比如用户、数据源和仪表盘等，目前 Grafana 支持 SQLite、MySQL、PostgreSQL 3 种数据库，默认使用的是 SQLite，该数据库文件会存储在 Grafana 的安装位置，所以需要对 Grafana 的安装目录进行持久化。 当然如果我们想要部署一个高可用版本的 Grafana 的话，那么使用 SQLite 数据库就不行了，需要切换到 MySQL 或者 PostgreSQL，我们可以在 Grafana 配置的 [database] 部分找到数据库的相关配置，Grafana 会将所有长期数据保存在数据库中，然后部署多个 Grafana 实例使用同一个数据库即可实现高可用。 ","date":"2022-08-17","objectID":"/grafana/:1:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"Grafana的核心功能和架构 Grafana提供了一个丰富的图表库，包括时序数据图、柱状图、饼图等多种类型，使其能够展示各种指标数据。 用户可以通过拖放的方式自定义仪表板，实现对数据的实时监控和分析。Grafana的前端界面使用AngularJS和React构建，后端则主要采用Go语言开发，确保了其高性能和灵活性。 ","date":"2022-08-17","objectID":"/grafana/:1:1","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"Grafana的安装与初步配置 Grafana支持多种安装方式，包括Docker容器、预编译的二进制包、源代码编译等，可以满足不同用户的需求。 ","date":"2022-08-17","objectID":"/grafana/:2:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"Docker 安装 使用Docker安装Grafana是一种快速而便捷的方法。用户只需要准备一个Docker环境，然后运行以下命令即可： docker run -d --name grafana -p 3000:3000 grafana/grafana:9.2.20 此命令会下载Grafana的Docker镜像，并在容器中启动Grafana服务，监听本地的3000端口。 安装完成后，需要对Grafana进行初步配置，包括设置监听端口、配置数据库等。这些配置可以在Grafana的配置文件grafana.ini中进行。 ","date":"2022-08-17","objectID":"/grafana/:2:1","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"kubernetes 安装 kubernetes 部署并切换数据源为MySQL,这样容器重建后配置的大屏不会丢失且企业生产环境也是通过这种方式来使用，方便dashboard大屏脚本在数据库中的管理。 grafana-configmap.yaml，数据库中配置MySQL 数据源连接信息，Grafana 默认使用的数据库是 sqlite3,这里做一下变更 apiVersion: v1 kind: ConfigMap metadata: name: grafana namespace: monitoring data: grafana.ini: | #################################### Database #################################### [database] # You can configure the database connection by specifying type, host, name, user and password # as separate properties or as on string using the url properties. # Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice type = mysql host = 192.168.1.11:3306 name = grafana user = root # If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\" password = abc@123A # Use either URL or the previous fields to configure the database # Example: mysql://user:secret@host:port/database url = mysql://root:password@192.168.1.11:3306/grafana grafana-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: grafana namespace: monitoring labels: app: grafana component: core spec: replicas: 1 selector: matchLabels: app: grafana template: metadata: labels: app: grafana component: core spec: containers: - image: grafana/grafana:9.2.20 name: grafana imagePullPolicy: IfNotPresent #securityContext: # runAsUser: 0 # fsGroup: 0 # env: resources: # keep request = limit to keep this container in guaranteed class limits: cpu: 100m memory: 256Mi requests: cpu: 100m memory: 100Mi env: # The following env variables set up basic auth twith the default admin user and admin password. - name: GF_AUTH_BASIC_ENABLED value: \"true\" - name: GF_AUTH_ANONYMOUS_ENABLED value: \"false\" - name: GF_PATHS_DATA value: \"/var/lib/grafana\" - name: GF_PATHS_CONFIG value: \"/etc/grafana/grafana.ini\" # - name: GF_AUTH_ANONYMOUS_ORG_ROLE # value: Admin # does not really work, because of template variables in exported dashboards: # - name: GF_DASHBOARDS_JSON_ENABLED # value: \"true\" readinessProbe: httpGet: path: /login port: 3000 initialDelaySeconds: 30 timeoutSeconds: 1 volumeMounts: # 数据持久化 - name: config-volume mountPath: /etc/grafana # - name: grafana-persistent-storage # mountPath: /var volumes: - name: config-volume configMap: name: grafana #- name: grafana-persistent-storage # hostPath: # path: /grafana-data # type: DirectoryOrCreate #- name: grafana-persistent-storage # emptyDir: {} grafana-service.yaml apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring labels: app: grafana component: core spec: type: NodePort ports: - port: 3000 targetPort: 3000 nodePort: 30009 selector: app: grafana component: core 部署 kubectl create namespace monitoring kubectl create -f grafana-configmap.yaml kubectl create -f grafana-deploy.yaml kubectl create -f grafana-service.yaml 部署成功 # kubectl -n monitoring get pod,svc NAME READY STATUS RESTARTS AGE pod/grafana-5d8c7d5c8c-znffb 1/1 Running 0 6m18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/grafana NodePort 10.110.17.117 \u003cnone\u003e 3000:30009/TCP 11m 成功部署后，数据库会反向生成相关的表信息 grafana\r","date":"2022-08-17","objectID":"/grafana/:2:2","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"访问功能界面 http://ip:30009 登录是默认账号口令为admin/admin,第一次登录时提示修改密码 grafana\r登录成功界面 grafana\r","date":"2022-08-17","objectID":"/grafana/:3:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"数据源深入集成 grafana\r在Grafana中，数据源的集成是构建有效监控和分析系统的关键步骤。Grafana支持众多流行的数据存储和监控工具作为数据源，包括时序数据库Prometheus, InfluxDB，日志和文档存储如Elasticsearch，以及传统的SQL数据库如MySQL和PostgreSQL。 ","date":"2022-08-17","objectID":"/grafana/:4:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"导入 Dashboard 为了能够快速对系统进行监控，我们可以直接复用别人的 Grafana Dashboard，在 Grafana 的官方网站上就有很多非常优秀的第三方 Dashboard，我们完全可以直接导入进来即可。 比如我们想要对所有的集群节点进行监控，也就是 node-exporter 采集的数据进行展示，这里我们就可以导入 https://grafana.com/grafana/dashboards/8919 这个 Dashboard。 在侧边栏点击 “+\"，选择 Import，在 Grafana Dashboard 的文本框中输入 8919 即可导入： grafana\r注意\r在导入之前需要先创建数据源，大屏是适配的Prometheus 数据源\r通过部署Prometheus 和 Node Exporter 采集的实时数据大屏 grafana\r","date":"2022-08-17","objectID":"/grafana/:5:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"Grafana 相关组件概念了解 ","date":"2022-08-17","objectID":"/grafana/:6:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"面板介绍 面板（Panel）是 Grafana 中基本可视化构建块，每个面板都有一个特定于面板中选择数据源的查询编辑器，每个面板都有各种各样的样式和格式选项，面板可以在仪表板上拖放和重新排列，它们也可以调整大小。 Panel 是 Grafana 中最基本的可视化单元，每一种类型的面板都提供了相应的查询编辑器(Query Editor)，让用户可以从不同的数据源（如 Prometheus）中查询出相应的监控数据，并且以可视化的方式展现，Grafana 中所有的面板均以插件的形式进行使用。 目前内置支持的面板包括：Time series（时间序列）是默认的也是主要的图形可视化面板、State timeline（状态时间表）状态随时间变化 、Status history（状态历史记录）、Bar chart（条形图）、Histogram（直方图）、Heatmap（热力图）、Pie chart（饼状图）、Stat（统计数据）等 图表 Time series 数据统计 stat 仪表盘 gauge 表格 table 饼状图 Pie Chart 热图 Heatmap ","date":"2022-08-17","objectID":"/grafana/:6:1","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"图形面板 数据源（Data Source） 添加面板（Add Panel） 添加参数（Dashboard settings） ","date":"2022-08-17","objectID":"/grafana/:6:2","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"图形定制 多个查询(query editor) 转换(Transform) 模板变量 ","date":"2022-08-17","objectID":"/grafana/:6:3","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"文本面板 # 这是一级标题 ## 这是二级标题 这是正文内容，**我是加粗**，`我是强调`。 \u003e 还可以添加引用的内容 ![这是一张图片](https://p8s.io/docs/assets/img/illustration.png \"grafana\") 渲染后的图片 grafana\r","date":"2022-08-17","objectID":"/grafana/:6:4","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"其它 安全与维护 用户认证与授权 数据备份与恢复 注释 Grafana 中提供了一个 Annotations 注释的方法，通过该方式可以在图形上标记一个点，当将鼠标悬停在上面的时候，可以获得该注释的具体信息，我们可以在 Panel 面板中某一个点添加注释，也可以在某一个区域内添加注释。 告警 Grafana 除了支持丰富的数据源和图表功能之外，还支持告警功能，该功能也使得 Grafana 从一个数据可视化工具成为了一个真正的监控利器。Grafana 可以通过 Alerting 模块的配置把监控数据中的异常信息进行告警，告警的规则可以直接基于现有的数据图表进行配置，在告警的时候也会把出现异常的图表进行通知，使得我们的告警通知更加友好。 ","date":"2022-08-17","objectID":"/grafana/:6:5","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Grafana"],"content":"参考 Grafana 官网 Grafana Dashboard模板-官网 ","date":"2022-08-17","objectID":"/grafana/:7:0","tags":["Grafana"],"title":"Grafana","uri":"/grafana/"},{"categories":["Helm"],"content":"记录一下helm 在生产中的实践 Helm 可以帮助我们管理 Kubernetes 应用程序 - Helm Charts 可以定义、安装和升级复杂的 Kubernetes 应用程序，Charts 包很容易创建、版本管理、分享和分布。 Helm 对于 Kubernetes 来说就相当于 yum 对于 Centos 来说，如果没有 yum 的话，我们在 Centos 下面要安装一些应用程序是极度麻烦的，同样的，对于越来越复杂的 Kubernetes 应用程序来说，如果单纯依靠我们去手动维护应用程序的 YAML 资源清单文件来说，成本也是巨大的。 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:0:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"Helm 与 Kubernetes 版本兼容性对照 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:1:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"Helm 3 版本与 Kubernetes 兼容性 Helm 版本 支持的 Kubernetes 版本范围 重要说明 Helm 3.13.x 1.25 - 1.28 最新稳定版 Helm 3.12.x 1.24 - 1.27 Helm 3.11.x 1.23 - 1.26 Helm 3.10.x 1.22 - 1.25 Helm 3.9.x 1.21 - 1.24 Helm 3.8.x 1.20 - 1.23 Helm 3.7.x 1.19 - 1.22 Helm 3.6.x 1.18 - 1.21 Helm 3.5.x 1.17 - 1.20 Helm 3.4.x 1.16 - 1.19 Helm 3.3.x 1.15 - 1.18 Helm 3.2.x 1.14 - 1.17 Helm 3.1.x 1.13 - 1.16 Helm 3.0.x 1.13 - 1.16 初始 Helm 3 版本 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:1:1","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"Helm 2 版本与 Kubernetes 兼容性 (已弃用) Helm 版本 支持的 Kubernetes 版本范围 状态 Helm 2.17.x 1.14 - 1.17 已弃用 Helm 2.16.x 1.13 - 1.16 已弃用 … … 已弃用 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:1:2","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"重要说明 向后兼容性：Helm 通常支持当前版本和之前几个版本的 Kubernetes。例如，Helm 3.13.x 支持 Kubernetes 1.25-1.28。 向前兼容性：较新版本的 Helm 可能不支持非常旧的 Kubernetes 集群。 最佳实践： 尽量使用 Helm 和 Kubernetes 的最新稳定版本 保持 Helm 版本与 Kubernetes 版本差距不超过 2-3 个小版本 生产环境避免使用边缘版本组合 检查方法： helm version kubectl version 升级建议：升级 Kubernetes 集群时，应考虑同时升级 Helm 到兼容版本。 请注意，具体 Chart 可能对 Kubernetes 版本有额外要求，建议查看 Chart 的文档或 requirements.yaml 文件。 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:1:3","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"安装 由于 Helm V2 版本必须在 Kubernetes 集群中安装一个 Tiller 服务进行通信，这样大大降低了其安全性和可用性，所以在 V3 版本中移除了服务端，采用了通用的 Kubernetes CRD 资源来进行管理，这样就只需要连接上 Kubernetes 即可，而且 V3 版本已经发布了稳定版 这里使用的是helm v3版本,链接 根据kubernetes 版本选择安装合适的helm 版本，这里使用二进制的方式进行安装，安装v3.10.0 版本 # 1. 下载安装包 $ wget https://get.helm.sh/helm-v3.10.0-linux-amd64.tar.gz # 2. 解压 $ tar -zxvf helm-v3.10.0-linux-amd64.tar.gz # 3. move $ mv linux-amd64/helm /usr/local/bin/helm ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:2:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"示例 Helm 其实就是读取的 kubeconfig 文件来访问集群的。 # 查看版本 $ helm version version.BuildInfo{Version:\"v3.10.0\", GitCommit:\"ce66412a723e4d89555dc67217607c6579ffcb21\", GitTreeState:\"clean\", GoVersion:\"go1.18.6\"} # 添加一个 chart 仓库 $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ \"stable\" has been added to your repositories $ helm repo list NAME URL stable http://mirror.azure.cn/kubernetes/charts/ # 用 search 命令来搜索可以安装的 chart 包，已经集成了将近300个可安装的chart包 zoms@crms-10-10-192-220[/tmp]$ helm search repo stable|more NAME CHART VERSION APP VERSION DESCRIPTION stable/acs-engine-autoscaler 2.2.2 2.1.1 DEPRECATED Scales worker nodes within agent pools stable/aerospike 0.3.5 v4.5.0.5 DEPRECATED A Helm chart for Aerospike in Kubern... stable/airflow 7.13.3 1.10.12 DEPRECATED - please use: https://github.com/air... # 安装之前先进行更新 $ helm repo update Hang tight while we grab the latest from your chart repositories... ...Successfully got an update from the \"stable\" chart repository Update Complete. ⎈Happy Helming!⎈ # 安装一个mysql 应用 helm install stable/mysql --generate-name # 了解 MySQL 这个 chart 包的一些特性 helm show chart stable/mysql # 了解 MySQL 这个 chart 包更多的特性 helm show all stable/mysql # 查看已安装的release helm ls # 删除release helm uninstall mysql-1575619811 # 删除release 时保留历史记录，方便取消删除 release（使用 helm rollback 命令） helm uninstall mysql-1575619811 --keep-history ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:3:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"定制 helm show values 命令来查看一个 chart 包的所有可配置的参数选项 查看本地chart包可配置的参数选项 $ helm show values ./nms-grafana/ -n zcm9 # Default values for nms-grafana. # This is a YAML-formatted file. # Declare variables to be passed into your templates fullnameOverride: nms-grafana replicaCount: 1 restartPolicy: Always .... 查看仓库chart包的所有可配置的参数选项,安装之前先看下镜像源是否可先pull下来，以及镜像的版本号 $ helm show values stable/mysql ## mysql image version ## ref: https://hub.docker.com/r/library/mysql/tags/ ## image: \"mysql\" imageTag: \"5.7.30\" strategy: type: Recreate busybox: image: \"busybox\" 上面我们看到的所有参数都是可以用自己的数据来覆盖的，可以在安装的时候通过 YAML 格式的文件来传递这些参数 $ cat config.yaml mysqlUser: user0 mysqlPassword: user0pwd mysqlDatabase: user0db persistence: enabled: false $ helm install -f config.yaml mysql stable/mysql NAME: mysql LAST DEPLOYED: Fri Dec 6 17:46:56 2019 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: MySQL can be accessed via port 3306 on the following DNS name from within your cluster: mysql.default.svc.cluster.local release 安装成功后查看对应的Pod信息 $ kubectl get pod -l release=mysql NAME READY STATUS RESTARTS AGE mysql-ddd798f48-gnrzd 0/1 PodInitializing 0 119s $ kubectl describe pod mysql-ddd798f48-gnrzd ...... Environment: MYSQL_ROOT_PASSWORD: \u003cset to the key 'mysql-root-password' in secret 'mysql'\u003e Optional: false MYSQL_PASSWORD: \u003cset to the key 'mysql-password' in secret 'mysql'\u003e Optional: false MYSQL_USER: user0 MYSQL_DATABASE: user0db ...... 可以看到环境变量 MYSQL_USER=user0，MYSQL_DATABASE=user0db 的值和我们上面配置的值是一致的 在安装过程中，有两种方法可以传递配置数据： –values（或者 -f）：指定一个 YAML 文件来覆盖 values 值，可以指定多个值，最后边的文件优先 –set：在命令行上指定覆盖的配置 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:4:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"更多安装方式 helm install 命令可以从多个源进行安装： chart 仓库（类似于上面我们提到的） 本地 chart 压缩包（helm install foo-0.1.1.tgz） 本地解压缩的 chart 目录（helm install foo path/to/foo） 在线的 URL（helm install fool https://example.com/charts/foo-1.2.3.tgz） ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:5:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Helm"],"content":"升级和回滚 当新版本的 chart 包发布的时候，或者当你要更改 release 的配置的时候，你可以使用 helm upgrade 命令来操作。升级需要一个现有的 release，并根据提供的信息对其进行升级。因为 Kubernetes charts 可能很大而且很复杂，Helm 会尝试以最小的侵入性进行升级，它只会更新自上一版本以来发生的变化： $ helm upgrade -f config.yaml mysql stable/mysql Release \"mysql\" has been upgraded. Happy Helming! NAME: mysql LAST DEPLOYED: Fri Dec 6 21:06:11 2019 NAMESPACE: default STATUS: deployed REVISION: 2 ... 查看升级后的新设置是否生效 $ helm get values mysql USER-SUPPLIED VALUES: mysqlDatabase: user0db mysqlPassword: user0pwd mysqlRootPassword: passw0rd mysqlUser: user0 persistence: enabled: false 另外如生产基本上是只替换镜像tag $ sudo helm get values nms-grafana -n zcm9 USER-SUPPLIED VALUES: global: nodeSelector: aik: zcm9 repository: 10.10.192.220:52800 nms-activemq: image: tag: C_20220628200611 nms-grafana: image: tag: C_20220726192751-v9.0.1 ... 查看当前release 当前版本信息以及查看某个release 的历史 $ sudo helm -n zcm9 ls NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION minio zcm9 1 2022-08-11 19:27:49.792207699 +0800 CST failed minio-10.0.4 2022.1.8 nms-activemq zcm9 1 2022-11-09 13:31:42.996290693 +0800 CST deployed nms-activemq-0.1.0 9.0.44 nms-grafana zcm9 29 2023-05-04 20:34:46.578214434 +0800 CST deployed nms-grafana-0.1.0 aik ... $ sudo helm -n zcm9 history nms-grafana REVISION UPDATED STATUS CHART APP VERSION DESCRIPTION 20 Thu May 4 19:52:14 2023 superseded nms-grafana-0.1.0 aik Upgrade complete 21 Thu May 4 19:53:14 2023 superseded nms-grafana-0.1.0 aik Upgrade complete 22 Thu May 4 19:58:59 2023 superseded nms-grafana-0.1.0 aik Upgrade complete ... 回滚 # 上面可知本地安装的应用nms-grafana 迭代至了版本29,如果要回滚至指定版本可以通过一下方式 $ helm -n zcm9 rollback nms-grafana 21 ","date":"2022-07-21","objectID":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/:6:0","tags":["Helm"],"title":"Helm 生产实践记录","uri":"/helm%E7%94%9F%E4%BA%A7%E5%AE%9E%E8%B7%B5%E8%AE%B0%E5%BD%95/"},{"categories":["Django"],"content":"Docker 是一个用于构建、运行和分发应用程序的平台。它允许您将应用程序及其所有依赖项打包到一个容器中，然后可以在任何安装了 Docker 的服务器上运行。 这使得 Docker 成为部署 Web 应用程序的理想选择，因为它可以轻松地将应用程序从一个环境移动到另一个环境，而无需担心任何兼容性问题。 另一方面，Django 是一个 Python Web 框架，可以轻松创建强大且可扩展的 Web 应用程序。 Django 提供了许多开箱即用的功能，例如用户身份验证系统、数据库抽象层和模板引擎。 这使得 Django 的入门变得容易，并且快速、轻松地构建复杂的 Web 应用程序。 Docker 化和部署 Django 应用程序是一个相对简单的过程。涉及的主要步骤是： 为您的 Django 应用程序创建 Dockerfile。 从 Dockerfile 构建 Docker 映像。 将Docker镜像部署到生产环境。 ","date":"2022-07-20","objectID":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/:0:0","tags":["Django"],"title":"Django 应用容器化构建","uri":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/"},{"categories":["Django"],"content":"准备 基于 Python 3.9、Django 3.2、docker 20.10.13 ","date":"2022-07-20","objectID":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/:1:0","tags":["Django"],"title":"Django 应用容器化构建","uri":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/"},{"categories":["Django"],"content":"软件安装 Docker Python3 ","date":"2022-07-20","objectID":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/:1:1","tags":["Django"],"title":"Django 应用容器化构建","uri":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/"},{"categories":["Django"],"content":"创建Django项目 pip install django==3.2 django-admin startproject django-demo 这行代码将会在当前目录下创建一个 simplesite 目录 让我们看看 startproject 创建了些什么: django-demo/ manage.py django-demo/ __init__.py settings.py urls.py asgi.py wsgi.py 这些目录和文件的用处是： 最外层的 mysite/ 根目录只是你项目的容器， 根目录名称对 Django 没有影响，你可以将它重命名为任何你喜欢的名称。 manage.py: 一个让你用各种方式管理 Django 项目的命令行工具。 里面一层的 mysite/ 目录包含你的项目，它是一个纯 Python 包。 mysite/init.py：一个空文件，告诉 Python 这个目录应该被认为是一个 Python 包。 mysite/settings.py：Django 项目的配置文件。 mysite/urls.py：Django 项目的 URL 声明，就像你网站的“目录”。 mysite/asgi.py：作为你的项目的运行在 ASGI 兼容的 Web 服务器上的入口。 mysite/wsgi.py：作为你的项目的运行在 WSGI 兼容的Web服务器上的入口。 如果你的当前目录不是外层的 mysite 目录的话，请切换到此目录，然后运行下面的命令： $ python manage.py runserver ","date":"2022-07-20","objectID":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/:1:2","tags":["Django"],"title":"Django 应用容器化构建","uri":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/"},{"categories":["Django"],"content":"Dockerfile # 使用官方Python运行时作为父镜像 FROM python:3.9-alpine LABEL maintainer=\"lushuan2071@126.com\" # 设置 python 环境变量 ENV PYTHONUNBUFFERED 1 # 设置工作目录 WORKDIR /app # 将代码复制到容器中 COPY . /app # 添加 pip 清华镜像源 RUN pip install pip -U -i https://pypi.tuna.tsinghua.edu.cn/simple # 安装依赖 RUN pip install --no-cache-dir -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple # 暴露端口 EXPOSE 8000 # 运行Django应用 CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] 构建镜像 docker build -t mydjangoapp:v1.0 . -f Dockerfile 运行容器 docker run --name mydjangoapp -p 8000:8000 -d mydjangoapp:v1.0 构建记录 # docker build -t mydjangoapp:v1.0 . -f Dockerfile Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. STEP 1/7: FROM localhost/python:3.9-alpine STEP 2/7: LABEL maintainer=\"lushuan2071@126.com\" --\u003e bf70fc48b6d STEP 3/7: WORKDIR /app --\u003e 45b9db8d767 STEP 4/7: COPY . /app --\u003e 7df5ef439ea STEP 5/7: RUN pip install --no-cache-dir -r requirements.txt Collecting asgiref==3.8.1 Downloading asgiref-3.8.1-py3-none-any.whl (23 kB) Collecting Django==3.2.25 Downloading Django-3.2.25-py3-none-any.whl (7.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 139.1 kB/s eta 0:00:00 Collecting mysql-connector-python==8.4.0 Downloading mysql_connector_python-8.4.0-py2.py3-none-any.whl (565 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 565.1/565.1 kB 119.1 kB/s eta 0:00:00 Collecting sqlparse==0.5.0 Downloading sqlparse-0.5.0-py3-none-any.whl (43 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.0/44.0 kB 124.3 kB/s eta 0:00:00 Collecting typing_extensions==4.11.0 Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB) Collecting tzdata==2024.1 Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.4/345.4 kB 136.4 kB/s eta 0:00:00 Collecting pytz Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 505.5/505.5 kB 182.8 kB/s eta 0:00:00 Installing collected packages: pytz, tzdata, typing_extensions, sqlparse, mysql-connector-python, asgiref, Django Successfully installed Django-3.2.25 asgiref-3.8.1 mysql-connector-python-8.4.0 pytz-2024.1 sqlparse-0.5.0 typing_extensions-4.11.0 tzdata-2024.1 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv [notice] A new release of pip is available: 23.0.1 -\u003e 24.1.2 [notice] To update, run: pip install --upgrade pip --\u003e dd901d2c952 STEP 6/7: EXPOSE 8000 --\u003e f1884d94f8e STEP 7/7: CMD [\"python\", \"manage.py\", \"runserver\", \"0.0.0.0:8000\"] COMMIT mydjangoapp:v1.0 --\u003e 9253897f89f Successfully tagged localhost/mydjangoapp:v1.0 9253897f89fa29c91253cdb28e8494453dd2cf330b66a51968ec817e35faf19b # docker images|grep django Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. localhost/mydjangoapp v1.0 9253897f89fa 13 seconds ago 100 MB # docker run --name mydjangoapp -p 8000:8000 -d mydjangoapp:v1.0 Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. 49c212881ab19348ccb12f492a7e6f32f545efeff67daf27df6e2b9c7bf074ee # # docker ps Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 49c212881ab1 localhost/mydjangoapp:v1.0 python manage.py ... 6 seconds ago Up 7 seconds 0.0.0.0:8000-\u003e8000/tcp mydjangoapp # docker ps Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 49c212881ab1 localhost/mydjangoapp:v1.0 python manage.py ... 8 seconds ago Up 8 seconds 0.0.0.0:8000-\u003e8000/tcp mydjangoapp ","date":"2022-07-20","objectID":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/:2:0","tags":["Django"],"title":"Django 应用容器化构建","uri":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/"},{"categories":["Django"],"content":"参考 Python 镜像的全方位指南 原创 ","date":"2022-07-20","objectID":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/:3:0","tags":["Django"],"title":"Django 应用容器化构建","uri":"/django-%E5%BA%94%E7%94%A8%E5%AE%B9%E5%99%A8%E5%8C%96%E6%9E%84%E5%BB%BA/"},{"categories":["linux"],"content":"监控系统资源和进程工具 top 命令 快捷键及使用场景 1：显示每个 CPU 核心的详细信息。当你需要查看系统中每个 CPU 核心的负载和使用情况时，可以按下 1 键。 E：切换累计模式（Cumulative Mode）。在累计模式下，CPU 使用率和内存占用等指标会显示所有进程的累计值而不是单个进程的值。 M：按内存使用排序。按下 M 键后，top 命令会按照内存使用量的大小对进程进行排序，并显示最高的内存使用进程。 P：按 CPU 使用排序。按下 P 键后，top 命令会按照 CPU 使用率的大小对进程进行排序，并显示最高的 CPU 使用进程。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:0:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"内存 内存相关名词释义 1）Working Set Size(WSS)是指一个app保持正常运行所须的内存。比如一个应用在初始阶段申请了100G主存，在实际正常运行时每秒只需要50M，那么这里的50M就是一个WSS 2）RES：resident memory usage。常驻内存 3）RSS Resident Set Size 实际使用物理内存，（包含共享库占用的内存） 4）CACHE，是一种特殊的内存，因为主内存速度不够快，用少量的特别快的但特别昂贵的内存来做缓存加速,就是cache。简单来说，buffer是即将要被写入磁盘的，而cache是被从磁盘中读出来的。 5）SWAP 交换区间，内存不够用使用磁盘充当内存 查看占用内存较高的10个进程 ps -aux | sort -k4nr | head 查看进程占用的内存大小 # 通过端口获取进程号 netstat -anp|grep 9100 tcp6 0 0 :::9100 :::* LISTEN 5505/node_exporter # 通过进程id获取进程占用内存大小 cat /proc/5505/status|grep VmRSS VmRSS: 31500 kB 技巧\r注意：VmRSS对应的值就是物理内存占用\r","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:1:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"IO 查看磁盘io及传递状态 iostat -x 1 3 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:2:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"如何判断磁盘 io 性能达到瓶颈？ iostat 是一个用于监控系统磁盘IO使用情况的工具，可以显示每个磁盘的IO操作、IO延迟和数据吞吐量等信息。当磁盘IO达到瓶颈时，可以通过 iostat 来判断。 有以下几个指标可以帮助你判断磁盘IO是否达到瓶颈： %util：这是磁盘IO利用率的百分比，表示磁盘正在处理IO请求所占用的时间比例。当 %util 高于 80% 或 90% 时，磁盘IO可能已经达到瓶颈状态。 await：这是平均每个IO请求在队列中等待被处理的时间，表示系统平均IO响应时间。当 await 较高(通常大于 10ms)时，说明磁盘IO负载过重，也可能意味着磁盘IO已达到瓶颈。 svctm：这是平均每个IO请求的服务时间，表示磁盘处理IO请求的平均时间。当 svctm 较高时，系统处理IO请求的速度较慢，可能代表磁盘IO已经达到瓶颈状态。 总之，当磁盘IO利用率高、IO等待时间长或服务时间长时，磁盘IO可能已达到瓶颈状态。通常还需要综合考虑诸如系统性能需求、磁盘类型和配置等因素，才能确定是否对系统进行优化或升级。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:2:1","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看磁盘io占用较高的几个进程 可以使用 iotop 命令来查看磁盘IO占用较高的几个进程 $ iotop -oP Total DISK READ : 577.97 K/s | Total DISK WRITE : 115.06 K/s Actual DISK READ: 64.22 K/s | Actual DISK WRITE: 1934.59 K/s PID PRIO USER DISK READ DISK WRITE SWAPIN IO\u003e COMMAND 574 be/4 root 577.97 K/s 0.00 B/s 0.00 % 8.68 % [xfsaild/dm-0] 73282 be/4 root 0.00 B/s 45.49 K/s 0.00 % 0.47 % influxd 53910 be/4 root 0.00 B/s 0.00 B/s 0.00 % 0.02 % [kworker/7:0] 55873 be/4 systemd- 0.00 B/s 2.68 K/s 0.00 % 0.02 % mysqld ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:2:2","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"监控进程使用情况 pidstat 是一个用于监控进程资源使用情况的工具，可以提供有关 CPU、内存、磁盘IO、网络和上下文切换等方面的统计信息。以下是 pidstat 常用命令及其使用场景： 1.查看特定进程的资源使用情况： pidstat -p 该命令可用于监控指定 PID 的进程的资源使用情况，包括 CPU 占用率、内存使用量、上下文切换次数、磁盘IO等。 2.实时监控整个系统中进程的资源使用情况： pidstat -r 该命令可实时显示所有进程的内存使用情况，包括虚拟内存大小、物理内存大小、共享内存大小等。 3.监控进程的CPU使用情况： pidstat -u 通过设置 和 参数，该命令可定期显示进程的 CPU 使用情况，包括用户空间和内核空间的 CPU 使用时间、CPU 使用百分比等。 pidstat 命令常用于性能分析和资源监控，可以帮助定位和解决系统性能问题。具体使用场景包括但不限于： 查找 CPU 密集型进程、监测内存泄漏、检测磁盘IO瓶颈、分析网络传输情况等。 最佳实践 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:3:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"手动清理缓存buffer/cache 1.清理pagecache(页面缓存) echo 1 \u003e /proc/sys/vm/drop_caches #或者 sysctl -w vm.drop_caches=1 2.清理目录缓存 echo 2 \u003e /proc/sys/vm/drop_caches #或者 sysctl -w vm.drop_caches=2 3.清理pagecache、dentries和inodes echo 3 \u003e /proc/sys/vm/drop_caches #或者 sysctl -w vm.drop_caches=3 上面三种方式都是临时释放缓存的方法，想要永久释放缓存，需要在/etc/sysctl.conf文件中配置： vm.drop_caches=1/2/3,然后sysctl -p生效即可 温馨提示： 上面操作在大多数情况下都不会对系统造成伤害，只会有助于释放不用的内存 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:4:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看僵尸进程并kill掉 查看 ps -ef | grep defunct kill kill -9 父进程号 在linux系统中，进程有如下几种状态，它们随时可能处于以上状态中的一种： D = 不可中断的休眠 I = 空闲 R = 运行中 S = 休眠 T = 被调度信号终止 t = 被调试器终止 Z = 僵尸状态 我们可以在命令终端中通过top命令来查看系统进程和它的当前状态。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:4:1","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看进程占用的的物理内存大小指令 cat /proc/`ps -ef|grep grafana|grep -v grep|awk '{print $2}'`/status |grep VmRSS 指令分解 # 通过关键词xxx确认进程id，进程关键词如grafana $ ps -ef|grep grafana|grep -v grep|awk '{print $2}' 118581 # 通过进程标识查看服务器物理内存占用 $ cat /proc/118581/status |grep VmRSS VmRSS: 100788 kB # 动态查看进程资源占用 top -p 118581 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:5:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["linux"],"content":"查看TCP连接数 netstat -n | awk '/^tcp/ {++state[$NF]} END {for(key in state) print key,\"\\t\",state[key]}' 总结 Linux 服务器性能资源排查思路 在排查 Linux 服务器性能问题时，可以按照以下思路进行操作： 1.监测系统资源使用情况： 使用命令 top 或 htop 实时监视 CPU、内存和交换空间的使用情况。 使用命令 df 查看磁盘空间使用情况。 使用命令 free 查看内存使用情况。 使用命令 iostat 查看磁盘IO使用情况。 使用命令 sar 查看系统整体的资源利用率。 2.检查进程和服务： 使用命令 ps 或 top 查看正在运行的进程和它们的资源占用情况。 使用命令 systemctl status 检查服务的状态。 3.定位高负载进程： 使用命令 top 或 htop 查看 CPU 使用率最高的进程，并观察其 PID 和资源消耗情况。 使用命令 pidstat 监测指定 PID 的进程的资源使用情况。 4.分析日志信息： 查看系统日志（如 /var/log/messages）以了解系统运行期间发生的错误或异常情况。 查看应用程序日志，特别是与性能相关的日志，以寻找可能的瓶颈或错误。 5.检查网络连接和流量： 使用命令 netstat 或 ss 查看网络连接状态。 使用命令 iftop 或 nethogs 实时监测网络流量和使用情况。 6.进行性能调优： 调整系统内核参数，如文件句柄数、内存分配等，以提高性能。 优化关键应用程序的配置，如数据库、Web 服务器等。 7.使用性能分析工具： 使用工具如 perf、strace、tcpdump 等进行更深入的性能分析和故障排查。 这些步骤可以帮助你定位服务器的性能问题，并找出潜在的瓶颈或异常。根据具体的问题，你可能需要进一步深入研究和分析，以找到解决方案。同时，及时备份数据是必要的，以免在调优过程中发生数据丢失或其他意外。 ","date":"2022-06-29","objectID":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/:6:0","tags":["linux"],"title":"Linux 系统性能问题定位","uri":"/linux%E7%B3%BB%E7%BB%9F%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D/"},{"categories":["prometheus"],"content":"监控策略 目标 服务发现模式 监控方法 数据源 从集群各节点kubelet组件中获取节点kubelet的基本运行状态的监控指标 node 白盒监控 kubelet 从集群各节点kubelet内置的cAdvisor中获取，节点中运行的容器的监控指标 node 白盒监控 kubelet 从部署到各个节点的Node Exporter中采集主机资源相关的运行资源 node 白盒监控 node exporter 对于内置了Promthues支持的应用，需要从Pod实例中采集其自定义监控指标 pod 白盒监控 custom pod 获取API Server组件的访问地址，并从中获取Kubernetes集群相关的运行监控指标 endpoints 白盒监控 api server 获取集群中Service的访问地址，并通过Blackbox Exporter获取网络探测指标 service 黑盒监控 blackbox exporter 获取集群中Ingress的访问信息，并通过Blackbox Exporter获取网络探测指标 ingress 黑盒监控 blackbox exporter ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:1:0","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"k8s 插件采集指标说明 包含夜莺指标做横向对比参考 ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:0","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"Cadvisor 指标说明 cpu 指标 指标名 含义 prometheus metrics或计算方式 说明 cpu.util 容器cpu使用占其申请的百分比 sum (rate (container_cpu_usage_seconds_total[1m])) by( container) /( sum (container_spec_cpu_quota) by(container) /100000) * 100 0-100的范围 cpu.idle 容器cpu空闲占其申请的百分比 100 - cpu.util 0-100的范围 cpu.user 容器cpu用户态使用占其申请的百分比 sum (rate (container_cpu_user_seconds_total[1m])) by( container) /( sum (container_spec_cpu_quota) by(container) /100000) * 100 0-100的范围 cpu.sys 容器cpu内核态使用占其申请的百分比 sum (rate (container_cpu_sys_seconds_total[1m])) by( container) /( sum (container_spec_cpu_quota) by(container) /100000) * 100 0-100的范围 cpu.cores.occupy 容器cpu使用占用机器几个核 rate(container_cpu_usage_seconds_total[1m]) 0到机器核数上限,结果为1就是占用1个核 cpu.spec.quota 容器的CPU配额 container_spec_cpu_quota 为容器指定的CPU个数*100000 cpu.throttled.util 容器CPU执行周期受到限制的百分比 未录入 0-100的范围 cpu.periods 容器生命周期中度过的cpu周期总数 counter型无需计算 使用rate/increase 查看 cpu.throttled.periods 容器生命周期中度过的受限的cpu周期总数 counter型无需计算 使用rate/increase 查看 cpu.throttled.time 容器被节流的总时间 ) counter型无需计算 单位(纳秒 mem 指标 夜莺指标名 含义 prometheus metrics或计算方式 说明 mem.bytes.total 容器的内存限制 无需计算 单位byte 对应pod yaml中resources.limits.memory mem.bytes.used 当前内存使用情况，包括所有内存，无论何时访问 container_memory_rss + container_memory_cache + kernel memory 单位byte mem.bytes.used.percent 容器内存使用率 container_memory_usage_bytes/container_spec_memory_limit_bytes *100 范围0-100 mem.bytes.workingset 容器真实使用的内存量，也是limit限制时的 oom 判断依据 container_memory_max_usage_bytes \u003e container_memory_usage_bytes \u003e= container_memory_working_set_bytes \u003e container_memory_rss 单位byte mem.bytes.workingset.percent 容器真实使用的内存量百分比 container_memory_working_set_bytes/container_spec_memory_limit_bytes *100 范围0-100 mem.bytes.cached 容器cache内存量 container_memory_cache 单位byte mem.bytes.rss 容器rss内存量 container_memory_rss 单位byte mem.bytes.swap 容器cache内存量 container_memory_swap 单位byte filesystem \u0026\u0026 disk.io 指标 夜莺指标名 含义 prometheus metrics或计算方式 说明 disk.bytes.total 容器可以使用的文件系统总量 container_fs_limit_bytes (单位：字节) disk.bytes.used 容器已经使用的文件系统总量 container_fs_usage_bytes (单位：字节) disk.bytes.used.percent 容器文件系统使用百分比 container_fs_usage_bytes/container_fs_limit_bytes *100 范围0-100 disk.io.read.bytes 容器io.read qps rate(container_fs_reads_bytes_total)[1m] (单位：bps) disk.io.write.bytes 容器io.write qps rate(container_fs_write_bytes_total)[1m] (单位：bps) network 指标 夜莺指标名 含义 prometheus metrics或计算方式 说明 net.in.bytes 容器网络接收数据总数 rate(container_network_receive_bytes_total)[1m] (单位：bytes/s) net.out.bytes 容器网络积传输数据总数） rate(container_network_transmit_bytes_total)[1m] (单位：bytes/s) net.in.pps 容器网络接收数据包pps rate(container_network_receive_packets_total)[1m] (单位：p/s) net.out.pps 容器网络发送数据包pps rate(container_network_transmit_packets_total)[1m] (单位：p/s) net.in.errs 容器网络接收数据错误数 rate(container_network_receive_errors_total)[1m] (单位：bytes/s) net.out.errs 容器网络发送数据错误数 rate(container_network_transmit_errors_total)[1m] (单位：bytes/s) net.in.dropped 容器网络接收数据包drop pps rate(container_network_receive_packets_dropped_total)[1m] (单位：p/s) net.out.dropped 容器网络发送数据包drop pps rate(container_network_transmit_packets_dropped_total)[1m] (单位：p/s) container_network_{tcp,udp}_usage_total 默认不采集是因为 –disable_metrics=tcp, udp ,因为开启cpu压力大 system 指标 夜莺指标名 含义 prometheus metrics或计算方式 说明 sys.ps.process.count 容器中running进程个数 container_processes (单位：个) sys.ps.thread.count 容器中进程running线程个数 container_threads (单位：个) sys.fd.count.used 容器中打开文件描述符个数 container_file_descriptors (单位：个) sys.fd.soft.ulimits 容器中root process Soft ulimit container_ulimits_soft (单位：个) sys.socket.count.used 容器中打开套接字个数 container_sockets (单位：个) sys.task.state 容器中task 状态分布 container_tasks_state (单位：个) ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:1","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"kube-apiserver metrics 指标说明 指标名 类型 含义 说明 apiserver_request_total counter 对APIServer不同请求的计数。 apiserver_request_duration_seconds_sum gauge 统计APIServer客户端对APIServer的访问时延。对APIServer不同请求的时延分布。 apiserver_request_duration_seconds_count gauge 请求延迟记录数 计算平均延迟:apiserver_request_duration_seconds_sum/apiserver_request_duration_seconds_count apiserver_admission_controller_admission_duration_seconds_bucket Gauge 准入控制器（Admission Controller）的处理延时。标签包括准入控制器名字、操作（CREATE、UPDATE、CONNECT等）、API资源、操作类型（validate或admit）和请求是否被拒绝（true或false）。 apiserver_admission_webhook_admission_duration_seconds_bucket Gauge 准入Webhook（Admission Webhook）的处理延时。标签包括准入控制器名字、操作（CREATE、UPDATE、CONNECT等）、API资源、操作类型（validate或admit）和请求是否被拒绝（true或false）。 apiserver_admission_webhook_admission_duration_seconds_count Counter 准入Webhook（Admission Webhook）的处理请求统计。标签包括准入控制器名字、操作（CREATE、UPDATE、CONNECT等）、API资源、操作类型（validate或admit）和请求是否被拒绝（true或false）。 apiserver_response_sizes_sum counter 请求响应大小记录和 apiserver_response_sizes_count counter 请求响应大小记录数 authentication_attempts counter 认证尝试数 authentication_duration_seconds_sum counter 认证耗时记录和 authentication_duration_seconds_count counter 认证耗时记录数 apiserver_tls_handshake_errors_total counter tls握手失败计数 apiserver_client_certificate_expiration_seconds_sum gauge 证书过期时间总数 apiserver_client_certificate_expiration_seconds_count gauge 证书过期时间记录个数 apiserver_client_certificate_expiration_seconds_bucket gauge 证书过期时间分布 apiserver_current_inflight_requests gauge APIServer当前处理的请求数。包括ReadOnly和Mutating两种 apiserver_current_inqueue_requests gauge 是一个表向量， 记录最近排队请求数量的高水位线 apiserver请求限流 apiserver_flowcontrol_current_executing_requests gauge 记录包含执行中（不在队列中等待）请求的瞬时数量 APF api的QOS APIPriorityAndFairness apiserver_flowcontrol_current_inqueue_requests gauge 记录包含排队中的（未执行）请求的瞬时数量 workqueue_adds_total counter wq 入队数 workqueue_retries_total counter wq retry数 workqueue_longest_running_processor_seconds gauge wq中最长运行时间 workqueue_queue_duration_seconds_sum gauge wq中等待延迟记录和 workqueue_queue_duration_seconds_count gauge wq中等待延迟记录数 workqueue_work_duration_seconds_sum gauge wq中处理延迟记录和 workqueue_work_duration_seconds_count gauge wq中处理延迟记录数 up Gauge 服务可用性，1 表示服务可用，0 表示服务不可用 关键指标： $interval 表示间隔时间，例如 5m $quantile 0 ≤ 值 ≤ 1，用于计算当前样本数据值的分布情况，当值为 0.5 时，即表示找到当前样本数据中的中位数 名称 PromQL 说明 API QPS sum (irate(apiserver_request_total[$interval])) APIServer总QPS 读请求成功率 sum(irate(apiserver_request_total{code=~“20.*\",verb=~“GET|LIST”}[$interval]))/sum(irate(apiserver_request_total{verb=~“GET|LIST”}[$interval])) APIServer读请求成功率。 写请求成功率 sum(irate(apiserver_request_total{code=~“20.*\",verb!~“GET|LIST|WATCH|CONNECT”}[$interval]))/sum(irate(apiserver_request_total{verb!~“GET|LIST|WATCH|CONNECT”}[$interval])) APIServer写请求成功率。 在处理读请求数量 sum(apiserver_current_inflight_requests{request_kind=“readOnly”}) 所有 APIServer 当前在处理读请求数量; 不加 sum() 显示每个 apiserver 在处理的读请求数量 在处理写请求数量 sum(apiserver_current_inflight_requests{request_kind=“mutating”}) APIServer当前在处理写请求数量（老版本 k8s requestKind） GET读请求时延 histogram_quantile($quantile, sum(irate(apiserver_request_duration_seconds_bucket{verb=“GET”,resource!=”\",subresource!~“log|proxy”}[$interval])) by (pod, verb, resource, subresource, scope, le)) 展示GET请求的响应时间，维度包括APIServer Pod、Verb(GET)、Resources（例如Configmaps、Pods、Leases等）、Scope（范围例如Namespace级别、Cluster级别）。 LIST读请求时延 histogram_quantile($quantile, sum(irate(apiserver_request_duration_seconds_bucket{verb=“LIST”}[$interval])) by (pod_name, verb, resource, scope, le)) 展示LIST请求的响应时间，维度包括APIServer Pod、Verb(GET)、Resources（例如Configmaps、Pods、Leases等）、Scope（范围例如Namespace级别、Cluster级别）。 写请求时延 histogram_quantile($quantile, sum(irate(apiserver_request_duration_seconds_bucket{verb!~“GET|WATCH|LIST|CONNECT”}[$interval])) by (cluster, pod_name, verb, resource, scope, le)) 展示Mutating请求的响应时间，维度包括APIServer Pod、Verb(GET)、Resources（例如Configmaps、Pods、Leases等）、Scope（范围例如Namespace级别、Cluster级别）。 ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:2","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"etcd metrics 指标说明 指标名 类型 含义 说明 etcd_db_total_size_in_bytes gauge db物理文件大小 etcd_object_counts gauge etcd对象按种类计数 etcd_request_duration_seconds_sum gauge etcd请求延迟记录和 etcd_request_duration_seconds_count gauge etcd请求延迟记录数 ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:3","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"kube-scheduler 指标说明 指标名 类型 含义 说明 scheduler_e2e_scheduling_duration_seconds_sum gauge 端到端调度延迟记录和 scheduler_e2e_scheduling_duration_seconds_count gauge 端到端调度延迟记录数 scheduler_pod_scheduling_duration_seconds_sum gauge 调度延迟记录和 分析次数 scheduler_pod_scheduling_duration_seconds_count gauge 调度延迟记录数 scheduler_pending_pods gauge 调度队列pending pod数 scheduler_queue_incoming_pods_total counter 进入调度队列pod数 scheduler_scheduling_algorithm_duration_seconds_sum gauge 调度算法延迟记录和 scheduler_scheduling_algorithm_duration_seconds_count gauge 调度算法延迟记录数 scheduler_pod_scheduling_attempts_sum gauge 成功调度一个pod 的尝试次数记录和 scheduler_pod_scheduling_attempts_count gauge 成功调度一个pod 的尝试次数记录数 ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:4","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"coredns 指标说明 指标名 类型 含义 说明 coredns_dns_requests_total counter 解析请求数 A记录，AAAA记录，other记录 coredns_dns_responses_total counter 解析响应数 NOERROR，NXDOMAIN，REFUSED coredns_cache_entries gauge 缓存记录数 成功或失败 coredns_cache_hits_total counter 缓存命中数 成功或失败 coredns_cache_misses_total counter 缓存未命中数 成功或失败 coredns_dns_request_duration_seconds_sum gauge 解析延迟记录和 coredns_dns_request_duration_seconds_count gauge 解析延迟记录数 coredns_dns_response_size_bytes_sum gauge 解析响应大小记录和 coredns_dns_response_size_bytes_count gauge 解析响应大小记录数 ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:5","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"kube-stats-metrics 指标说明 pod metrics 指标名 类型 含义 kube_pod_status_phase gauge pod状态统计:Pending，Succeeded，Failed，Running，Unknown kube_pod_container_status_waiting counter pod处于waiting状态，值为1代表waiting kube_pod_container_status_waiting_reason gauge pod处于waiting状态原因：ContainerCreating，CrashLoopBackOff pod启动崩溃,再次启动然后再次崩溃，CreateContainerConfigError，ErrImagePull，ImagePullBackOff，CreateContainerError，InvalidImageName kube_pod_container_status_terminated gauge pod处于terminated状态，值为1代表terminated kube_pod_container_status_terminated_reason gauge pod处于terminated状态原因：OOMKilled，Completed，Error，ContainerCannotRun，DeadlineExceeded，Evicted kube_pod_container_status_restarts_total counter pod中的容器重启次数 kube_pod_container_resource_requests_cpu_cores gauge pod容器cpu limit kube_pod_container_resource_requests_memory_bytes gauge pod容器mem limit(单位:字节) deployment metrics 指标名 类型 含义 kube_deployment_status_replicas gauge dep中的pod num kube_deployment_status_replicas_available gauge dep中的 可用pod num kube_deployment_status_replicas_unavailable gauge dep中的 不可用pod num daemonSet metrics 指标名 类型 含义 kube_daemonset_status_number_available gauge ds 可用数 kube_daemonset_status_number_unavailable gauge ds 不可用数 kube_daemonset_status_number_ready gauge ds ready数 kube_daemonset_status_number_misscheduled gauge 未经过调度运行ds的节点数 kube_daemonset_status_current_number_scheduled gauge ds目前运行节点数 kube_daemonset_status_desired_number_scheduled gauge 应该运行ds的节点数 daemonSet metrics 指标名 类型 含义 kube_statefulset_status_replicas gauge ss副本总数 kube_statefulset_status_replicas_current gauge ss当前副本数 kube_statefulset_status_replicas_updated gauge ss已更新副本数 kube_statefulset_replicas gauge ss目标副本数 Job metrics 指标名 类型 含义 kube_job_status_active gauge job running pod数 kube_job_status_succeeded gauge job 成功 pod数 kube_job_status_failed gauge job 失败 pod数 kube_job_complete gauge job 是否完成 kube_job_failed gauge job 是否失败 CronJob metrics 指标名 类型 含义 kube_cronjob_status_active gauge job running pod数 kube_cronjob_spec_suspend gauge =1代表 job 被挂起 kube_cronjob_next_schedule_time gauge job 下次调度时间 kube_cronjob_status_last_schedule_time gauge job 下次调度时间 PersistentVolume metrics 指标名 类型 含义 kube_persistentvolume_capacity_bytes gauge pv申请大小 kube_persistentvolume_status_phase gauge pv状态:Pending，Available，Bound，Released，Failed PersistentVolumeClaim metrics 指标名 类型 含义 kube_persistentvolumeclaim_resource_requests_storage_bytes gauge pvc request大小 kube_persistentvolumeclaim_status_phase gauge pvc状态:Lost，Bound，Pending ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:6","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["prometheus"],"content":"node metrics 指标说明 指标名 类型 含义 kube_node_status_condition gauge condition:NetworkUnavailable，MemoryPressure，DiskPressure，PIDPressure，Ready kube_node_status_allocatable_cpu_cores gauge 节点可以分配cpu核数 kube_node_status_allocatable_memory_bytes gauge 节点可以分配内存总量(单位：字节) kube_node_spec_taint gauge 节点污点情况 kube_node_status_capacity_memory_bytes gauge 节点内存总量(单位：字节) kube_node_status_capacity_cpu_cores gauge 节点cpu核数 kube_node_status_capacity_pods gauge 节点可运行的pod总数 ","date":"2022-06-29","objectID":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/:2:7","tags":["prometheus"],"title":"Prometheus 监控k8s 组件指标梳理","uri":"/prometheus%E7%9B%91%E6%8E%A7k8s%E6%8C%87%E6%A0%87%E5%92%8C%E7%9B%91%E6%8E%A7%E7%BB%84%E4%BB%B6%E7%AD%96%E7%95%A5/"},{"categories":["Kubernetes"],"content":"理解ConfigMap 为了能够准确和深刻理解Kubernetes ConfigMap的功能和价值，我们需要从Docker说起。我们知道，Docker通过将程序、依赖库、数据及配置文件“打包固化”到一个不变的镜像文件中的做法，解决了应用的部署的难题，但这同时带来了棘手的问题，即配置文件中的参数在运行期如何修改的问题。我们不可能在启动Docker容器后再修改容器里的配置文件，然后用新的配置文件重启容器里的用户主进程。为了解决这个问题，Docker提供了两种方式： 在运行时通过容器的环境变量来传递参数； 通过Docker Volume将容器外的配置文件映射到容器内。 这两种方式都有其优势和缺点，在大多数情况下，后一种方式更合适我们的系统，因为大多数应用通常从一个或多个配置文件中读取参数。但这种方式也有明显的缺陷：我们必须在目标主机上先创建好对应的配置文件，然后才能映射到容器里。上述缺陷在分布式情况下变得更为严重，因为无论采用哪种方式，写入（修改）多台服务器上的某个指定文件，并确保这些文件保持一致，都是一个很难完成的目标。此外，在大多数情况下，我们都希望能集中管理系统的配置参数，而不是管理一堆配置文件。针对上述问题，Kubernetes给出了一个很巧妙的设计实现，如下所述。 首先，把所有的配置项都当作keyvalue字符串，当然value可以来自某个文本文件，比如配置项password=123456、user=root、host=192.168.8.4用于表示连接FTP服务器的配置参数。这些配置项可以作为Map表中的一个项，整个Map的数据可以被持久化存储在Kubernetes的Etcd数据库中，然后提供API以方便Kubernetes相关组件或客户应用CRUD操作这些数据，上述专门用来保存配置参数的Map就是Kubernetes ConfigMap资源对象。 ","date":"2022-06-13","objectID":"/kubernetes-configmap%E5%A4%9A%E6%96%87%E4%BB%B6%E6%8C%82%E8%BD%BD%E8%87%B3%E5%90%8C%E4%B8%80%E4%B8%AApod%E5%86%85%E7%9B%AE%E5%BD%95%E5%AE%9E%E8%B7%B5/:1:0","tags":["Kubernetes"],"title":"Kubernetes ConfigMap多文件挂载至同一个pod内目录实践","uri":"/kubernetes-configmap%E5%A4%9A%E6%96%87%E4%BB%B6%E6%8C%82%E8%BD%BD%E8%87%B3%E5%90%8C%E4%B8%80%E4%B8%AApod%E5%86%85%E7%9B%AE%E5%BD%95%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"痛点改进 接下来，Kubernetes提供了一种内建机制，将存储在etcd中的ConfigMap通过Volume映射的方式变成目标Pod内的配置文件，不管目标Pod被调度到哪台服务器上，都会完成自动映射。进一步地，如果ConfigMap中的key/value数据被修改，则映射到Pod中的“配置文件”也会随之自动更新。于是，Kubernetes ConfigMap就成了分布式系统中最为简单（使用方法简单，但背后实现比较复杂）且对应用无侵入的配置中心。 ","date":"2022-06-13","objectID":"/kubernetes-configmap%E5%A4%9A%E6%96%87%E4%BB%B6%E6%8C%82%E8%BD%BD%E8%87%B3%E5%90%8C%E4%B8%80%E4%B8%AApod%E5%86%85%E7%9B%AE%E5%BD%95%E5%AE%9E%E8%B7%B5/:2:0","tags":["Kubernetes"],"title":"Kubernetes ConfigMap多文件挂载至同一个pod内目录实践","uri":"/kubernetes-configmap%E5%A4%9A%E6%96%87%E4%BB%B6%E6%8C%82%E8%BD%BD%E8%87%B3%E5%90%8C%E4%B8%80%E4%B8%AApod%E5%86%85%E7%9B%AE%E5%BD%95%E5%AE%9E%E8%B7%B5/"},{"categories":["Kubernetes"],"content":"实践 需求 将一个configmap 内的两个文件挂载在一个pod的不同目录下 /app/conf/grafanaoracle.properties /app/start-timeshift.sh 创建一个configmap apiVersion: v1 data: grafanaoracle.properties: | name=kevin nage=19 start-timeshift.sh: | #!/bin/sh cd /app/influxdb-timeshift-proxy INFLUXDB=nms-influxdb:8086 /usr/bin/npm run start kind: ConfigMap metadata: labels: app.kubernetes.io/managed-by: Helm name: test-config 创建一个 deploy apiVersion: apps/v1 kind: Deployment metadata: labels: zcm-app: test-configmap name: test-configmap spec: replicas: 1 selector: matchLabels: zcm-app: test-configmap template: metadata: labels: zcm-app: test-configmap spec: containers: - env: - name: CLOUD_APP_NAME value: paas_test-configmap image: nginx imagePullPolicy: IfNotPresent name: test-configmap ports: - containerPort: 9999 name: http-oracle protocol: TCP volumeMounts: # 关键配置 开始 - mountPath: /app/start-timeshift.sh name: properties readOnly: true subPath: start-timeshift.sh - mountPath: /app/conf/grafanaoracle.properties name: properties readOnly: true subPath: grafanaoracle.properties dnsPolicy: ClusterFirst restartPolicy: Always volumes: - configMap: defaultMode: 420 items: - key: grafanaoracle.properties # key 和 path 同名即可 path: grafanaoracle.properties - key: start-timeshift.sh path: start-timeshift.sh name: test-configmap name: properties # 关键配置 结束 ","date":"2022-06-13","objectID":"/kubernetes-configmap%E5%A4%9A%E6%96%87%E4%BB%B6%E6%8C%82%E8%BD%BD%E8%87%B3%E5%90%8C%E4%B8%80%E4%B8%AApod%E5%86%85%E7%9B%AE%E5%BD%95%E5%AE%9E%E8%B7%B5/:3:0","tags":["Kubernetes"],"title":"Kubernetes ConfigMap多文件挂载至同一个pod内目录实践","uri":"/kubernetes-configmap%E5%A4%9A%E6%96%87%E4%BB%B6%E6%8C%82%E8%BD%BD%E8%87%B3%E5%90%8C%E4%B8%80%E4%B8%AApod%E5%86%85%E7%9B%AE%E5%BD%95%E5%AE%9E%E8%B7%B5/"},{"categories":["Django"],"content":"Django 模板语法 模板引擎是一种可以让开发者把服务端数据填充到html网页中完成渲染效果的技术。它实现了把前端代码和服务端代码分离的作用，让项目中的业务逻辑代码和数据表现代码分离，让前端开发者和服务端开发者可以更好的完成协同开发。 信息\r静态网页：页面上的数据都是写死的，万年不变 动态网页：页面上的数据是从后端动态获取的（比如后端获取当前时间；后端获取数据库数据然后传递给前端页面） Django框架中内置了web开发领域非常出名的一个DjangoTemplate模板引擎（DTL）。DTL官方文档 要在django框架中使用模板引擎把视图中的数据更好的展示给客户端，需要完成3个步骤： 信息\r在项目配置文件中指定保存模板文件的模板目录。一般模板目录都是设置在项目根目录或者主应用目录下。 在视图中基于django提供的渲染函数绑定模板文件和需要展示的数据变量 在模板目录下创建对应的模板文件，并根据模板引擎内置的模板语法，填写输出视图传递过来的数据。 配置模板目录：在当前项目根目录下创建了模板目录templates. 然后在settings.py, 模板相关配置，找到TEMPLATES配置项，填写DIRS设置模板目录。 # 模板引擎配置 TEMPLATES = [ { 'BACKEND': 'django.template.backends.django.DjangoTemplates', 'DIRS': [ BASE_DIR / \"templates\", # 路径拼接 ], 'APP_DIRS': True, 'OPTIONS': { 'context_processors': [ 'django.template.context_processors.debug', 'django.template.context_processors.request', 'django.contrib.auth.context_processors.auth', 'django.contrib.messages.context_processors.messages', ], }, }, ] ","date":"2022-05-20","objectID":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/:1:0","tags":["Django"],"title":"Django 模板语法","uri":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/"},{"categories":["Django"],"content":"简单案例 为了方便接下里的演示内容，我这里创建创建一个新的子应用myapp python manage.py startapp myapp settings.py，注册子应用，代码： INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'myapp', # 开发者创建的子应用，这填写就是子应用的导包路径 ] 总路由加载子应用路由,urls.py，代码： from django.contrib import admin from django.urls import path,include urlpatterns = [ path('admin/', admin.site.urls), # path(\"路由前缀/\", include(\"子应用目录名.路由模块\")) path(\"users/\", include(\"users.urls\")), path(\"tem/\", include(\"myapp.urls\")), ] 在子应用目录下创建urls.py子路由文件，代码如下： \"\"\"子应用路由\"\"\" from django.urls import path, re_path from . import views urlpatterns = [ path(\"index\", views.index), ] myapp.views.index，代码： from django.shortcuts import render def index(request): # 要显示到客户端的数据 name = \"hello DTL!\" # return render(request, \"模板文件路径\",context={字典格式：要在客户端中展示的数据}) return render(request, \"index.html\",context={\"name\":name}) ","date":"2022-05-20","objectID":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/:1:1","tags":["Django"],"title":"Django 模板语法","uri":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/"},{"categories":["Django"],"content":"render函数内部本质 django render函数内部本质\rfrom django.shortcuts import render from django.template.loader import get_template from django.http.response import HttpResponse def index(request): name = \"hello world!\" # 1. 初始化模板,读取模板内容,实例化模板对象 # get_template会从项目配置中找到模板目录，我们需要填写的参数就是补全模板文件的路径 template = get_template(\"myapp/index.html\") # 2. 识别context内容, 和模板内容里面的标记[标签]替换,针对复杂的内容,进行正则的替换 context = {\"name\": name} content = template.render(context, request) # render中完成了变量替换成变量值的过程，这个过程使用了正则。 print(content) # 3. 通过response响应对象,把替换了数据的模板内容返回给客户端 return HttpResponse(content) # 上面代码的简写,直接使用 django.shortcuts.render # return render(request, \"index.html\",context={\"name\":name}) # return render(request,\"index.html\", locals()) # data = {} # data[\"name\"] = \"xiaoming\" # data[\"message\"] = \"你好！\" # return render(request,\"index.html\", data) DTL模板文件与普通html文件的区别在哪里？ DTL模板文件是一种带有特殊语法的HTML文件，这个HTML文件可以被Django编译，可以传递参数进去，实现数据动态化。在编译完成后，生成一个普通的HTML文件，然后发送给客户端。 开发中，我们一般把开发中的文件分2种，分别是静态文件和动态文件。 技巧\r静态文件，数据保存在当前文件，不需要经过任何处理就可以展示出去。普通html文件，图片，视频，音频等这一类文件叫静态文件。 动态文件，数据并不在当前文件，而是要经过服务端或其他程序进行编译转换才可以展示出去。 编译转换的过程往往就是使用正则或其他技术把文件内部具有特殊格式的变量转换成真实数据。 动态文件，一般数据会保存在第三方存储设备，如数据库中。django的模板文件，就属于动态文件。 ","date":"2022-05-20","objectID":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/:1:2","tags":["Django"],"title":"Django 模板语法","uri":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/"},{"categories":["Django"],"content":"模板语法 变量渲染（深度查询、过滤器） {{val}} {{val|filter_name:参数}} 标签 嵌套和继承 变量渲染之深度查询 def index(request): name = \"root\" age = 13 sex = True lve = [\"swimming\", \"shopping\", \"coding\", \"game\"] bookinfo = {\"id\": 1, \"price\": 9.90, \"name\": \"Django 3天入门到挣扎\", } book_list = [ {\"id\": 10, \"price\": 9.90, \"name\": \"Django 3天入门到挣扎\", }, {\"id\": 11, \"price\": 19.90, \"name\": \"Django 7天入门到垂死挣扎\", }, ] return render(request, 'index.html', locals()) 模板代码，templates/index.html： \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003ename={{ name }}\u003c/p\u003e \u003cp\u003e{{ age }}\u003c/p\u003e \u003cp\u003e{{ sex }}\u003c/p\u003e \u003cp\u003e列表成员\u003c/p\u003e \u003cp\u003e{{ lve }}\u003c/p\u003e \u003cp\u003e{{ lve.0 }}\u003c/p\u003e \u003cp\u003e{{ lve | last }}\u003c/p\u003e \u003cp\u003e字典成员\u003c/p\u003e \u003cp\u003eid={{ bookinfo.id }}\u003c/p\u003e \u003cp\u003eprice={{ bookinfo.price }}\u003c/p\u003e \u003cp\u003ename={{ bookinfo.name }}\u003c/p\u003e \u003cp\u003e复杂列表\u003c/p\u003e \u003cp\u003e{{ book_list.0.name }}\u003c/p\u003e \u003cp\u003e{{ book_list.1.name }}\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 变量渲染之内置过滤器 语法： {{obj|过滤器名称:过滤器参数}} 内置过滤器 过滤器 用法 代码 last 获取列表/元组的最后一个成员 {{liast | last}} first 获取列表/元组的第一个成员 {{list|first}} length 获取数据的长度 {{list | length}} defualt 当变量没有值的情况下, 系统输出默认值, {{str|default=“默认值”}} safe 让系统不要对内容中的html代码进行实体转义 {{htmlcontent| safe}} upper 字母转换成大写 {{str | upper}} lower 字母转换成小写 {{str | lower}} title 每个单词首字母转换成大写 {{str | title}} date 日期时间格式转换 `{{ value cut 从内容中截取掉同样字符的内容 {{content | cut:“hello”}} list 把内容转换成列表格式 {{content | list}} add 加法 {{num| add}} filesizeformat 把文件大小的数值转换成单位表示 {{filesize | filesizeformat}} join 按指定字符拼接内容 {{list| join(\"-\")}} random 随机提取某个成员 {list | random}} slice 按切片提取成员 {{list | slice:\":-2\"}} truncatechars 按字符长度截取内容 {{content | truncatechars:30}} truncatewords 按单词长度截取内容 同上 过滤器的使用 视图代码 myapp.views.py; def index(request): \"\"\"过滤器 filters\"\"\" content = \"Django template\" # content1 = '\u003cscript\u003ealert(1);\u003c/script\u003e' from datetime import datetime now = datetime.now() content2= \"hello wrold!\" return render(request,\"myapp/index.html\",locals()) # 模板代码,myapp/templates/index.html: {{ content | safe }} {{ content1 | safe }} {# 过滤器本质就是函数,但是模板语法不支持小括号调用,所以需要使用:号分割参数 #} \u003cp\u003e{{ now | date:\"Y-m-d H:i:s\" }}\u003c/p\u003e \u003cp\u003e{{ conten1 | default:\"默认值\" }}\u003c/p\u003e {# 一个数据可以连续调用多个过滤器 #} \u003cp\u003e{{ content2 | truncatechars:6 | upper }}\u003c/p\u003e 自定义过滤器 虽然官方已经提供了许多内置的过滤器给开发者,但是很明显,还是会有存在不足的时候。例如:希望输出用户的手机号码时, 13912345678 —-» 139*****678，这时我们就需要自定义过滤器。要声明自定义过滤器并且能在模板中正常使用,需要完成2个前置的工作: # 1. 当前使用和声明过滤器的子应用必须在setting.py配置文件中的INSTALLED_APPS中注册了!!! INSTALLED_APPS = [ 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'myapp', ] # 2. 自定义过滤器函数必须被 template.register进行装饰使用. # 而且过滤器函数所在的模块必须在templatetags包里面保存 # 在myapp子应用下创建templatetags包[必须包含__init__.py], 在包目录下创建任意py文件 # myapp.templatetags.my_filters.py代码: from django import template register = template.Library() # 自定义过滤器 @register.filter(\"mobile\") def mobile(content): return content[:3]+\"*****\"+content[-3:] # 3. 在需要使用的模板文件中顶部使用load标签加载过滤器文件my_filters.py并调用自定义过滤器 # myapp.views.py,代码: def index(request): \"\"\"自定义过滤器 filters\"\"\" moblie_number = \"13312345678\" return render(request,\"index2.html\",locals()) # templates/index2.html,代码: \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e {{ moblie_number| mobile }} \u003c/body\u003e \u003c/html\u003e 标签 if 视图代码,myapp.views.py: def index(request): name = \"xiaoming\" age = 19 sex = True lve = [\"swimming\", \"shopping\", \"coding\", \"game\"] user_lve = \"sleep\" bookinfo = {\"id\": 1, \"price\": 9.90, \"name\": \"python3天入门到挣扎\", } book_list = [ {\"id\": 10, \"price\": 9.90, \"name\": \"python3天入门到挣扎\", }, {\"id\": 11, \"price\": 19.90, \"name\": \"python7天入门到垂死挣扎\", }, ] return render(request, 'index.html', locals()) 模板代码,templates/index.html，代码： 路由代码： \"\"\"子应用路由\"\"\" from django.urls import path, re_path from . import views urlpatterns = [ # .... path(\"index\", views.index), ] for 视图代码, myapp.views.py: def index7(request): book_list1 = [ {\"id\": 11, \"name\": \"python基础入门\", \"price\": 130.00}, {\"id\": 17, \"name\": \"Go基础入门\", \"price\": 230.00}, {\"id\": 23, \"name\": \"PHP基础入门\", \"price\": 330.00}, {\"id\": 44, \"name\": \"Java基础入门\", \"price\": ","date":"2022-05-20","objectID":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/:1:3","tags":["Django"],"title":"Django 模板语法","uri":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/"},{"categories":["Django"],"content":"静态文件 开发中在开启了debug模式时，django可以通过配置，允许用户通过对应的url地址访问django的静态文件。 setting.py，代码： STATIC_ROOT = BASE_DIR / 'static' STATIC_URL = '/static/' # django模板中，可以引用{{STATIC_URL}}变量避免把路径写死。 总路由，urls.py，代码： from django.views.static import serve as serve_static urlpatterns = [ path('admin/', admin.site.urls), # 对外提供访问静态文件的路由，serve_static 是django提供静态访问支持的映射类。依靠它，客户端才能访问到django的静态文件。 path(r'static/\u003cpath:path\u003e', serve_static, {'document_root': settings.STATIC_ROOT},), ] 注意\r项目上线以后，关闭debug模式时，django默认是不提供静态文件的访问支持，项目部署的时候，我们会通过收集静态文件使用nginx这种web服务器来提供静态文件的访问支持。\r","date":"2022-05-20","objectID":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/:1:4","tags":["Django"],"title":"Django 模板语法","uri":"/django-%E6%A8%A1%E6%9D%BF%E8%AF%AD%E6%B3%95/"},{"categories":["Kubernetes"],"content":"背景 项目现场通过 kubeadm 部署一套 k8s 临时环境 版本是v1.22, 因为是临时环境，部署方式是一主两从的模式， 部署后十天客户直接将主节点的内存进行了扩容，导致现场的应用服务无法访问。 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"现象 打开访问应用门户，提示 nginx 网关访问报错 503 Service Temporarily Unavailable ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"定位 通过 kubectl 查看 pod 状态发现很多应用是 CrashLoopBackOff,猜测是系统基础组件出现异常,查看应用日志提示 jdbc 连接 MySQL 异常，查看MySQL 日志发现端口52306被占用。 排查为 apiserver和etcd连接的随机端口占用了52306 # sudo netstat -nap|grep 52306 tcp 0 0 127.0.0.1:52306 127.0.0.1:2379 ESTABLISHED 30166/kube-apiserve tcp 0 0 127.0.0.1:2379 127.0.0.1:52306 ESTABLISHED 23788/etcd ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"问题处理 确认端口占用情况：执行 sudo netstat -lnp | grep 命令查看指定端口是否被占用 释放端口：如果该端口已被占用，可以通过 sudo lsof -i: 命令查找占用端口的进程 临时处理，pod 重建 kill 掉 etcd 的静态 pod，端口可能还是会重新被占用 永久处理，主机 k8s.conf 预留端口 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:4:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"保留预留端口 $ cat /etc/sysctl.d/k8s.conf net.ipv4.ip_forward = 1 kernel.core_pattern=/tmp/zcore/core.%h~%e fs.inotify.max_user_watches = 1048576 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_local_reserved_ports = 52000-52999 net.ipv4.ip_local_reserved_ports新增预留端口配置，保留端口范围不被占用，告诉内核保留端口范围从 52000 到 52999，以便这些端口不会被普通应用程序占用 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:4:1","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"/etc/sysctl.d/k8s.conf 配置项说明 在 Kubernetes 1.22 版本的 /etc/sysctl.d/k8s.conf 配置文件中，可能包含以下一些配置项： 1、 net.bridge.bridge-nf-call-ip6tables： 含义：控制是否将 IPv6 数据包传递给 iptables 的 netfilter 框架进行处理。如果设置为 1，则表示启用；如果设置为 0，则表示禁用。 默认值：1 2、 net.bridge.bridge-nf-call-iptables： 含义：控制是否将数据包传递给 iptables 的 netfilter 框架进行处理。如果设置为 1，则表示启用；如果设置为 0，则表示禁用。 默认值：1 3、 net.ipv4.ip_forward： 含义：控制 Linux 内核是否允许 IP 数据包转发。如果设置为 1，则表示启用 IP 转发；如果设置为 0，则表示禁用 IP 转发。 默认值：0 4、 net.ipv4.conf.all.forwarding： 含义：控制所有网络接口的 IP 转发功能。如果设置为 1，则表示启用 IP 转发；如果设置为 0，则表示禁用 IP 转发。 默认值：0 5、 net.ipv4.conf.default.forwarding： 含义：控制默认网络接口的 IP 转发功能。如果设置为 1，则表示启用 IP 转发；如果设置为 0，则表示禁用 IP 转发。 默认值：0 以上是常见的示例配置项和默认值，但实际的配置项和默认值可能会根据操作系统和 Kubernetes 版本而有所不同。在实际使用中，请根据文档和操作系统的要求进行正确配置。 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/:5:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次 Apiserver 和 Etcd 连接的随机端口占用冲突问题","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1apiserver%E5%92%8Cetcd%E8%BF%9E%E6%8E%A5%E7%9A%84%E9%9A%8F%E6%9C%BA%E7%AB%AF%E5%8F%A3%E5%8D%A0%E7%94%A8%E5%86%B2%E7%AA%81%E9%97%AE%E9%A2%98/"},{"categories":["Kubernetes"],"content":"问题现象 生产环境无法访问，这里主要是梳理遇到问题应该有的一个排查思路 portal-exception\r","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次项目生产环境故障分析","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes"],"content":"问题排查 登录节点查看namespace 下各pod状态 kubectl get pod -o wide -n prod 发现portal cmdb application等均处于异常状态 由于portal启动会依赖cmdb 优先查看cmdb的日志，发现报错连接redis异常 查看redis状态 处于正常状态 kubectl get pod -o wide -n prod|grep redis 进入cmdb 容器进行redis 连接测试 telnet redis 6379 发现解析域名 redis 有问题 因此怀疑coredns存在问题 查看 coredns 状态 kubectl get pod -o wide -n kube-system|grep coredns 发现pod处于terminating以及pending 由于coredns配置有节点选择器，只会调度到k8s master节点 此外对master做了taint ,—-防止其它的各种系统组件向Master调度，导致master资源受压缩。（此污点对已经调度在该节点的pod不会产生驱逐，但是新建pod的将无法调度） $ kubectl describe node 10.10.xxx Name: 10.10.xxx Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=10.10.xxx kubernetes.io/os=linux kubernetes.io/role=master zcm.role=k8s Annotations: node.alpha.kubernetes.io/ttl: 0 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Sat, 25 Mar 2023 10:08:15 +0800 Taints: scheduler=custom:NoSchedule Unschedulable: false 导致coredns pending 临时处理,将节点选择器移除 coredns调度成功，启动完成 portal cmdb等依赖redis的服务自行恢复 生产环境恢复访问 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次项目生产环境故障分析","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes"],"content":"问题分析 问题发生前，集成对10.10.xxx等k8s master机器进行了迁移操作，主机发生了重启。 因此coredns发生重新调度，此时由于节点选择器以及taint的缘故，coredns无法成功调度启动， 进而影响了容器内对redis的解析 ，导致依赖redis的容器不断重启，生产环境无法访问。 后续对各个k8s集群的coredns配置了对污点的容忍，避免类似问题的再次发生。 ","date":"2022-05-18","objectID":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes 记录一次项目生产环境故障分析","uri":"/%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%95%85%E9%9A%9C%E5%88%86%E6%9E%90/"},{"categories":["Kubernetes"],"content":"背景 statefulset 通过nfs 动态创建pv，发现无法创建成功 ","date":"2022-05-18","objectID":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes 通过 nfs 动态创建 pv 异常","uri":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/"},{"categories":["Kubernetes"],"content":"报错日志 Warning FailedMount 2m50s (x6 over 6m4s) kubelet Unable to attach or mount volumes: unmounted volumes=[data], unattached volumes=[kube-api-access-8p8wf data]: error processing PVC kube-logging/data-es-cluster-0: PVC is not bound 通过报错日志可以看的出来pvc 无法绑定pv,通过命令查看pvc创建成功，pv 则没有进行创建 StatefulSet 挂载 NFS 存储卷时出现了问题,导致 PVC 没有被成功绑定到 PV的原因 常见原因有: PVC 和 PV 不匹配 PVC 的 storage class、访问模式、大小资源请求等需要和 PV 定义一致,否则不会被匹配到合适的 PV。 NFS 服务器配置问题 NFS 服务器需要正常启动、导出共享目录,并且 Kubernetes 节点能够访问到 NFS 服务器。 RBAC 鉴权问题 Kubernetes 节点上的 kubelet 需要有获取、挂载 PV 的权限。 NFS Client 配置问题 Kubernetes 节点上需要安装 NFS Client,并且配置可以访问 NFS 服务器。 StatefulSet 配置错误 StatefulSet 的 volumeClaimTemplates 需要与 PVC 的定义相匹配。 存储卷读写权限问题 存储卷需要有读写权限,避免因权限问题无法挂载。 ","date":"2022-05-18","objectID":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes 通过 nfs 动态创建 pv 异常","uri":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/"},{"categories":["Kubernetes"],"content":"nfs 在kubernetes中动态创建pv和版本的关系 在早期的 Kubernetes 版本中(如 v1.11 之前),NFS provisioner 并不包含在默认部署中,这会导致通过 NFS 存储类无法动态创建 PV。 从 Kubernetes v1.11 开始,NFS 动态供应功能成为了默认部署的一部分,但也需要进行额外的配置,主要步骤包括: 安装 NFS 客户端组件 部署 NFS 动态供应器 external-provisioner 创建 StorageClass,指定 nfs 作为 provisioner 创建 PersistentVolumeClaim, 引用该 StorageClass 此时,NFS 供应器就可以根据 PVC 的请求动态创建 PV 来绑定 PVC。 所以简单来说,Kubernetes 低版本中需要手动安装和配置 NFS 动态供应器; 高版本中已内置但也需要显式配置才能启用该功能。正确配置后就可以通过 NFS StorageClass 来动态创建 PV。 这里的高版本指的是v1.20 之后，在 kube-apiserver 的启动参数中不移除 API 对象中的 selfLink 字段。 selfLink 是 Kubernetes 中每个 API 对象的一个字段,用于表示对象自身的 URL 地址 从 Kubernetes 1.20 版本开始,该字段默认被移除了。但可以通过设置 RemoveSelfLink=false 来保留该字段 原因是 selfLink 字段已很少被用到,但删去后可能会影响部分老版本的客户端。所以提供了这个特性开关来向后兼容 一般来说,除非确实需要兼容老版本客户端,否则不建议保留 selfLink 字段 显式开启方式： $ vi /etc/kubernetes/manifests/kube-apiserver.yaml .... spec: containers: - command: - kube-apiserver - --feature-gates=RemoveSelfLink=false ","date":"2022-05-18","objectID":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes 通过 nfs 动态创建 pv 异常","uri":"/kubernetes%E9%80%9A%E8%BF%87nfs%E5%8A%A8%E6%80%81%E5%88%9B%E5%BB%BApv%E5%BC%82%E5%B8%B8/"},{"categories":["Redis"],"content":"基本概念 Redis（REmote DIctionary Service）是一个开源的键值对数据库服务器。 Redis是一个开源（BSD许可），内存存储的数据结构服务器，可用作数据库，高速缓存和消息队列代理。它支持字符串、哈希表、列表、集合、有序集合，位图，hyperloglogs等数据类型。内置复制、Lua脚本、LRU收回、事务以及不同级别磁盘持久化功能，同时通过Redis Sentinel提供高可用，通过Redis Cluster提供自动分区。 redis explained\r","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:1:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"使用场景 数据库 缓存 MQ ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:1:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"Redis优势 性能极高 数据类型丰富，单键值对最大支持512M大小的数据 简单易用，支持所有主流编程语言 支持数据持久化，主从复制，哨兵模式等高可用特性 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:1:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"竞品对比 Memcached vs Redis redis-vs-memcached\r","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:1:3","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"安装配置 这里安装的环境是Linux Ubuntu 22.04.4 LTS 操作系统，其它安装方式见安装介绍 sudo apt install lsb-release curl gpg curl -fsSL https://packages.redis.io/gpg | sudo gpg --dearmor -o /usr/share/keyrings/redis-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/redis-archive-keyring.gpg] https://packages.redis.io/deb $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/redis.list sudo apt-get update sudo apt-get install redis Linux Centos 安装方式 $ yum install redis -y ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:2:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"启动 Redis root@k8s-master01:~/redis# redis-server 9204:C 17 Jul 2024 17:47:22.376 * oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 9204:C 17 Jul 2024 17:47:22.377 * Redis version=7.2.5, bits=64, commit=00000000, modified=0, pid=9204, just started 9204:C 17 Jul 2024 17:47:22.377 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 9204:M 17 Jul 2024 17:47:22.377 * Increased maximum number of open files to 10032 (it was originally set to 1024). 9204:M 17 Jul 2024 17:47:22.377 * monotonic clock: POSIX clock_gettime _._ _.-``__ ''-._ _.-`` `. `_. ''-._ Redis 7.2.5 (00000000/0) 64 bit .-`` .-```. ```\\/ _.,_ ''-._ ( ' , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|'` _.-'| Port: 6379 | `-._ `._ / _.-' | PID: 9204 `-._ `-._ `-./ _.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | https://redis.io `-._ `-._`-.__.-'_.-' _.-' |`-._`-._ `-.__.-' _.-'_.-'| | `-._`-._ _.-'_.-' | `-._ `-._`-.__.-'_.-' _.-' `-._ `-.__.-' _.-' `-._ _.-' `-.__.-' 9204:M 17 Jul 2024 17:47:22.378 # Warning: Could not create server TCP listening socket *:6379: bind: Address already in use 9204:M 17 Jul 2024 17:47:22.378 # Failed listening on port 6379 (tcp), aborting. root@k8s-master01:~/redis# netstat -anp|grep 6379 tcp 0 0 127.0.0.1:6379 0.0.0.0:* LISTEN 8799/redis-server 1 tcp6 0 0 ::1:6379 :::* LISTEN 8799/redis-server 1 客户端命令连接redis 服务，并通过命令操作redis root@k8s-master01:~/redis# redis-cli 127.0.0.1:6379\u003e set key1 value1 OK 127.0.0.1:6379\u003e get key1 \"value1\" 备注：如果设置的键中有中文查看时是返回的二进制，需要使用redis-cli --raw 登录 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:2:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"图形化工具RedisInsight ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:2:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"使用方式 CLI 命令行 API (Application Programming interface) GUI (Graphical User Interface) ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:2:3","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"常用命令 redis 的使用命令很多,这里重点介绍几种常用的命令 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"Key Commands SET key value #: set a key to a value GET key #: get the value of a key (string only) DEL key #: delete a key EXPIRE key seconds #: set a timeout on a key TTL key #: get the remaining time to live of a key KEYS pattern #: find all keys matching a pattern SCAN cursor [MATCH pattern] [COUNT count] [TYPE type] #: iterate over keys with a cursor ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"String Commands APPEND key value #: append a value to a key STRLEN key #: get the length of the value of a key INCR key #: increment the value of a key by 1 DECR key #: decrement the value of a key by 1 INCRBY key increment #: increment the value of a key by a specified amount DECRBY key decrement #: decrement the value of a key by a specified amount ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"Hash Commands HSET key field value #: set a field in a hash to a value HGET key field #: get the value of a field in a hash HDEL key field [field …] #: delete one or more fields from a hash HGETALL key #: get all fields and values from a hash HKEYS key #: get all fields from a hash HVALS key #: get all values from a hash HEXISTS key field #: check if a field exists in a hash HLEN key #: get the number of fields in a hash hash 命令实际使用方式 # Initialize the hash HMSET user name \"John Doe\" age 30 email \"lushuan2071@126.com\" # Get the value of a field HGET user name # Output: \"John Doe\" # Get the values of multiple fields HMGET user name age # Output: 1) \"John Doe\" 2) \"30\" # Get all fields and values HGETALL user # Output: 1) \"name\" 2) \"John Doe\" 3) \"age\" 4) \"30\" 5) \"email\" 6) \"johndoe@example.com\" # Get all fields HKEYS user # Output: 1) \"name\" 2) \"age\" 3) \"email\" # Get all values HVALS user # Output: 1) \"John Doe\" 2) \"30\" 3) \"johndoe@example.com\" # Get the number of fields HLEN user # Output: 3 # Modify a field HSET user age 31 HGET user age # Output: \"31\" # Increment a field HINCRBY user age 1 HGET user age # Output: \"32\" # Delete a field HDEL user email HKEYS user # Output: 1) \"name\" 2) \"age\" ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:3","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"List Commands LPUSH key value [value …] #: prepend one or more values to a list RPUSH key value [value …] #: append one or more values to a list LPOP key #: remove and return the first element of a list RPOP key #: remove and return the last element of a list LINDEX key index #: get the element at a specified index in a list LLEN key #: get the length of a list LRANGE key start stop #: get a range of elements from a list LREM key count value #: remove a specified number of occurrences of a value from a list list 使用实例 # Initialize the list LPUSH numbers 3 2 1 # Get the length of the list LLEN numbers # Output: 3 # Get an element by index LINDEX numbers 1 # Output: \"2\" # Get a range of elements LRANGE numbers 0 1 # Output: 1) \"1\" 2) \"2\" # Append an element RPUSH numbers 4 LRANGE numbers 0 -1 # Output: 1) \"1\" 2) \"2\" 3) \"3\" 4) \"4\" # Pop an element LPOP numbers # Output: \"1\" RPOP numbers # Output: \"4\" LRANGE numbers 0 -1 # Output: 1) \"2\" 2) \"3\" # Insert an element LINSERT numbers BEFORE 2 1 LRANGE numbers 0 -1 # Output: 1) \"1\" 2) \"2\" 3) \"3\" # Remove an element LREM numbers 1 2 LRANGE numbers 0 -1 # Output: 1) \"1\" 2) \"3\" # Trim the list LTRIM numbers 0 0 LRANGE numbers 0 -1 # Output: 1) \"1\" ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:4","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"Set Commands SADD key member [member …] #: add one or more members to a set SREM key member [member …] #: remove one or more members from a set SMEMBERS key #: get all members of a set SISMEMBER key member #: check if a member exists in a set SINTER key [key …] #: get the intersection of sets SUNION key [key …] #: get the union of sets Set 使用实例，注意，由于set不是有序数据类型，所以运行以下命令时，项的顺序可能会有所不同: # Initialize the set SADD fruits apple banana orange # Get the number of elements in the set SCARD fruits # Output: 3 # Check if an element is a member of the set SISMEMBER fruits apple # Output: 1 (true) # Get all members of the set SMEMBERS fruits # Output: 1) \"apple\" 2) \"banana\" 3) \"orange\" # Remove an element from the set SREM fruits banana SMEMBERS fruits # Output: 1) \"apple\" 2) \"orange\" # Add multiple elements to the set SADD fruits pear peach SMEMBERS fruits # Output: 1) \"apple\" 2) \"orange\" 3) \"pear\" 4) \"peach\" # Get the intersection of multiple sets SADD fruits2 orange kiwi SINTER fruits fruits2 # Output: 1) \"orange\" # Get the union of multiple sets SUNION fruits fruits2 # Output: 1) \"apple\" 2) \"orange\" 3) \"pear\" 4) \"peach\" 5) \"kiwi\" # Get a random element from the set SRANDMEMBER fruits # Output: \"pear\" ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:5","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"Sorted Set Commands ZADD key score member [score member …] #: add one or more members with scores to a sorted set ZREM key member [member …] #: remove one or more members from a sorted set ZRANGE key start stop [WITHSCORES] #: The order of elements is from the lowest to the highest score. Elements with the same score are ordered lexicographically. ZREVRANGE key start stop [WITHSCORES] #: Returns the specified range of elements in the sorted set stored at key. The elements are considered to be ordered from the highest to the lowest score. ZSCORE key member #: get the score of a member in a sorted set 实例 # Initialize the sorted set ZADD scores 90 \"Alice\" 85 \"Bob\" 95 \"Charlie\" # Get the number of elements in the sorted set ZCARD scores # Output: 3 # Get the rank of an element ZRANK scores \"Charlie\" # Output: 2 # Get the score of an element ZSCORE scores \"Bob\" # Output: 85 # Get a range of elements by rank ZRANGE scores 0 1 # Output: 1) \"Bob\" 2) \"Alice\" # Get a range of elements by score ZRANGEBYSCORE scores 90 95 # Output: 1) \"Alice\" 2) \"Charlie\" # Increment the score of an element ZINCRBY scores 10 \"Alice\" ZSCORE scores \"Alice\" # Output: 100 # Remove an element from the sorted set ZREM scores \"Bob\" ZRANGE scores 0 -1 # Output: 1) \"Charlie\" 2) \"Alice\" # Get the highest-scoring element(s) ZREVRANGE scores 0 0 # Output: 1) \"Alice\" ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:3:6","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"十大数据模型 redis 初始化不同的数据类型，更多数据类型可以查看官网介绍 redis-datatype\r# String SET key value SET name \"John Doe\" # List RPUSH key element [element ...] RPUSH numbers 1 2 3 4 5 # Set SADD key member [member ...] SADD colors red green blue # Hash HSET key field value [field value ...] HSET user id 1 name \"John Doe\" email \"john.doe@example.com\" # Sorted set ZADD key score member [score member ...] ZADD scores 90 \"John Doe\" 80 \"Jane Doe\" 95 \"Bob Smith\" ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:4:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"5种基本数据类型 首先对redis来说，所有的key（键）都是字符串。我们在谈基础数据结构时，讨论的是存储值的数据类型，主要包括常见的5种数据类型，分别是：String、List、Set、Zset、Hash。 redis datatype common in use\r结构类型 结构存储的值 结构的读写能力 String字符串 可以是字符串、整数或浮点数 对整个字符串或字符串的一部分进行操作；对整数或浮点数进行自增或自减操作； List列表 一个链表，链表上的每个节点都包含一个字符串 对链表的两端进行push和pop操作，读取单个或多个元素；根据值查找或删除元素； Set集合 包含字符串的无序集合 字符串的集合，包含基础的方法有看是否存在添加、获取、删除；还包含计算交集、并集、差集等 Hash散列 包含键值对的无序散列表 包含方法有添加、获取、删除单个元素 Zset有序集合 和散列一样，用于存储键值对 字符串成员与浮点数分数之间的有序映射；元素的排列顺序由分数的大小决定；包含方法有添加、获取、删除单个元素以及根据分值范围或成员来获取元素 String字符串 String是redis中最基本的数据类型，一个key对应一个value。 String类型是二进制安全的，意思是 redis 的 string 可以包含任何数据。如数字，字符串，jpg图片或者序列化的对象。 List列表 Redis中的List其实就是链表（Redis用双端链表实现List）。 使用List结构，我们可以轻松地实现最新消息排队功能（比如新浪微博的TimeLine）。List的另一个应用就是消息队列，可以利用List的 PUSH 操作，将任务存放在List中，然后工作线程再用 POP 操作将任务取出进行执行。 Set 集合 Redis 的 Set 是 String 类型的无序集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。 Redis 中集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。 Hash 散列 Redis hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表，hash 特别适合用于存储对象。 Zset有序集合 Redis 有序集合和集合一样也是 string 类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个 double 类型的分数。redis 正是通过分数来为集合中的成员进行从小到大的排序。 有序集合的成员是唯一的, 但分数(score)却可以重复。有序集合是通过两种数据结构实现： ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:4:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"5种高级数据类型 消息队列 Stream 地理空间 Geospatial HyperLogLog 是一种用来做基数统计的算法 Bitmap 即位图数据结构，都是操作二进制位来进行记录，只有0 和 1 两个状态 位域 Bitfield ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:4:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"事务 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:5:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"什么是 Redis 事务 Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:5:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"Redis事务相关命令和使用 MULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。 EXEC：执行事务中的所有操作命令。 DISCARD：取消事务，放弃执行事务块中的所有命令。 WATCH：监视一个或多个key,如果事务在执行前，这个key(或多个key)被其他命令修改，则事务被中断，不会执行事务中的任何命令。 UNWATCH：取消WATCH对所有key的监视。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:5:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"实例 127.0.0.1:6379\u003e MULTI OK 127.0.0.1:6379(TX)\u003e SET key1 1 QUEUED 127.0.0.1:6379(TX)\u003e SET key2 2 QUEUED 127.0.0.1:6379(TX)\u003e EXEC OK OK 127.0.0.1:6379\u003e GET key1 1 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:5:3","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"数据持久化 为了防止数据丢失以及服务重启时能够恢复数据，Redis支持数据的持久化，主要分为两种方式，分别是RDB和AOF; 当然实际场景下还会使用这两种的混合模式。 RDB(Redis Database) AOF(Append Only File) ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:6:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"RDB RDB 就是 Redis DataBase 的缩写，中文名为快照/内存快照，RDB持久化是把当前进程数据生成快照保存到磁盘上的过程，由于是某一时刻的快照，那么快照中的值要早于或者等于内存中的值。 可以理解为数据的快照机制，是某一个时间点上数据的完整副本，就是定时或者手动的给数据做一下快照，所以触发方式有两种，定时或者手动触发 在某个快照时间点修改过的数据就会丢失掉，所以RDB 更适合用来做备份 Redis中默认的周期新设置 # 周期性执行条件的设置格式为 save \u003cseconds\u003e \u003cchanges\u003e # 默认的设置为： save 900 1 save 300 10 save 60 10000 # 以下设置方式为关闭RDB快照功能 save \"\" 实例 xxd 是linux 查看二进制或者16进制的文件内容的命令，rdb备份文件默认目录是/var/lib/redis/ root@k8s-master01:/etc/redis# redis-cli 127.0.0.1:6379\u003e set name lushuan OK 127.0.0.1:6379\u003e save OK 127.0.0.1:6379\u003e exit root@k8s-master01:/etc/redis# cd /var/lib/redis/ root@k8s-master01:/var/lib/redis# ls dump.rdb root@k8s-master01:/var/lib/redis# xxd dump.rdb 00000000: 5245 4449 5330 3031 31fa 0972 6564 6973 REDIS0011..redis 00000010: 2d76 6572 0537 2e32 2e35 fa0a 7265 6469 -ver.7.2.5..redi 00000020: 732d 6269 7473 c040 fa05 6374 696d 65c2 s-bits.@..ctime. 00000030: ffd0 9766 fa08 7573 6564 2d6d 656d c220 ...f..used-mem. 00000040: 7812 00fa 0861 6f66 2d62 6173 65c0 00fe x....aof-base... 00000050: 00fb 0600 0007 636f 7572 7365 321b 4859 ......course2.HY 00000060: 4c4c 0100 0000 0300 0000 0000 0000 6821 LL............h! 00000070: 8048 6c80 4366 844c 0600 0663 6f75 7273 .Hl.Cf.L...cours 00000080: 6524 4859 4c4c 0100 0000 0400 0000 0000 e$HYLL.......... 00000090: 0080 4303 8442 5284 4af7 8057 cf80 486c ..C..BR.J..W..Hl 000000a0: 8043 6684 4c06 0004 6e61 6d65 076c 7573 .Cf.L...name.lus 000000b0: 6875 616e 0006 7265 7375 6c74 2448 594c huan..result$HYL 000000c0: 4c01 0000 0006 0000 0000 0000 0043 0384 L............C.. 000000d0: 4252 844a f780 57cf 8048 6c80 4366 844c BR.J..W..Hl.Cf.L 000000e0: 0600 046b 6579 32c0 0200 046b 6579 31c0 ...key2....key1. 000000f0: 01ff 7d49 6715 3957 c40c ..}Ig.9W.. root@k8s-master01:/var/lib/redis# 触发方式 save命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存 比较大的实例会造成长时间阻塞，线上环境不建议使用 bgsave命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子 进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短 RDB 优缺点 优点 RDB文件是某个时间节点的快照，默认使用LZF算法进行压缩，压缩后的文件体积远远小于内存大小，适用于备份、全量复制等场景； Redis加载RDB文件恢复数据要远远快于AOF方式； 缺点 RDB方式实时性不够，无法做到秒级的持久化； 每次调用bgsave都需要fork子进程，fork子进程属于重量级操作，频繁执行成本较高； RDB文件是二进制的，没有可读性，AOF文件在了解其结构的情况下可以手动修改或者补全； 版本兼容RDB文件问题； 针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:6:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"AOF Redis是“写后”日志，Redis先执行命令，把数据写入内存，然后才记录日志。日志里记录的是Redis收到的每一条命令，这些命令是以文本形式保存。PS: 大多数的数据库采用的是写前日志（WAL），例如MySQL，通过写前日志和两阶段提交，实现数据和逻辑的一致性。 为了解决RDB 在快照时阻塞服务状态，即便是RDB 的bgsave fork一个子进程备份时时间很短，不过在这很短的时间内redis 还是不能处理任何请求，无法做到秒级的快照，为了解决这个问题，可以使用AOF(追加文件)的持久化方式。 当开启AOF redis 重启后会先从磁盘上读取AOF 持久化的数据至内存中。 虽然 bgsave 执行时不阻塞主线程，但是，如果频繁地执行全量快照，也会带来两方面的开销： 一方面，频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。 另一方面，bgsave 子进程需要通过 fork 操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork 这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁 fork 出 bgsave 子进程，这就会频繁阻塞主线程了。 那么，有什么其他好方法吗？此时，我们可以做增量快照，就是指做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。这个比较好理解。 但是它需要我们使用额外的元数据信息去记录哪些数据被修改了，这会带来额外的空间开销问题。那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢？ 这个时候就需要引入RDB和AOF的混合方式解决方案了。 redis.conf中配置AOF 默认情况下，Redis是没有开启AOF的，可以通过配置redis.conf文件来开启AOF持久化，关于AOF的配置如下： # appendonly参数开启AOF持久化 appendonly no # AOF持久化的文件名，默认是appendonly.aof appendfilename \"appendonly.aof\" # AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的 dir ./ # 同步策略 # appendfsync always appendfsync everysec # appendfsync no # aof重写期间是否同步 no-appendfsync-on-rewrite no # 重写触发配置 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb # 加载aof出错如何处理 aof-load-truncated yes # 文件重写策略 aof-rewrite-incremental-fsync yes 以下是Redis中关于AOF的主要配置信息： appendonly：默认情况下AOF功能是关闭的，将该选项改为yes以便打开Redis的AOF功能。- - – — appendfilename：这个参数项很好理解了，就是AOF文件的名字。 appendfsync：这个参数项是AOF功能最重要的设置项之一，主要用于设置“真正执行”操作命令向AOF文件中同步的策略。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:6:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"RDB和AOF混合方式 Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:6:3","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"主从复制 我们知道要避免单点故障，即保证高可用，便需要冗余（副本）方式提供集群服务。而Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。本文主要阐述Redis的主从复制。 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。 主从复制的作用主要包括： 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服- 务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。 主从库之间采用的是读写分离的方式。 读操作：主库、从库都可以接收； 写操作：首先到主库执行，然后，主库将写操作同步给从库。 redis 读写分离\r这里在同一台主机上演示，备份redis.conf 为redis-6380.conf，修改以下四个配置项 port 6380 ... ... pidfile /var/run/reids_6380.pid ... ... dbfilename dump-6380.rdb # 在同一台主机做测试时使用 ... ... replicaof 127.0.0.1 6379 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:7:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"启动从节点 redis-server redis-6380.conf 客户端连接从节点,可以看到角色为从节点，这样表示主从配置就成功了。 root@k8s-master01:/etc/redis# redis-cli -p 6380 127.0.0.1:6379\u003e info replication # Replication role:slave ... ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:7:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"高可用：哨兵机制（Redis Sentinel） 在上文主从复制的基础上，如果主节点出现故障该怎么办呢？ 在 Redis 主从集群中，哨兵机制是实现主从库自动切换的关键机制，它有效地解决了主从复制模式下故障转移的问题。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:8:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"哨兵机制（Redis Sentinel） Redis Sentinel，即Redis哨兵，在Redis 2.8版本开始引入。哨兵的核心功能是主节点的自动故障转移。 哨兵集群监控的逻辑图： redis 哨兵机制\r哨兵实现了什么功能呢？下面是Redis官方文档的描述： 监控（Monitoring）：哨兵会不断地检查主节点和从节点是否运作正常。 自动故障转移（Automatic failover）：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 配置提供者（Configuration provider）：客户端在初始化时，通过连接哨兵来获得当前Redis服务的主节点地址。 通知（Notification）：哨兵可以将故障转移的结果发送给客户端。 其中，监控和自动故障转移功能，使得哨兵可以及时发现主节点故障并完成转移；而配置提供者和通知功能，则需要在与客户端的交互中才能体现。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:8:1","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"哨兵集群的组建 上图中哨兵集群是如何组件的呢？哨兵实例之间可以相互发现，要归功于 Redis 提供的 pub/sub 机制，也就是发布 / 订阅机制。 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:8:2","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"启动哨兵节点 sentinel.conf,模拟还是当前主机下 sentinel monitor master 127.0.0.1 6379 启动哨兵节点，会以哨兵模式进行启动 redis-sentinel sentinel.conf 注意：生产环境需要三个哨兵节点 ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:8:3","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["Redis"],"content":"参考 https://redis.io/docs/latest/develop/ https://www.redis.net.cn/ Redis 设计与实现 https://datmt.com/backend/redis-command-cheatsheet/ https://pdai.tech/md/db/nosql-redis/db-redis-overview.html ","date":"2022-04-19","objectID":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/:9:0","tags":["Redis"],"title":"Redis 知识体系","uri":"/redis%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"},{"categories":["linux"],"content":"netstat netstat是一个常用的命令行网络工具，用于显示与网络连接、路由表和网络接口相关的信息。以下是一些常用的netstat命令： netstat -tunlp：显示所有TCP和UDP端口的监听情况，以及对应的进程信息。 netstat -tuln：显示所有TCP端口的监听情况，包括本地地址、外部地址和状态。 netstat -a：显示所有活动的网络连接，包括监听和非监听状态。 netstat -r：显示路由表，包括目标地址、网关、子网掩码和接口。 netstat -s：显示网络统计信息，包括传输的数据包数量、错误数量和丢失数量等。 netstat -i：显示网络接口信息，包括接口名称、MAC地址、IP地址和状态等。 netstat -c：持续输出网络连接的信息，每隔一段时间刷新一次。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:1:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"sar sar（System Activity Reporter）是一个性能监控工具，它可以提供系统资源使用情况的历史数据。以下是一些常用的sar命令和相应的使用场景： sar -u：显示CPU使用情况。 使用场景：了解系统CPU的平均负载、用户空间CPU使用率、系统空间CPU使用率，以及等待I/O的CPU使用率。 sar -r：显示内存使用情况。 使用场景：查看主要的内存指标，如总内存、可用内存、内存利用率、缓存和缓冲区。 sar -n DEV：显示网络接口的数据传输情况。 使用场景：监测网络流量、带宽使用情况，识别网络瓶颈。 sar -q：显示系统负载情况。 使用场景：通过观察平均负载、运行队列长度和就绪进程数，了解系统性能和负载情况。 sar -b：显示系统的I/O使用情况。 使用场景：监测磁盘IO的情况，包括块读写次数、块传输速率，查找磁盘性能问题。 sar -W：显示系统交换空间的使用情况。 使用场景：了解交换空间的使用情况，识别系统内存不足导致的交换瓶颈。 sar -d：显示块设备的I/O统计信息。 使用场景：查看块设备（硬盘）的读写操作、传输速率和请求队列长度。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:2:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"Example 统计sockets连接新 sar -n SOCK $ sar -n SOCK 1 1 Linux 4.4.115-1.el7.elrepo.x86_64 (chm) 2022年05月20日 _x86_64_ (12 CPU) 14时05分40秒 totsck tcpsck udpsck rawsck ip-frag tcp-tw 14时05分41秒 7725 76 5 0 0 642 平均时间: 7725 76 5 0 0 642 totsck 当前被使用的socket总数 tcpsck 当前正在被使用的TCP的socket总数 udpsck 当前正在被使用的UDP的socket总数 rawsck 当前正在被使用于RAW的skcket总数 if-frag 当前的IP分片的数目 tcp-tw TCP套接字中处于TIME-WAIT状态的连接数量 这些sar命令可用于监控系统资源的使用情况，帮助诊断性能问题和优化系统配置。您可以通过调整命令的参数来获取特定的监控数据，并结合其他工具和命令进行综合分析 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:2:1","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"telnet telnet是一个用于远程登录和管理远程主机的网络协议和工具。以下是一些常用的telnet命令和相应的使用场景： 1.telnet host：连接到指定的远程主机。 使用场景：远程登录到其他计算机，进行命令行管理和操作。 2.telnet host port：连接到指定主机的特定端口。 使用场景：检查特定服务的可用性，如通过telnet测试SMTP服务器是否能够连接。 3.Ctrl + ]：进入telnet控制台。 使用场景：在telnet会话中，按下Ctrl + ]键后，可以进入控制台，执行一些附加功能，如终止会话、更改设置等。 4.send string：发送字符串到远程主机。 使用场景：在telnet会话中，可以使用send命令发送特定的字符串至远程主机，用于模拟用户输入等情景。 5.display或toggle options：显示或切换选项状态。 使用场景：在telnet会话中，可以使用这两个命令查看或切换当前会话的选项状态，例如回显、行编辑等。 6.quit：退出当前telnet会话。 使用场景：在完成telnet会话后，使用该命令退出并关闭连接。 telnet命令可以用于远程登录和管理远程主机，但由于安全性较差，已逐渐被SSH等更安全的协议取代。在实际应用中，建议使用更加安全的远程连接方法，如SSH（Secure Shell）。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:3:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["linux"],"content":"ss ss（Socket Statistics）是一个Linux系统中用于显示当前活动套接字（socket）信息的命令。它可以提供更详细和全面的套接字统计数据。以下是一些常用的ss命令和相应的使用场景： 1.ss -t：显示TCP套接字信息。 使用场景：查看当前系统上的TCP连接信息，包括本地地址和端口、远程地址和端口、连接状态等。 2.ss -u：显示UDP套接字信息。 使用场景：查看当前系统上的UDP连接信息，包括本地地址和端口、远程地址和端口等。 3.ss -l：显示监听套接字信息。 使用场景：查看当前系统上正在监听的套接字信息，包括监听的协议、本地地址和端口等。 4.ss -p：显示套接字及其关联进程信息。 使用场景：查看每个套接字对应的关联进程的详细信息，包括进程ID、用户、命令等。 5.ss -n：以数字形式展示套接字信息。 使用场景：显示IP地址和端口号时不进行反向解析，加快输出速度。 6.ss -s：显示套接字统计摘要(包含IPv6)。 使用场景：显示系统级别的套接字统计信息，包括打开的套接字数、活动连接数、侦听套接字数等。 7.ss -o：显示定时器相关信息。 使用场景：查看系统中的定时器事件相关信息，如计时器名称、超时时间、重复间隔等。 这些ss命令可用于监控和诊断网络连接和套接字的状态，帮助定位网络问题并进行性能调优。在实际应用中，可以结合其他命令和工具（如grep、netstat等）进行更深入的分析和排查。 ","date":"2022-03-29","objectID":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/:4:0","tags":["linux"],"title":"Linux 网络排障","uri":"/linux%E7%BD%91%E7%BB%9C%E6%8E%92%E9%9A%9C/"},{"categories":["Python"],"content":"介绍 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:1:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"Python 简介 摘要\rPython 是一个高层次的结合了解释性、编译性、互动性和面向对象的脚本语言。 Python 的设计具有很强的可读性，相比其他语言经常使用英文关键字，其他语言的一些标点符号，它具有比其他语言更有特色语法结构。 Python 是一种解释型语言： 这意味着开发过程中没有了编译这个环节。类似于PHP和Perl语言。 Python 是交互式语言： 这意味着，您可以在一个Python提示符，直接互动执行写你的程序。 Python 是面向对象语言: 这意味着Python支持面向对象的风格或代码封装在对象的编程技术。 Python 是初学者的语言：Python 对初级程序员而言，是一种伟大的语言，它支持广泛的应用程序开发，从简单的文字处理到 WWW 浏览器再到游戏。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:1:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"Python 特点 信息\r易于学习：Python有相对较少的关键字，结构简单，和一个明确定义的语法，学习起来更加简单。 易于阅读：Python代码定义的更清晰。 易于维护：Python的成功在于它的源代码是相当容易维护的。 一个广泛的标准库：Python的最大的优势之一是丰富的库，跨平台的，在UNIX，Windows和Macintosh兼容很好。 互动模式：互动模式的支持，您可以从终端输入执行代码并获得结果的语言，互动的测试和调试代码片断。 可移植：基于其开放源代码的特性，Python已经被移植（也就是使其工作）到许多平台。 可扩展：如果你需要一段运行很快的关键代码，或者是想要编写一些不愿开放的算法，你可以使用C或C++完成那部分程序，然后从你的Python程序中调用。 数据库：Python提供所有主要的商业数据库的接口。 GUI编程：Python支持GUI可以创建和移植到许多系统调用。 可嵌入: 你可以将Python嵌入到C/C++程序，让你的程序的用户获得\"脚本化\"的能力。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:1:2","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"安装 Python 3 Python最新源码，二进制文档，新闻资讯等可以在Python的官网查看到： Python官网：https://www.python.org/ 你可以在以下链接中下载 Python 的文档，你可以下载 HTML、PDF 和 PostScript 等格式的文档。 Python文档下载地址：https://www.python.org/doc/ ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:2:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"Window 平台安装 Python 以下为在 Window 平台上安装 Python 的简单步骤： 打开 WEB 浏览器访问https://www.python.org/downloads/windows/ ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:2:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"设置开发环境 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:3:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"使用虚拟环境（virtualenv 或 venv）命令行 venv，Python官方用于创建虚拟环境的工具。 cd xxx/xxx/crm python3.9 -m venv ddd python3.7 -m venv xxxx python3.7 -m venv /xxx/xxx/xxx/xx/ppp virtualenv 【推荐】 pip install virtualenv cd /xxx/xx/ virtualenv ddd --python=python3.9 # or virtualenv /xxx/xx/ddd --python=python3.9 激活虚拟环境 win cd F:\\envs\\crm\\Scripts activate mac source /虚拟环境目录/bin/activate ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:3:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"通过配置编辑器（如 VS Code、PyCharm 等） 通过配置编辑器如Pycharm 创建虚拟环境是一种更方便的方式，原理还是通过命令行，通过界面化操作会直观一些 virtualenv\r","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:3:2","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"基础语法 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:4:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"变量 变量规则 变量只能包含字母、数字和下划线 变量不可以包含空格，可以使用下划线来分隔单词 不要将python 关键字作为变量 变量名应该简短具有描述性 慎用小写字母i和大写字母O，会被误看出数字1和0 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:4:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"控制流（if、for、while） 条件语句 if 条件语句 if 判断条件： 执行语句…… else： 执行语句…… 多条件匹配时 if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环语句 while 判断条件： 执行语句…… Gif 演示 Python while 语句执行过程 loop-over-python-list-animation\rfor循环语句 for iterating_var in sequence: statements(s) 流程图 python_for_loop\r实例 #!/usr/bin/python # -*- coding: UTF-8 -*- for letter in 'Python': # 第一个实例 print('当前字母 :', letter) fruits = ['banana', 'apple', 'mango'] for fruit in fruits: # 第二个实例 print('当前水果 :', fruit) print(\"Good bye!\") 通过序列索引迭代 #!/usr/bin/python # -*- coding: UTF-8 -*- fruits = ['banana', 'apple', 'mango'] for index in range(len(fruits)): print('当前水果 :', fruits[index]) print(\"Good bye!\") ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:4:2","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"函数 函数定义和调用 def greet_user(): # 1. 关键字def \"\"\"显示简单的问候语，文档字符串注释，方便生成程序文档\"\"\" # 2. 注释 print(\"hello\") # 3. 函数体 greet_user() # 4. 调用 参数 实参和形参 def greet_user(username): pass username=\"xiao hua\" greet_user(username) # 形参 greet_user(\"xiao hua\") # 实参 默认参数 # 可写函数说明 def printinfo(name, age=35): \"\"\"打印任何传入的字符串\"\"\" print(\"Name: \", name) print(\"Age \", age) return # 调用printinfo函数 printinfo(age=50, name=\"miki\") printinfo(name=\"miki\") ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:4:3","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"异常处理 捕捉异常可以使用try/except语句。 try/except语句用来检测try语句块中的错误，从而让except语句捕获异常信息并处理。 如果你不想在异常发生时结束你的程序，只需在try里捕获它。 try….except…else 以下为简单的try….except…else的语法： try: \u003c语句\u003e #运行别的代码 except \u003c名字\u003e： \u003c语句\u003e #如果在try部份引发了'name'异常 except \u003c名字\u003e，\u003c数据\u003e: \u003c语句\u003e #如果引发了'name'异常，获得附加的数据 else: \u003c语句\u003e #如果没有异常发生 try的工作原理是，当开始一个try语句后，python就在当前程序的上下文中作标记，这样当异常出现时就可以回到这里，try子句先执行，接下来会发生什么依赖于执行时是否出现异常。 如果当try后的语句执行时发生异常，python就跳回到try并执行第一个匹配该异常的except子句，异常处理完毕，控制流就通过整个try语句（除非在处理异常时又引发新的异常）。 如果在try后的语句里发生了异常，却没有匹配的except子句，异常将被递交到上层的try，或者到程序的最上层（这样将结束程序，并打印缺省的出错信息）。 如果在try子句执行时没有发生异常，python将执行else语句后的语句（如果有else的话），然后控制流通过整个try语句。 实例 下面是简单的例子，它打开一个文件，在该文件中的内容写入内容，且并未发生异常： #!/usr/bin/python # -*- coding: UTF-8 -*- try: fh = open(\"testfile\", \"w\") fh.write(\"这是一个测试文件，用于测试异常!!\") except IOError: print(\"Error: 没有找到文件或读取文件失败\") else: print(\"内容写入文件成功\") fh.close() 输出结果 $ python test.py 内容写入文件成功 $ cat testfile # 查看写入的内容 这是一个测试文件，用于测试异常!! try-finally 语句 try-finally 语句无论是否发生异常都将执行最后的代码。 try: \u003c语句\u003e finally: \u003c语句\u003e #退出try时总会执行 raise 示例 #!/usr/bin/python # -*- coding: UTF-8 -*- try: fh = open(\"testfile\", \"w\") fh.write(\"这是一个测试文件，用于测试异常!!\") finally: print(\"Error: 没有找到文件或读取文件失败\") 输出 $ python test.py Error: 没有找到文件或读取文件失败 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:4:4","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"数据结构 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:5:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"列表（List） 列表式什么? 列表由一系列按特定顺序排列的元素组成 Python有6个序列的内置类型，但最常见的是列表和元组。 序列都可以进行的操作包括索引，切片，加，乘，检查成员。 此外，Python已经内置确定序列的长度以及确定最大和最小的元素的方法。 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 创建一个列表，只要把逗号分隔的不同的数据项使用方括号括起来即可。如下所示： list1 = ['physics', 'chemistry', 1997, 2000] list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"] 列表操作 list1 = ['physics', 'chemistry', 1997, 2000] list2 = [1, 2, 3, 4, 5, 6, 7] # 1. 访问列表中的值 print(\"list1[0]: \", list1[0]) print(\"list2[1:5]: \", list2[1:5]) # 2. 更新列表 list2.append(8) print(list2) # [1, 2, 3, 4, 5, 6, 7, 8] # 3. 删除列表中的值，通过下标索引删除 del list2[1] print(list2) # [1, 3, 4, 5, 6, 7, 8] 列表脚本操作符 Python 表达式 结果 描述 len([1, 2, 3]) 3 长度 [1, 2, 3] + [4, 5, 6] [1, 2, 3, 4, 5, 6] 组合 [‘Hi!’] * 4 [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’] 重复 3 in [1, 2, 3] True 元素是否存在于列表中 for x in [1, 2, 3]: print x, 1 2 3 迭代 Python列表函数\u0026方法 序号 函数 1 cmp(list1, list2) 比较两个列表的元素 2 len(list) 列表元素个数 3 max(list) 返回列表元素最大值 4 min(list) 返回列表元素最小值 5 list(seq) 将元组转换为列表 Python包含以下方法: 序号 方法 1 list.append(obj) 在列表末尾添加新的对象 2 list.count(obj) 统计某个元素在列表中出现的次数 3 list.extend(seq) 在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表） 4 list.index(obj) 从列表中找出某个值第一个匹配项的索引位置 5 list.insert(index, obj) 将对象插入列表 6 list.pop([index=-1]) 移除列表中的一个元素（默认最后一个元素），并且返回该元素的值 7 list.remove(obj) 移除列表中某个值的第一个匹配项 8 list.reverse() 反向列表中元素 9 list.sort(cmp=None, key=None, reverse=False) 对原列表进行排序 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:5:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"元组（Tuple） Python的元组与列表类似，不同之处在于元组的元素不能修改。 元组使用小括号，列表使用方括号。 元组创建很简单，只需要在括号中添加元素，并使用逗号隔开即可。 示例 tup1 = ('physics', 'chemistry', 1997, 2000) tup2 = (1, 2, 3, 4, 5 ) tup3 = \"a\", \"b\", \"c\", \"d\" # 空元组 tup1 = () # 元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 元组操作 #!/usr/bin/python tup1 = ('physics', 'chemistry', 1997, 2000) tup2 = (1, 2, 3, 4, 5, 6, 7) # 1. 访问元组 print(\"tup1[0]: \", tup1[0]) print(\"tup2[1:5]: \", tup2[1:5]) # 2. 修改元组(元组中的元素值是不允许修改的，但我们可以对元组进行连接组合) # 以下修改元组元素操作是非法的。 #tup2[0] = 100 # 创建一个新的元组 tup3 = tup1 + tup2 print(tup3) # 3. 删除元组(元组中的元素值是不允许删除的，但我们可以使用del语句来删除整个元组) print(tup2) del tup2 print(\"After deleting tup2 : \") print(tup2) 元组运算符 与字符串一样，元组之间可以使用 + 号和 * 号进行运算。这就意味着他们可以组合和复制，运算后会生成一个新的元组。 Python 表达式 结果 描述 len((1, 2, 3)) 3 计算元素个数 (1, 2, 3) + (4, 5, 6) (1, 2, 3, 4, 5, 6) 连接 (‘Hi!’,) * 4 (‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’) 复制 3 in (1, 2, 3) True 元素是否存在 for x in (1, 2, 3): print x, 1 2 3 迭代 元组索引，截取 因为元组也是一个序列，所以我们可以访问元组中的指定位置的元素，也可以截取索引中的一段元素，如下所示： 元组： L = ('spam', 'Spam', 'SPAM!') Python 表达式 结果 描述 L[2] ‘SPAM!’ 读取第三个元素 L[-2] ‘Spam’ 反向读取，读取倒数第二个元素 L[1:] (‘Spam’, ‘SPAM!’) 截取元素 元组内置函数 序号 方法及描述 1 cmp(tuple1, tuple2) 比较两个元组元素。 2 len(tuple) 计算元组元素个数。 3 max(tuple) 返回元组中元素最大值。 4 min(tuple) 返回元组中元素最小值。 5 tuple(seq) 将列表转换为元组。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:5:2","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"字典（Dictionary） 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割，整个字典包括在花括号 {} 中 ,格式如下所示 d = {key1 : value1, key2 : value2 } 键一般是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一。 字典操作 #!/usr/bin/python dict = {'Name': 'Zara', 'Age': 7, 'Class': 'First'} # 1. 访问字典 print(\"dict['Name']: \", dict['Name']) print(\"dict['Age']: \", dict['Age']) # 2. 修改字典 dict['Age'] = 8 # 更新 dict['School'] = \"CODEXY\" # 添加 print(\"dict['Age']: \", dict['Age']) print(\"dict['School']: \", dict['MIT']) # 3. 删除字典元素 del dict['Name'] # 删除键是'Name'的条目 dict.clear() # 清空词典所有条目 del dict # 删除词典 字典的两个特性 不允许同一个键出现两次。创建时如果同一个键被赋值两次，后一个值会被覆盖 键必须不可变，所以可以用数字，字符串或元组充当，所以用列表就不行 字典内置函数\u0026方法 Python字典包含了以下内置函数： 序号 函数及描述 1 cmp(dict1, dict2) 比较两个字典元素。 2 len(dict) 计算字典元素个数，即键的总数。 3 str(dict) 输出字典可打印的字符串表示。 4 type(variable) 返回输入的变量类型，如果变量是字典就返回字典类型。 Python字典包含了以下内置方法： 序号 函数及描述 1 dict.clear() 删除字典内所有元素 2 dict.copy() 返回一个字典的浅复制 3 dict.fromkeys(seq[, val]) 创建一个新字典，以序列 seq 中元素做字典的键，val 为字典所有键对应的初始值 4 dict.get(key, default=None) 返回指定键的值，如果值不在字典中返回default值 5 dict.has_key(key) 如果键在字典dict里返回true，否则返回false 6 dict.items() 以列表返回可遍历的(键, 值) 元组数组 7 dict.keys() 以列表返回一个字典所有的键 8 dict.setdefault(key, default=None) 和get()类似, 但如果键不存在于字典中，将会添加键并将值设为default 9 dict.update(dict2) 把字典dict2的键/值对更新到dict里 10 dict.values() 以列表返回字典中的所有值 11 pop(key[,default]) 删除字典给定键 key 所对应的值，返回值为被删除的值。key值必须给出。 否则，返回default值。 12 popitem() 随机返回并删除字典中的一对键和值。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:5:3","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"集合（Set） 在 Python 中，集合（Set）是一种无序且元素唯一的数据类型。它基于哈希表实现，因此支持高效的成员检测和删除操作。 创建集合 可以使用大括号 {} 或者 set() 构造函数来创建集合。 # 使用大括号创建集合 my_set = {1, 2, 3, 4, 5} # 使用 set() 构造函数创建集合 another_set = set([5, 6, 7, 8, 9]) 集合的特点 无序性：集合中的元素没有固定的顺序。 唯一性：集合中不允许包含重复的元素。 可变性：集合是可变的，可以添加或删除元素。 哈希性：集合中的元素必须是可哈希的，即不可变类型，如整数、浮点数、元组、字符串等。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:5:4","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"面向对象编程 Python从设计之初就已经是一门面向对象的语言，正因为如此，在Python中创建一个类和对象是很容易的。 面向对象技术简介 类(Class): 用来描述具有相同的属性和方法的对象的集合。它定义了该集合中每个对象所共有的属性和方法。对象是类的实例。 类变量：类变量在整个实例化的对象中是公用的。类变量定义在类中且在函数体之外。类变量通常不作为实例变量使用。 数据成员：类变量或者实例变量, 用于处理类及其实例对象的相关的数据。 方法重写：如果从父类继承的方法不能满足子类的需求，可以对其进行改写，这个过程叫方法的覆盖（override），也称为方法的重写。 局部变量：定义在方法中的变量，只作用于当前实例的类。 实例变量：在类的声明中，属性是用变量来表示的。这种变量就称为实例变量，是在类声明的内部但是在类的其他成员方法之外声明的。 继承：即一个派生类（derived class）继承基类（base 1. class）的字段和方法。继承也允许把一个派生类的对象作为一个基类对象对待。例如，有这样一个设计：一个Dog类型的对象派生自Animal类，这是模拟\"是一个（is-a）“关系（例图，Dog是一个Animal）。 实例化：创建一个类的实例，类的具体对象。 方法：类中定义的函数。 对象：通过类定义的数据结构实例。对象包括两个数据成员（类变量和实例变量）和方法。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:6:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"类和对象 创建类 class ClassName: '''类的帮助信息''' #类文档字符串 class_suite #类体 类的帮助信息可以通过ClassName.__doc__查看。 class_suite 由类成员，方法，数据属性组成。 示例 #!/usr/bin/python # -*- coding: UTF-8 -*- class Employee: '所有员工的基类' empCount = 0 def __init__(self, name, salary): self.name = name self.salary = salary Employee.empCount += 1 def displayCount(self): print(\"Total Employee %d\" % Employee.empCount) def displayEmployee(self): print(\"Name : \", self.name, \", Salary: \", self.salary) 创建类的实例对象 实例化类其他编程语言中一般用关键字 new，但是在 Python 中并没有这个关键字，类的实例化类似函数调用方式。 以下使用类的名称 Employee 来实例化，并通过 __init__ 方法接收参数。 \"创建 Employee 类的第一个对象\" emp1 = Employee(\"Zara\", 2000) \"创建 Employee 类的第二个对象\" emp2 = Employee(\"Manni\", 5000) Python 内置类属性 __dict__ : 类的属性（包含一个字典，由类的数据属性组成） __doc__ :类的文档字符串 __name__: 类名 __module__: 类定义所在的模块（类的全名是'__main__.className'，如果类位于一个导入模块mymod中，那么className.__module__ 等于 mymod） __bases__ : 类的所有父类构成元素（包含了一个由所有父类组成的元组） ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:6:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"类的继承 面向对象的编程带来的主要好处之一是代码的重用，实现这种重用的方法之一是通过继承机制。 通过继承创建的新类称为子类或派生类，被继承的类称为基类、父类或超类。 继承语法 class 派生类名(基类名) ... 在python中继承中的一些特点： 1、如果在子类中需要父类的构造方法就需要显示的调用父类的构造方法，或者不重写父类的构造方法。详细说明可查看：python 子类继承父类构造函数说明。 2、在调用基类的方法时，需要加上基类的类名前缀，且需要带上 self 参数变量。区别在于类中调用普通函数时并不需要带上 self 参数 3、Python 总是首先查找对应类型的方法，如果它不能在派生类中找到对应的方法，它才开始到基类中逐个查找。（先在本类中查找调用的方法，找不到才去基类中找）。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:6:2","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"模块和包 Python 模块(Module)，是一个 Python 文件，以 .py 结尾，包含了 Python 对象定义和Python语句。 模块让你能够有逻辑地组织你的 Python 代码段。 把相关的代码分配到一个模块里能让你的代码更好用，更易懂。 模块能定义函数，类和变量，模块里也能包含可执行的代码。 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:7:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"模块的导入 模块定义好后，我们可以使用 import 语句来引入模块，语法如下： import module1[, module2[,... moduleN]] 比如要引用模块 math，就可以在文件最开始的地方用 import math 来引入。在调用 math 模块中的函数时，必须这样引用： 模块名.函数名 from…import 语句 Python 的 from 语句让你从模块中导入一个指定的部分到当前命名空间中。语法如下： from modname import name1[, name2[, ... nameN]] from…import* 语句 把一个模块的所有内容全都导入到当前的命名空间也是可行的，只需使用如下声明： from modname import * ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:7:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"文件操作 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:8:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"打开和关闭文件 Python 提供了必要的函数和方法进行默认情况下的文件基本操作。你可以用 file 对象做大部分的文件操作。 open 函数 你必须先用Python内置的open()函数打开一个文件，创建一个file对象，相关的方法才可以调用它进行读写。 语法： file object = open(file_name [, access_mode][, buffering]) 各个参数的细节如下： file_name：file_name变量是一个包含了你要访问的文件名称的字符串值。 access_mode：access_mode决定了打开文件的模式：只读，写入，追加等。所有可取值见如下的完全列表。这个参数是非强制的，默认文件访问模式为只读(r)。 buffering:如果buffering的值被设为0，就不会有寄存。如果buffering的值取1，访问文件时会寄存行。如果将buffering的值设为大于1的整数，表明了这就是的寄存区的缓冲大小。如果取负值，寄存区的缓冲大小则为系统默认。 读写文件内容 不同模式打开文件的完全列表： 模式 描述 t 文本模式 (默认)。 x 写模式，新建一个文件，如果该文件已存在则会报错。 b 二进制模式。 + 打开一个文件进行更新(可读可写)。 U 通用换行模式（不推荐）。 r 以只读方式打开文件。文件的指针将会放在文件的开头。这是默认模式。 rb 以二进制格式打开一个文件用于只读。文件指针将会放在文件的开头。这是默认模式。一般用于非文本文件如图片等。 r+ 打开一个文件用于读写。文件指针将会放在文件的开头。 rb+ 以二进制格式打开一个文件用于读写。文件指针将会放在文件的开头。一般用于非文本文件如图片等。 w 打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb 以二进制格式打开一个文件只用于写入。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 w+ 打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。 wb+ 以二进制格式打开一个文件用于读写。如果该文件已存在则打开文件，并从开头开始编辑，即原有内容会被删除。如果该文件不存在，创建新文件。一般用于非文本文件如图片等。 a 打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 ab 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入。 a+ 打开一个文件用于读写。如果该文件已存在，文件指针将会放在文件的结尾。文件打开时会是追加模式。如果该文件不存在，创建新文件用于读写。 ab+ 以二进制格式打开一个文件用于追加。如果该文件已存在，文件指针将会放在文件的结尾。如果该文件不存在，创建新文件用于读写。 下图很好的总结了这几种模式： file accessmode\r模式 r r+ w w+ a a+ 读 + + + + 写 + + + + + 创建 + + + + 覆盖 + + 指针在开始 + + + + 指针在结尾 + + ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:8:1","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"使用 with 语句处理文件 在 Python 中，with 语句用于管理代码块执行过程中的上下文环境，确保在进入和退出代码块时，资源的正确获取和释放。它通常与支持上下文管理协议（Context Management Protocol）的对象一起使用，如文件操作、数据库连接、网络连接等需要在使用后及时关闭或释放资源的情况。 with open('example.txt', 'r') as file: content = file.read() print(content) # 在退出 with 块后，文件会自动关闭，即使出现异常也能保证资源的释放 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:8:2","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"常用标准库 操作系统接口（os 模块） 文件路径操作（pathlib 模块） 时间和日期处理（datetime 模块） 正则表达式（re 模块） Python 常用模块和常用第三方模块示例 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:9:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"进阶话题 并发编程（多线程和多进程） 异步编程（asyncio 模块） 数据库访问（sqlite3、SQLAlchemy 等） ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:10:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"实际应用示例 简单的 Web 开发（使用 Flask 或 Django） 数据分析与可视化（使用 Pandas 和 Matplotlib） ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:11:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"资源推荐 官方文档链接 谷歌Python代码风格指南 中文翻译 30个Python常用极简代码，拿走就用 收藏 | 学习Python的11个顶级Github存储库 改善Python程序的91个建议 Django Django-中文 ","date":"2022-03-20","objectID":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/:12:0","tags":["Python"],"title":"Python3 快速上手指南","uri":"/python3-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/"},{"categories":["Python"],"content":"项目目录布局 project_name ├── docs │ ├── make.bat │ ├── Makefile │ └── source │ ├── conf.py │ └── index.rst ├── examples │ └── example.py ├── project_name │ └── package_name │ └── __init__.py │ ├── module1.py │ ├── module2.py │ ├── package1/ │ │ ├── __init__.py │ │ ├── module3.py │ │ ├── module4.py │ │ └── ... ├── tests │ └── __init__.py ├── .gitignore ├── LICENSE.txt ├── MANIFEST.in ├── README.md ├── requirements.txt ├── setup.py 在这个结构中： project_name/ 是项目的根目录。 project_name/project_name/ 目录包含项目的源代码，init.py 文件是一个特殊的文件，它标志着该目录是一个 Python 包。 module1.py 和 module2.py 是 Python 模块，它们包含 Python 代码。 package1/ 是一个 Python 包，它可以包含其他模块和子包。包内的 init.py 文件用于组织包的内容。 tests/ 目录包含项目的测试代码。 docs/ 目录包含项目的文档。 setup.py 是一个常见的 Python 项目配置文件，它用于安装，卸载，打包等任务。 requirements.txt 列出了项目的依赖。 Python 项目结构并没有严格的规范，不过这种结构基本就是 Python 社区中通用的最佳实践，你可以在很多开源 Python 项目中看到这种结构。 ","date":"2022-03-18","objectID":"/python-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97_%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95/:0:0","tags":["Python"],"title":"Python 快速上手指南 项目目录","uri":"/python-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97_%E9%A1%B9%E7%9B%AE%E7%9B%AE%E5%BD%95/"},{"categories":["Nginx"],"content":"Nginx 是什么？ Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器。 Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的. 第一个公开版本0.1.0发布于2004年10月4日。 ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:1:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"四个发行版本 开源版：https://nginx.org/ 商业版：https://nginx.com/ f5 Openresty: https://openresty.org/cn/ Tengine: https://tengine.taobao.org/ ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:2:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"为什么选择Nginx? 互联网公司大都选择 Nginx Nginx 技术成熟，具备的功能是企业最常使用而且最需要的 适合当前主流架构趋势, 微服务、云架构、中间层 统一技术栈, 降低维护成本, 降低技术更新成本。 ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:3:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"Nginx 特点 开源,可以从官网直接获取源代码 高并发：高性能,Nginx性能非常残暴,支持海量并发，顶住10万以上连接是没有问题的 低内存消耗：在高性能的同时，保持很低的内存消耗； 模块化：Nginx具有丰富的模块可以按需使用，并且有开发能力的技术人员还可以二次开发 热启动：例如当修改配置文件后，不需要停止与启动就可以让配置生效 ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:4:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"Nginx 应用场景 Nginx 应用场景\r","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:5:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"Nginx 的组成 Nginx 应用场景\r可以将Nginx 理解为一辆小汽车，nginx.conf 可比作驾驶员，控制这Nginx的的行为，但是当汽车在驾驶的过程的过程中，出现了问题，这个时候就需要一个黑匣子到底是汽车本身出现了问题，还是驾驶员驾驶不当出现了问题。Nginx 二进制可执行文件是由本身的框架和第三方模块，编译的可执行文件，这个文件可以理解为汽车本身。它有完整的系统，所有的功能都有它提供。access.log 可以理解为这个汽车经过任何一个地方形成的一个GPS轨迹，error.log 可以理解为汽车的黑匣子,当遇到不可预期的问题时，可以查看该日志文件记录信息进行问题定位。如果要对nginx 进行运维分析，可以对access.log进行分析。如果遇到一些未知的错误，这时就要对error.log进行分析了。 ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:6:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"Nginx 重要配置文件说明 1. 查看配置文件 $ rpm -ql nginx ................................................... /etc/logrotate.d/nginx #nginx日志切割的配置文件 /etc/nginx/nginx.conf #nginx主配置文件 /etc/nginx/conf.d #子配置文件 /etc/nginx/conf.d/default.conf #默认展示的页面一样 /etc/nginx/mime.types #媒体类型 （http协议中的文件类型） /etc/sysconfig/nginx #systemctl 管理 nginx的使用的文件 /usr/lib/systemd/system/nginx.service #systemctl 管理nginx（开 关 重启 reload) /usr/sbin/nginx #nginx命令 /usr/share/nginx/html #站点目录 网站的根目录 /var/log/nginx #nginx日志 access.log 访问日志 2. 查看已经编译的模块 [root@www html]# nginx -V nginx version: nginx/1.20.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) built with OpenSSL 1.1.1k FIPS 25 Mar 2021 TLS SNI support enabled configure arguments: --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --http-proxy-temp-path=/var/lib/nginx/tmp/proxy --http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/tmp/uwsgi --http-scgi-temp-path=/var/lib/nginx/tmp/scgi --pid-path=/run/nginx.pid --lock-path=/run/lock/subsys/nginx --user=nginx --group=nginx --with-compat --with-debug --with-file-aio --with-google_perftools_module --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_degradation_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_mp4_module --with-http_perl_module=dynamic --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_xslt_module=dynamic --with-mail=dynamic --with-mail_ssl_module --with-pcre --with-pcre-jit --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-threads --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic' --with-ld-opt='-Wl,-z,relro -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -Wl,-E' 配置文件注解 第一部分：配置文件主区域配置 user nginx; #定义运行nginx进程的用户 worker_processes auto; #Nginx运行的work进程数量(建议与CPU数量一致或 auto) error_log /var/log/nginx/error.log warn; #nginx错误日志 pid /var/run/nginx.pid; #nginx运行pid 第二部分：配置文件事件区域 events { worker_connections 1024; #每个 worker 进程支持的最大连接数 } 第三部分：配置http区域 http { include /etc/nginx/mime.types; #Nginx支持的媒体类型库文件 default_type application/octet-stream; #默认的媒体类型 log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; #访问日志保存路径 sendfile on; #开启高效传输模式 #tcp_nopush on; #必须配合tcp_nopush使用，当数据包累计到一定大小后就发送 keepalive_timeout 65; #连接超时时间，单位是秒 #gzip on; #开启文件压缩 include /etc/nginx/conf.d/*.conf; #包含子配置文件 } 第四部分：子配置文件内容 server { listen 80; #指定监听端口 server_name localhost; #指定监听的域名 location / { root /usr/share/nginx/html; #定义站点的目录 index index.html index.htm; #定义首页文件 } } http server location 扩展了解项 http{}层下允许有多个 Server{}层，一个 Server{}层下又允许有多个 Location http{} 标签主要用来解决用户的请求与响应。 server{} 标签主要用来响应具体的某一个网站。 location{} 标签主要用于匹配网站具体 URL 路径 ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:6:1","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"Nginx 配置语法 配置文件由指令和指令块构成 每条指令以；分号结尾，指令与参数间以空格符号分隔 指令块以{} 大括号将多条指令组织在一起 include 语句允许组合多个配置文件以提升可维护性 使用#符号添加注释，提高可读性 使用$符号使用变量 部分指令的参数支持正则表达式 Nginx 语法示例 Nginx 语法示例\r","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:6:2","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"配置参数 时间的单位 ms: milliseconds s: seconds m: minutes h: hours d: days w: weeks M: months,30 days y: years,.365 days 空间的单位 bytes kilobytes megabytes gigabytes ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:6:3","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"http 配置的指令块 http server upstream location http 配置的指令块\r","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:6:4","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"Nginx 命令行 格式： nginx- s reload 帮助： -h 使用指定的配置文件： -c 指定配置命令： -g 指定运行目录： -p 发送信号：-s 立刻停止服务： stop 优雅的停止服务：quit 重载配置文件：reload 重新开始记录日志文件：reopen 测试配置文件是否有语法错误：-t -T 打印nginx 的版本信息、编译信息等：-v -V ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:6:5","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"参考 http://nginx.org/ https://www.nginx-cn.net/ https://matob.web.id/random/nginx/ ","date":"2022-01-19","objectID":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/:7:0","tags":["Nginx"],"title":"Nginx 介绍-快速了解","uri":"/nginx%E4%BB%8B%E7%BB%8D-%E5%BF%AB%E9%80%9F%E4%BA%86%E8%A7%A3/"},{"categories":["Nginx"],"content":"基本原理 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:1:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 进程模型 Nginx启动后以daemon形式在后台运行，后台进程包含一个master进程和多个worker进程。如下图所示： [root@www html]# ps -ef --forest|grep nginx|grep -v grep root 59984 1 0 13:23 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 59985 59984 0 13:23 ? 00:00:00 \\_ nginx: worker process nginx 59986 59984 0 13:23 ? 00:00:00 \\_ nginx: worker process nginx 59987 59984 0 13:23 ? 00:00:00 \\_ nginx: worker process nginx 59988 59984 0 13:23 ? 00:00:00 \\_ nginx: worker process nginx是由一个master管理进程，多个worker进程处理工作的多进程模型。基础架构设计，如下图所示： Nginx 架构\rMaster负责管理worker进程，worker进程负责处理网络事件。整个框架被设计为一种依赖事件驱动、异步、非阻塞的模式。 why ——为什么选择master/worker 这种模型？ 如此设计的优点有： 可以充分利用多核机器，增强并发处理能力。 多worker间可以实现负载均衡。 Master监控并统一管理worker行为。在worker异常后，可以主动拉起worker进程，从而提升了系统的可靠性。并且由Master进程控制服务运行中的程序升级、配置项修改等操作，从而增强了整体的动态可扩展与热更的能力。 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:1:1","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx进程结构 Nginx 进程结构\r进程结构分类 单进程结构 适用与开发测试环境 多进程结构 适用于生产环境 所有的worker 进程是处理真正的请求的 master 进程是监控 worker 进程是不是在正常的工作 缓存是要在多个进程间进行共享的 这些进程间的通讯都是使用共享内存来解决的 问题： nginx 为什么会有多个worker进程？ 因为nginx 是采用事件驱动的模型以后，它希望每一个worker 进程从头到尾占有一颗 cpu,所以我们要把worker 进程的数量配置和服务器上的cpu核数一致 Nginx 服务器，正常运行过程中： 多进程：一个 Master 进程、多个 Worker 进程 Master 进程：管理 Worker 进程 对外接口：接收外部的操作（信号） 对内转发：根据外部的操作的不同，通过信号管理 Worker 监控：监控 worker 进程的运行状态，worker 进程异常终止后，自动重启 worker 进程 Worker 进程：所有 Worker 进程都是平等的 实际处理：网络请求，由 Worker 进程处理； Worker 进程数量：在 nginx.conf 中配置，一般设置为核心数，充分利用 CPU 资源，同时，避免进程数量过多，避免进程竞争 CPU 资源，增加上下文切换的损耗。 HTTP 连接建立和请求处理过程： Nginx 启动时，Master 进程，加载配置文件 Master 进程，初始化监听的 socket Master 进程，fork 出多个 Worker 进程 Worker 进程，竞争新的连接，获胜方通过三次握手，建立 Socket 连接，并处理请求 Nginx 高性能、高并发： Nginx 采用：多进程 + 异步非阻塞方式（IO 多路复用 epoll） 请求的完整过程： 建立连接 读取请求：解析请求 处理请求 响应请求 请求的完整过程，对应到底层，就是：读写 socket 事件 进程结构示例演示 可以看到当执行nginx -s reload重载命令后，会重新生成三个进程，进程id59985 59986 59987 59988变更为62907 62908 62909 62910,同理执行kill -SIGHUP 59984 是一样的效果 [root@www ~]# ps -ef --forest|grep nginx|grep -v grep root 59984 1 0 13:24 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 59985 59984 0 13:24 ? 00:00:00 \\_ nginx: worker process nginx 59986 59984 0 13:24 ? 00:00:00 \\_ nginx: worker process nginx 59987 59984 0 13:24 ? 00:00:00 \\_ nginx: worker process nginx 59988 59984 0 13:24 ? 00:00:00 \\_ nginx: worker process [root@www ~]# nginx -s reload [root@www ~]# ps -ef --forest|grep nginx|grep -v grep root 59984 1 0 13:24 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 62907 59984 0 17:26 ? 00:00:00 \\_ nginx: worker process nginx 62908 59984 0 17:26 ? 00:00:00 \\_ nginx: worker process nginx 62909 59984 0 17:26 ? 00:00:00 \\_ nginx: worker process nginx 62910 59984 0 17:26 ? 00:00:00 \\_ nginx: worker process 单独发送终止信号终止一个 worker 进程会发生什么？ [root@www ~]# ps -ef --forest|grep nginx|grep -v grep root 59984 1 0 13:24 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 62918 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process nginx 62919 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process nginx 62920 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process nginx 62921 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process [root@www ~]# kill -SIGTERM 62918 [root@www ~]# ps -ef --forest|grep nginx|grep -v grep root 59984 1 0 13:24 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 62919 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process nginx 62920 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process nginx 62921 59984 0 17:31 ? 00:00:00 \\_ nginx: worker process nginx 62928 59984 0 17:33 ? 00:00:00 \\_ nginx: worker process ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:1:2","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Master 进程 核心逻辑 master进程的主逻辑在ngx_master_process_cycle，核心关注源码： ngx_master_process_cycle(ngx_cycle_t cycle) { ... ngx_start_worker_processes(cycle, ccf-worker_processes, NGX_PROCESS_RESPAWN); ... for ( ;; ) { if (delay) {...} ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle-log, 0, sigsuspend); sigsuspend(\u0026set); ngx_time_update(); ngx_log_debug1(NGX_LOG_DEBUG_EVENT, cycle-log, 0, wake up, sigio %i, sigio); if (ngx_reap) { ngx_reap = 0; ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle-log, 0, reap children); live = ngx_reap_children(cycle); } if (!live \u0026\u0026 (ngx_terminate ngx_quit)) {...} if (ngx_terminate) {...} if (ngx_quit) {...} if (ngx_reconfigure) {...} if (ngx_restart) {...} if (ngx_reopen) {...} if (ngx_change_binary) {...} if (ngx_noaccept) { ngx_noaccept = 0; ngx_noaccepting = 1; ngx_signal_worker_processes(cycle, ngx_signal_value(NGX_SHUTDOWN_SIGNAL)); } } } 由上述代码，可以理解，master进程主要用来管理worker进程，具体包括如下4个主要功能： 1.接受来自外界的信号。其中master循环中的各项标志位就对应着各种信号，如：ngx_quit代表QUIT信号，表示优雅的关闭整个服务。 2.向各个worker进程发送信。比如ngx_noaccept代表WINCH信号，表示所有子进程不再接受处理新的连接，由master向所有的子进程发送QUIT信号量。 3.监控worker进程的运行状态。比如ngx_reap代表CHILD信号，表示有子进程意外结束，这时需要监控所有子进程的运行状态，主要由ngx_reap_children完成。 4.当woker进程退出后（异常情况下），会自动重新启动新的woker进程。主要也是在ngx_reap_children ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:1:3","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 进程管理：信号 多进程之间通讯可以使用信号、共享内存等，当进行 Nginx 进程间的管理时通常只使用信号 Nginx 进程管理：信号\r这里可以看到worker 进程接收到的信号和master 进程接收到信号基本上是一一对应的，为什么不直接对worker 进程直接发送信号呢，是因为需要通过master 进程来进行管理，这也是正常的处理方式。 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:1:4","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx reload热更新 Nginx reload热更新\r","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:2:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"不停机载入新配置 Nginx 不停机载入新配置\r","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:2:1","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"热重载-配置热更 Nginx 热重载-配置热更\rNginx热更配置时，可以保持运行中平滑更新配置，具体流程如下： 更新nginx.conf配置文件，向master发送SIGHUP信号或执行nginx -s reload Master进程使用新配置，启动新的worker进程 使用旧配置的worker进程，不再接受新的连接请求，并在完成已存在的连接后退出 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:2:2","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 热升级-程序热更 Nginx 版本热升级\rnginx热升级过程如下： 将旧Nginx文件换成新Nginx文件（注意备份） 向master进程发送USR2信号（平滑升级到新版本的Nginx程序） master进程修改pid文件号，加后缀.oldbin master进程用新Nginx文件启动新master进程，此时新老master/worker同时存在。 向老master发送WINCH信号，关闭旧worker进程，观察新worker进程工作情况。若升级成功，则向老master进程发送QUIT信号，关闭老master进程；若升级失败，则需要回滚，向老master发送HUP信号（重读配置文件），向新master发送QUIT信号，关闭新master及worker。 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:2:3","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Worker 进程 核心逻辑 worker进程的主逻辑在ngx_worker_process_cycle，核心关注源码： ngx_worker_process_cycle(ngx_cycle_t cycle, void data) { ngx_int_t worker = (intptr_t) data; ngx_process = NGX_PROCESS_WORKER; ngx_worker = worker; ngx_worker_process_init(cycle, worker); ngx_setproctitle(worker process); for ( ;; ) { if (ngx_exiting) {...} ngx_log_debug0(NGX_LOG_DEBUG_EVENT, cycle-log, 0, worker cycle); ngx_process_events_and_timers(cycle); if (ngx_terminate) {...} if (ngx_quit) {...} if (ngx_reopen) {...} } } 由上述代码，可以理解，worker进程主要在处理网络事件，通过ngx_process_events_and_timers方法实现，其中事件主要包括：网络事件、定时器事件。 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:3:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 的事件处理模型 request：Nginx 中 http 请求。 基本的 HTTP Web Server 工作模式： 接收请求：逐行读取请求行和请求头，判断段有请求体后，读取请求体 处理请求 返回响应：根据处理结果，生成相应的 HTTP 请求（响应行、响应头、响应体） Nginx 也是这个套路，整体流程一致。 Nginx 请求流程\r","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:4:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 模块 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:5:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 模块化体系结构 Nginx 模块化体系结构\rnginx的模块根据其功能基本上可以分为以下几种类型： event module: 搭建了独立于操作系统的事件处理机制的框架，及提供了各具体事件的处理。包括ngx_events_module， ngx_event_core_module和ngx_epoll_module等。nginx具体使用何种事件处理模块，这依赖于具体的操作系统和编译选项。 phase handler: 此类型的模块也被直接称为handler模块。主要负责处理客户端请求并产生待响应内容，比如ngx_http_static_module模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。 output filter: 也称为filter模块，主要是负责对输出的内容进行处理，可以对输出进行修改。例如，可以实现对输出的所有html页面增加预定义的footbar一类的工作，或者对输出的图片的URL进行替换之类的工作。 upstream: upstream模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取响应，发回客户端。upstream模块是一种特殊的handler，只不过响应内容不是真正由自己产生的，而是从后端服务器上读取的。 load-balancer: 负载均衡模块，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:5:1","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 模块介绍 Nginx一个非常重要的特性就是拥有丰富的模块，有核心的模块，拓展的模块和第三方拓展模块。 Nginx模块主要可以分为以下几类： 核心模块： HTTP 模块：用来发布http web服务网站的模块。 event模块：用来处理nginx 访问请求，并进行回复。 基本模块： HTTP Access模块: 用来进行虚拟主机发布访问模块，起到记录访问日志。 HTTP FastCGI模块：用于和PHP程序进行交互的模块，负责将来访问nginx 的PHP请求转发到后端的PHP上。 HTTP Proxy模块：配置反向代理转发的模块，负责向后端传递参数。 HTTP Rewrite模块：支持Rewrite 规则重写，支持域名跳转。 Nginx 模块中的内聚和抽象 Nginx 模块中的内聚和抽象\rNginx 模块分类 Nginx 模块分类\r模块的使用: 如我们想要打开gzip 压缩，可以在Nginx 模块官网上找到对应的模块参考ngx_http_gzip_module,查看使用示例 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:5:2","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"Nginx 进程间架构图 Nginx 进程间架构图\r","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:6:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"附录 百万并发下 Nginx 的优化之道 Nginx vs Apache ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:7:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":"参考 https://aosabook.org/en/v2/nginx.html https://www.fatalerrors.org/a/analysis-of-nginx-architecture.html Nginx 平台初探 https://www.bilibili.com/video/BV1KP4y1M75J?p=5\u0026vd_source=7e8c74e6ef5f4645e4247ef4120b7971 https://www.bilibili.com/video/BV1rY411E795/?spm_id_from=333.788.recommend_more_video.0\u0026vd_source=7e8c74e6ef5f4645e4247ef4120b7971 ","date":"2022-01-16","objectID":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/:8:0","tags":["Nginx"],"title":"Nginx 架构原理","uri":"/nginx-%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86/"},{"categories":["Nginx"],"content":" 方式一：yum 安装(Redhat) 方式二：二进制预编译安装 指定安装版本 安装操作系统版本CentOs7.x ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:0:0","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"方式一： yum 包管理器安装 这里选择通过yum 源进行安装，安装操作系统为Linux CentOS 7.9 CentOs 配置阿里云 yum 源 # 备份 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup # 下载yum 源 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo # 清除yum缓存 yum clean all # 缓存阿里云源 yum makecache # 测试阿里云源 yum list 安装依赖包， EPEL 仓库中有 Nginx 的安装包 yum install openssl-devel pcre-devel epel-release -y 安装nginx 服务 # 查看安装提供的版本 $ sudo yum list nginx --showduplicates | sort -r 已加载插件：fastestmirror 已安装的软件包 可安装的软件包 * updates: mirrors.aliyun.com nginx.x86_64 1:1.20.1-10.el7 epel nginx.x86_64 1:1.20.1-10.el7 @epel Loading mirror speeds from cached hostfile * extras: mirrors.aliyun.com * epel: pubmirror1.math.uh.edu * base: mirrors.aliyun.com # 安装指定版本 yum install nginx-1.20.1 -y # or 添加 Nginx 源 rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm # 查看nginx 版本 $ nginx -v nginx version: nginx/1.20.1 启动服务并配置开机自启动 systemctl start nginx systemctl eable nginx 测试 curl 192.168.1.20 浏览器访问 # 关闭防火墙 systemctl stop firewalld ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:1:0","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"方式二：二进制预编译安装 在Linux系统上安装Nginx的二进制版本通常指的是从Nginx官方网站下载预编译的软件包，而不是通过包管理器（如apt或yum）进行安装。以下是针对Nginx 1.20.1版本的二进制安装步骤： ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:0","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 1: 下载 Nginx 首先，你需要访问Nginx官方网站找到1.20.1版本的下载链接。对于稳定版，你可以使用wget命令直接下载： wget https://nginx.org/download/nginx-1.20.1.tar.gz ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:1","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 2: 解压下载的文件 下载完成后，解压缩文件到当前目录： tar -zxvf nginx-1.20.1.tar.gz 这将创建一个名为nginx-1.20.1的文件夹。 ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:2","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 3: 配置和编译 进入解压后的目录并准备编译Nginx。首先运行配置脚本以设置编译环境： cd nginx-1.20.1 ./configure 默认情况下，上述命令会使用默认配置进行编译。如果你想自定义安装路径或其他选项，可以添加相应的参数。例如，指定不同的安装前缀路径： ./configure --prefix=/data/nginx --with-http_ssl_module --with-http_v2_module --with-http_realip_module --with-http_addition_module --with-http_sub_module --with-http_stub_status_module --with-http_gzip_static_module --with-http_addition_module --with-pcre --with-stream 配置完成后，编译源代码： make ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:3","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 4: 安装 编译成功后，执行以下命令安装Nginx： sudo make install 这将会把Nginx安装到你之前配置时指定的位置，默认情况下是/data/nginx。 ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:4","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 5: 设置软链接（可选） 如果你希望简化Nginx命令的调用，可以为Nginx创建一个软链接到系统的sbin目录中： sudo ln -s /data/nginx/sbin/nginx /usr/local/sbin/nginx 这样，你就可以直接使用nginx命令来管理和操作Nginx服务器了。 ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:5","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 6: 启动 Nginx 安装完成后，可以通过以下命令启动Nginx： nginx # 指定配置文件 nginx -c /data/nginx/conf/nginx.conf 如果你想检查Nginx配置是否正确，可以在启动前使用如下命令测试配置文件： nginx -t 指定配置文件检查配置文件语法是否正常 sudo nginx -t -c /path/to/your/nginx.conf 重新加载Nginx配置 sudo nginx -s reload -c /path/to/your/nginx.conf ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:6","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤7 添加到shell 配置文件中 添加到你的shell配置文件（如.bashrc或.profile）中 echo 'export PATH=$PATH:/usr/local/sbin' \u003e\u003e ~/.bashrc source ~/.bashrc ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:7","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"步骤 8: 配置防火墙和SELinux（如果适用） 根据你的服务器安全设置，可能需要调整防火墙规则允许HTTP(S)流量，并且如果启用了SELinux，则可能还需要对其进行适当的配置以便让Nginx正常工作。 ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:8","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"注意事项 依赖库：确保系统已安装必要的依赖库，如PCRE、Zlib和OpenSSL等，这些对于Nginx的一些功能是必需的。如果你在./configure阶段遇到缺少依赖的问题，你可能需要先安装这些依赖。 更新与维护：虽然可以从二进制安装开始，但后续的更新可能会比通过包管理器更加复杂。建议定期检查Nginx官网获取最新的安全补丁和版本更新。 通过以上步骤，你应该能够成功地在Linux系统上安装Nginx 1.20.1的二进制版本。 Nginx 访问首页\r","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:2:9","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"Nginx 管理 ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:3:0","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"1. 通过systemctl或者service管理 nginx -t systemctl start nginx systemctl reload nginx systemctl restart nginx systemctl stop nginx ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:3:1","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"2. 通过nginx原生命令管理 设置nginx 软连接 sudo ln -s /PATH/nginx/sbin/nginx /usr/local/sbin/nginx 查找Nginx二进制文件的位置 通常，Nginx的二进制文件位于/usr/sbin/nginx，但这也可能根据你的安装位置有所不同。如果不确定，可以使用which nginx或whereis nginx来查找。 which nginx 或者 whereis nginx 检查Nginx配置文件语法是否正确 在启动或重新加载Nginx之前，建议先检查配置文件的语法是否有误。 $ nginx -t 这将测试配置文件的有效性，并告知你是否存在任何语法错误。 启动Nginx 要启动Nginx，请使用以下命令： $ nginx 如果没有指定配置文件路径，Nginx会默认寻找位于/etc/nginx/nginx.conf的主配置文件。如果你想指定一个不同的配置文件，可以使用-c选项： $ nginx -c /path/to/nginx.conf 停止Nginx 有几种方式可以停止Nginx： 快速停止：立即停止Nginx，不等待当前处理的请求完成。 $ nginx -s stop 优雅地停止（推荐）：允许Nginx完成当前正在处理的所有请求后再停止。 $ nginx -s quit 重新加载Nginx配置 当你修改了Nginx配置文件并希望重新加载而不中断现有连接时，可以使用以下命令： $ nginx -s reload 这个命令会告诉Nginx重新加载其配置文件。如果有任何语法错误，Nginx将忽略此命令并不会重新加载配置。 重启Nginx 虽然没有直接的“重启”命令，但你可以组合使用上述命令实现类似的效果。例如，先发送停止信号再启动Nginx： $ nginx -s quit \u0026\u0026 /usr/sbin/nginx 不过，更常用的是简单地使用reload命令，它可以在大多数情况下满足需求而不需要完全重启服务。 注意事项 在执行这些操作前，确保有足够的权限。通常需要以root用户或使用sudo执行这些命令。 直接使用Nginx命令行工具进行管理时，注意不要与systemctl或service同时使用，以免造成冲突或意外的服务状态变化。 总是先检查配置文件的正确性（使用nginx -t），然后再尝试重新加载或重启服务，以避免因配置错误导致服务无法正常启动。 ","date":"2022-01-15","objectID":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/:3:2","tags":["Nginx"],"title":"Nginx 安装部署","uri":"/nginx%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"},{"categories":["Nginx"],"content":"背景 整理在使用 Nginx 过程中频繁使用的Nginx 常用配置，另外补充一些常用命令和解决方案 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:1:0","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 命令行常用命令 nginx -s reload # 向主进程发送信号，重新加载配置文件，热重启 nginx -s reopen # 重启 Nginx nginx -s stop # 快速关闭 nginx -s quit # 等待工作进程处理完成后关闭 nginx -t # 查看当前 Nginx 配置是否有错误 nginx -t -c \u003c配置路径\u003e # 检查配置是否有问题，如果已经在配置目录，则不需要 - c ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:2:0","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"常用模块 nginx模块分为两种，官方和第三方，我们通过命令 nginx -V 查看 nginx已经安装的模块 [root@www ~]# nginx -V nginx version: nginx/1.20.1 built by gcc 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) built with OpenSSL 1.1.1k FIPS 25 Mar 2021 TLS SNI support enabled configure arguments: --prefix=/usr/share/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib64/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --http-client-body-temp-path=/var/lib/nginx/tmp/client_body --http-proxy-temp-path=/var/lib/nginx/tmp/proxy --http-fastcgi-temp-path=/var/lib/nginx/tmp/fastcgi --http-uwsgi-temp-path=/var/lib/nginx/tmp/uwsgi --http-scgi-temp-path=/var/lib/nginx/tmp/scgi --pid-path=/run/nginx.pid --lock-path=/run/lock/subsys/nginx --user=nginx --group=nginx --with-compat --with-debug --with-file-aio --with-google_perftools_module --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_degradation_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_image_filter_module=dynamic --with-http_mp4_module --with-http_perl_module=dynamic --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-http_xslt_module=dynamic --with-mail=dynamic --with-mail_ssl_module --with-pcre --with-pcre-jit --with-stream=dynamic --with-stream_ssl_module --with-stream_ssl_preread_module --with-threads --with-cc-opt='-O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic' --with-ld-opt='-Wl,-z,relro -specs=/usr/lib/rpm/redhat/redhat-hardened-ld -Wl,-E' Nginx模块名称 模块作用 ngx_http_access_module 四层基于IP的访问控制，可以通过匹配客户端源IP地址进行限制 ngx_http_auth_basic_module 状态页，使用basic机制进行用户认证，在编译安装nginx的时候需要添加编译参数–withhttp_stub_status_module，否则配置完成之后监测会是提示语法错误 ngx_http_stub_status_module 状态统计模块 ngx_http_gzip_module 文件的压缩功能 ngx_http_gzip_static_module 静态压缩模块 ngx_http_ssl_module nginx 的https 功能 ngx_http_rewrite_module 重定向模块，解析和处理rewrite请求 ngx_http_referer_module 防盗链功能，基于访问安全考虑 ngx_http_proxy_module 将客户端的请求以http协议转发至指定服务器进行处理 ngx_stream_proxy_module tcp负载，将客户端的请求以tcp协议转发至指定服务器处理 ngx_http_fastcgi_module 将客户端对php的请求以fastcgi协议转发至指定服务器助理 ngx_http_uwsgi_module 将客户端对Python的请求以uwsgi协议转发至指定服务器处理 ngx_http_headers_module 可以实现对头部报文添加指定的key与值 ngx_http_upstream_module 负载均衡模块，提供服务器分组转发、权重分配、状态监测、调度算法等高级功能 ngx_stream_upstream_module 后端服务器分组转发、权重分配、状态监测、调度算法等高级功能 ngx_http_fastcgi_module 实现通过fastcgi协议将指定的客户端请求转发至php-fpm处理 ngx_http_flv_module 为flv伪流媒体服务端提供支持 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:3:0","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 核心配置 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:4:0","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"配置块嵌套 配置块嵌套\r","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:4:1","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 默认配置文件 [root@www nginx]# cat nginx.conf|grep -v \\# user nginx; worker_processes auto; error_log /var/log/nginx/error.log; pid /run/nginx.pid; include /usr/share/nginx/modules/*.conf; events { worker_connections 1024; } http { # 数据传输性能相关的参数; log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log /var/log/nginx/access.log main; sendfile on; tcp_nopush on; tcp_nodelay on; keepalive_timeout 65; types_hash_max_size 4096; include /etc/nginx/mime.types; default_type application/octet-stream; include /etc/nginx/conf.d/*.conf; server { # 端口号 listen 80; listen [::]:80; # server_name 指令后可以跟多个指令，第一个为主域名 server_name _; #不做域名匹配，只根据虚拟主机内的port去匹配 root /usr/share/nginx/html; # 模块化引入外部配置 include /etc/nginx/default.d/*.conf; # nginx强大的基于url处理用户请求，就是基于location来的 location / { # 定义网站根目录 root /usr/share/nginx/html; # 定义首页文件 index index.html; } error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } } } main: 全局设置 events: 配置影响 Nginx 服务器或与用户的网络连接 http:http 模块设置 upstream: 负载均衡设置 server:http 服务器配置，一个 http 模块中可以有多个 server 模块 location:url 匹配配置，一个 server 模块中可以包含多个 location 模块 一个 nginx 配置文件的结构就像 nginx.conf 显示的那样，配置文件的语法规则： 配置文件由模块组成 使用#添加注释 使用 $ 使用变量 使用 include 引用多个配置文件 每一条语句结尾必须是分号结束 以区段形式的配置参数，需要有闭合的花括号 {} 不同作用域的配置参数，不能瞎嵌套 server{}是用于定义nginx的 http核心模块功能的子配置，必须防止在http{}括号外中 写在http{}外层，与其同级，语法报错 include配置参数：include /etc/nginx/default.d/*.conf; 导入外部的配置文件，优化，简化主配置文件的格式 http{}利用include导入外部的 server{}配置 include得写在http{}花括号内，才表示给这个区域导入外部的配置文件 只针对http{}区域生效 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:4:2","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Http Location 指令模块 location 是 ngx_http_core_module 核心模块下的指令，并且是最常用的指令，这里做一下整理记录 location 匹配规则\r指令的Location Syntax: location [ = | ~ | ~* | ^~ ] uri { ... } location @name { ... } Default: — Context: server, location location 匹配顺序\r举例 # = 精确匹配 location = / { [ configuration A ] } location / { [ configuration B ] } location /documents/ { [ configuration C ] } location ^~ /images/ { [ configuration D ] } location ~* \\.(gif|jpg|jpeg)$ { [ configuration E ] } 示例\r\" / “请求将匹配配置A， \" /index.html “请求将匹配配置B， \" /documents/document.html “请求将匹配配置C， \" /images/1.gif “请求将匹配配置D， \" /documents/1.jpg “请求将匹配配置E。\r","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:4:3","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 场景方案 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:0","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 配置动静分离 什么是动静分离 在 Web 开发中，通常来说，动态资源其实就是指那些后台资源，而静态资源就是指 HTML，JavaScript，CSS，img 等文件。 在使用前后端分离之后，可以很大程度的提升静态资源的访问速度，同时在开发过程中也可以让前后端开发并行可以有效的提高开发时间，也可以有效的减少联调时间 。 适合中小型网址 动静分离方案 直接使用不同的域名，把静态资源放在独立的云服务器上，这个种方案也是目前比较推崇的。 动态请求和静态文件放在一起，通过 nginx 配置分开 server { location /www/ { root /www/; index index.html index.htm; } location /image/ { root /image/; } } ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:1","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"nginx 配置反向代理 反向代理常用于不想把端口暴露出去，直接访问域名处理请求。 server { listen 80; server_name blog.dazhongma.top; location /swoole/ { proxy_pass http://127.0.0.1:9501; } location /node/ { proxy_pass http://127.0.0.1:9502; } } ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:2","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 负载均衡配置 # 定义一个名为phpServer的上游服务器组，用于负载均衡 upstream phpServer{ # 指定上游服务器组中的服务器列表和端口 server 127.0.0.1:9501; # 服务器1的地址和端口 server 127.0.0.1:9502; # 服务器2的地址和端口 server 127.0.0.1:9503; # 服务器3的地址和端口 } # 定义一个服务器块，用于处理HTTP请求 server { # 监听80端口，即HTTP默认端口 listen 80; # 设置服务器的域名 server_name blog.dazhongma.top; # 定义对根目录的请求的处理方式 location / { # 将请求代理到上游服务器组phpServer proxy_pass http://phpServer; # 关闭代理中的重定向 proxy_redirect off; # 设置代理请求中的Host头，使用原始请求的Host头 proxy_set_header Host $host; # 设置代理请求中的X-Real-IP头，使用原始请求的IP地址 proxy_set_header X-Real-IP $remote_addr; # 设置当代理请求失败时，如何进行下一步操作 proxy_next_upstream error timeout invalid_header; # 设置代理请求时，临时文件的最大大小为0，即不使用临时文件 proxy_max_temp_file_size 0; # 设置代理连接的超时时间 proxy_connect_timeout 90; # 设置代理发送请求的超时时间 proxy_send_timeout 90; # 设置代理读取响应的超时时间 proxy_read_timeout 90; # 设置代理使用的缓冲区大小 proxy_buffer_size 4k; # 设置代理使用的缓冲区数量和大小 proxy_buffers 4 32k; # 设置代理忙碌时使用的缓冲区大小 proxy_busy_buffers_size 64k; # 设置代理临时文件写入的最大大小 proxy_temp_file_write_size 64k; } } 常用负载均衡策略 round-robin / 轮询： 到应用服务器的请求以 round-robin / 轮询的方式被分发 upstream phpServer{ server 127.0.0.1:9501 weight=3; server 127.0.0.1:9502; server 127.0.0.1:9503; } 在这个配置中，每 5 个新请求将会如下的在应用实例中分派： 3 个请求分派去 9501, 一个去 9502, 另外一个去 9503. least-connected / 最少连接：下一个请求将被分派到活动连接数量最少的服务器 upstream phpServer{ least_conn; server 127.0.0.1:9501; server 127.0.0.1:9502; server 127.0.0.1:9503; } 当某些请求需要更长时间来完成时，最少连接可以更公平的控制应用实例上的负载。 ip-hash/IP 散列： 使用 hash 算法来决定下一个请求要选择哪个服务器 (基于客户端 IP 地址) upstream phpServer{ ip_hash; server 127.0.0.1:9501; server 127.0.0.1:9502; server 127.0.0.1:9503; } 将一个客户端绑定给某个特定的应用服务器； ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:3","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 配置跨域 由于浏览器同源策略的存在使得一个源中加载来自其它源中资源的行为受到了限制。即会出现跨域请求禁止。 所谓同源是指：域名、协议、端口相同。 URL 结果 原因 http://blog.dazhongma.top/other/index.html 成功 域名、协议、端口均 相同 https://blog.dazhongma.top/home/index.html 失败 协议不同 http://blog.dazhongma.top:8080/home/index.html 失败 端口不同 http://www.dazhongma.com/home/index.html 失败 域名不同 server { listen 80; server_name blog.dazhongma.top; root /Users/shiwenyuan/blog/public; index index.html index.htm index.php; location / { try_files $uri $uri/ /index.php?$query_string; } add_header 'Access-Control-Allow-Origin' \"$http_origin\"; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, DELETE, PUT, PATCH'; add_header 'Access-Control-Allow-Headers' 'DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,X-XSRF-TOKEN'; location ~ \\.php$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } Access-Control-Allow-Origin：允许的域名，只能填 *（通配符）或者单域名。 Access-Control-Allow-Methods: 允许的方法，多个方法以逗号分隔。 Access-Control-Allow-Headers: 允许的头部，多个方法以逗号分隔。 Access-Control-Allow-Credentials: 是否允许发送 Cookie。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:4","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"URLRewrite 伪静态配置 能隐藏后端服务器的地址 URLRewrite的使用场景 配置方式 http://192.168.143.101/index.jsp?pageNum=2 伪静态配置后，隐藏请求参数 http://192.168.143.101/2.html rewrite \u003cregex\u003e \u003creplacement\u003e [flag]; 关键字 正则 替代内容 flag标记 rewrite 参数的标签段位置：server,location,if flag标记说明： last # 本条规则匹配完成后，继续向下匹配新的location URI规则 break # 本条规则匹配完成即终止，不再匹配后面的任何规则 redirect # 返回302 临时重定向，浏览器地址会显示跳转后的URL地址 permant # 301 永久重定向，浏览器地址会显示跳转后的URL地址 配置 location / { # ([0-9]+) 入参，$1 是 ([0-9]+)的占位符 rewrite ^/([0-9]+).html$ /index.jsp?pageNum=$1 break; proxy_pass http://192.168.143.104:8080; } ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:5","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"配置防盗链 防盗链本质上是存在自己服务器上的资源，只能由自己的服务器来访问，其实更多的时候网站还是希望自己的能够有更多的流量进来，而不是添加防盗链防止别人引用，只是在个别的情况，例如微信公众号文章中的图片不允许别人引用时会添加防盗链 http协议中的referer nginx 防盗链配置 使用浏览器或curl检测 返回错误码 返回错误页面 整合rewrite 返回报错图片 防盗链配置 valid_referes none | blocked |server_names | strings ...; none，检测referer 头域不存在的情况 blocked,检测referer头域的值被防火墙或者代理服务器删除或伪装的情况。这种情况头域的值不以\"http://\" 或 “https://” 开头 server_names,设置一个或多个URL，检测Referer头域的值是否是这些URL中的某一个 配置示例 location ~*/(js|img|css) { # 检测来源的网址，而不是ip valid_referes 192.168.44.101; if($valid_referer){ return 403 } root html; index index.html index.htm; } 防盗链返回页面配置 # 防盗链返回页面配置 error_page 401 /401.html; location = /401.html { } error_page 404 /404.html; location = /404.html { } error_page 500 502 503 504 /50x.html; location = /50x.html { } ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:6","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 高可用配置 高可用场景及解决方案 安装keepalived 选举方式(优先级配置) 示例节点192.168.143.101和192.168.143.102,vip 192.168.143.100 安装 # 安装依赖 $ yum install openssl-devel $ yum install keepalived $ vim /etc/keepalived/keepalived.conf 192.168.143.101 master 节点keepalived配置 ! Configuration File for keepalived global_defs { router_id LVS_DEVEL-101 } vrrp_instance demo_keepalived { state MASTER # 注意修改网卡信息 interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.143.100 } } 192.168.143.102 backup 节点keepalived配置 ! Configuration File for keepalived global_defs { router_id LVS_DEVEL-102 } vrrp_instance demo_keepalived { state BACKUP # 注意修改网卡信息 interface ens32 virtual_router_id 51 priority 50 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.143.100 } } ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:7","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx https 加密认证访问 https 证书配置 https原理 CA 机构 证书 客户端(浏览器) 服务器端 证书自签名 在线证书申请 证书自签名 一般是用在内网中，通过openssl进行创建，对于公网还是需要进行申请购买的 生产nginx 自签名证书 # 命名时作区分使用(个人习惯) $ host_ip=192.168.143.101 # 1、模拟创建CA server_${host_ip}.key 私钥 ： 生成key：（生成rsa私钥，des3对称加密算法，openssl格式，2048位强度） # You must type in 4 to 1023 characters 使用 host_ip 地址即可 $ openssl genrsa -des3 -out server_${host_ip}.key 2048 # 2、通过以下方法生成没有密码的key $ openssl rsa -in server_${host_ip}.key -out server_${host_ip}.key # 3、[私钥加密]通过 CA server_${host_ip}.key 私钥钥生成自签名的 CA ca.crt 证书，该证书包含CA机构的公钥：（用来签署下面的server.csr文件,一路回车 $ openssl req -new -x509 -key server_${host_ip}.key -out ca.crt -days 3650 # 4、生成证书签名csr请求 certificate sign request，包含1）公钥 2）申请者信息 3）域名, ：一路回车 $ openssl req -new -key server_${host_ip}.key -out server.csr # 5、[公钥验签(签名)] 将csr证书签名请求发送给CA机构，CA机构中对 ca.crt 证书进行签名处理并生成crt server_${host_ip}.crt： # 也就是说公钥 server_${host_ip}.crt 相对于 公钥 ca.crt多了签名处理的 $ openssl x509 -req -days 3650 -in server.csr -CA ca.crt -CAkey server_${host_ip}.key -CAcreateserial -out server_${host_ip}.crt 总结： 生成私钥server_${host_ip}.key 生成没有密码的私钥key(可选) 自签名的 CA ca.crt 证书，该证书包含CA机构的公钥 生成证书签名csr请求(CSR) 公钥验签(签名) 验证证书 图形化工具XCA ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:8","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 开启http并重定向到https 开启http 开启http很简单，直接把listen 80;加到listen 443 ssl;上面去就可以了。或者新加一个server配置，如下： server { listen 443 ssl; server_name localhost; ssl_certificate /key-path/localhost.pem; ssl_certificate_key /key-path/localhost.key; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; location / { proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8000/; } } server { listen 80; server_name localhost; location / { proxy_set_header HOST $host; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8000/; } } 重定向到https的两种方式 要把http重定向到https也很简单，具体可以使用两种配置来实现。 第一种方式使用return 301如下： server { listen 80; server_name localhost; return 301 https://127.0.0.1$request_uri; } 第二种方式使用rewrite如下： server { listen 80; server_name localhost; rewrite ^(.*)$ https://$host$1 permanent; } 对于return和rewrite的区别，可以阅读这篇文章：Creating NGINX Rewrite Rules ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:9","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"nginx 对客户端和上游服务器使用keepalive 配置详解 Nginx 中的 keepalive 是一种机制，用于在客户端和服务器之间保持连接的活跃状态，从而避免每次请求都需要重新建立 TCP 连接所带来的开销。这可以显著提高性能，尤其是在高延迟网络环境中。以下是对 Nginx 配置 keepalive 的详细介绍。 1. HTTP Keepalive 对于 HTTP 协议，可以通过以下指令来配置 keepalive： keepalive_timeout：设置 keep-alive 客户端连接在服务器端保持打开状态的时间长度。例如： keepalive_timeout 65; 上面的例子表示如果一个连接在65秒内没有活动，那么它将被关闭。 keepalive_requests：指定一个 keep-alive 连接上可以服务的最大请求数量。默认情况下这个值是100。 keepalive_requests 100; keepalive_disable：允许禁用对某些浏览器的支持。一些旧版浏览器可能无法正确处理 keep-alive 请求。 keepalive_disable msie6; ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:10","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"2. Upstream Keepalive nginx 对上游服务器使用keepalive 配置详解 当与上游服务器（如应用服务器）通信时，也可以使用 keepalive 来优化连接。这需要通过 upstream 块进行配置，并且需要使用 keepalive 指令来指定要缓存的空闲 keepalive 连接数。 upstream backend { server backend1.example.com; server backend2.example.com; keepalive 32; # 设置保持连接的空闲连接数为32 keepalive_requests 1000; keepalive_timeout 65; # 可以不配，直接使用默认 } server { location / { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; } } 这里需要注意的是，为了使 upstream keepalive 起作用，必须将 proxy_http_version 设置为 1.1 并清除 Connection 头部字段。 注意事项 使用 keepalive 可能会增加服务器资源的消耗，因为它需要维护更多的连接状态。因此，合理设置 keepalive 相关参数是非常重要的。 对于 HTTPS 站点，除了上述配置外，还需要考虑 SSL/TLS 握手的成本。尽管 keepalive 可以重用已建立的安全连接，减少重复握手带来的延迟，但是首次连接仍需完成完整的 SSL/TLS 握手过程。 通过适当的配置，Nginx 的 keepalive 功能可以帮助提升网站或应用的响应速度和整体性能。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:11","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 内存与文件缓冲区 Nginx 作为一款高性能的HTTP和反向代理服务器，其处理请求的能力很大程度上依赖于其对内存和文件缓冲区的有效管理。下面介绍Nginx反向代理的内存与文件缓冲区的核心流程。 Header 缓冲区 proxy_buffer_size：这个指令用于设置读取来自后端服务器响应的第一部分（通常包括响应头）的缓冲区大小。由于响应头可能包含一些较大的Cookie或其它元数据，适当调整这个值可以确保头部信息能够被完整地缓存而不会导致额外的磁盘I/O操作。 Body 缓冲区 proxy_buffering on;：启用或禁用缓冲功能。当开启时，Nginx会尝试先将从上游服务器接收到的数据存储到由proxy_buffers指定的内存缓冲区中。如果响应数据量超过了内存缓冲区容量，则超出部分会被写入由proxy_temp_path定义的临时文件中。若关闭(off)，则所有响应内容都将直接发送给客户端，而不经过任何缓冲，这可能会增加客户端等待时间并减少服务器性能。 proxy_buffers 32 64k;：此指令定义了用于存储响应体的缓冲区数量及其大小。在这个例子中，它表示分配了32个每个大小为64KB的缓冲块来保存响应体数据。合理的配置可以根据预期的响应体大小及系统资源状况进行调整，以优化性能。 proxy_max_temp_file_size 1G;：当响应体过大以至于超过内存缓冲区容量时，Nginx会将多余的数据写入临时文件。该参数限制了这些临时文件的最大总尺寸，默认情况下是1GB。如果达到这个限制，Nginx将停止向磁盘写入新的数据，并开始直接将剩余的响应流式传输给客户端。 proxy_temp_file_write_size 8k;：控制一次写入临时文件的数据量。这里的例子表明每次写操作的大小被设定为8KB。合理设置这个值有助于平衡磁盘I/O负载，尤其是在处理大量并发请求时尤为重要。 通过上述配置，Nginx能有效地管理反向代理过程中的数据流动，既保证了快速响应又能有效利用服务器资源。需要注意的是，实际应用中应根据具体情况（如服务器硬件条件、预计的流量规模等）调整这些参数，以实现最佳性能。 示例 http { # 其他配置项... server { listen 80; server_name example.com; location / { proxy_pass http://backend_server; # 假设后端服务器地址 # 设置Header缓冲区大小 proxy_buffer_size 128k; # 启用响应体数据的缓冲功能 proxy_buffering on; # 设置用于读取响应体的缓冲区数量和大小 proxy_buffers 32 64k; # 当响应体过大时，限制写入磁盘的临时文件的最大总尺寸为1G proxy_max_temp_file_size 1G; # 控制一次写入临时文件的数据量为8KB proxy_temp_file_write_size 8k; } } # 定义上游服务器组 upstream backend_server { server 192.168.1.1:8080; # 后端服务器的实际IP和端口 server 192.168.1.2:8080; # 可以添加多个后端服务器进行负载均衡 } } 配置解释： proxy_buffer_size 128k;：增大了header部分的缓冲区大小到128KB，适用于包含较大头部信息的情况。 proxy_buffering on;：启用缓冲功能。这意味着Nginx会尝试先将从后端服务器接收到的数据存储在内存中，而不是直接传递给客户端。 proxy_buffers 32 64k;：设置了32块每块64KB大小的缓冲区来存储响应体内容。这样总共提供了2MB（32*64KB）的内存空间来缓存响应体数据。 proxy_max_temp_file_size 1G;：当响应体太大以至于超过内存缓冲区容量时，Nginx会将超出的数据写入磁盘上的临时文件，此设置限制了这些临时文件的最大总尺寸为1GB。 proxy_temp_file_write_size 8k;：控制了一次写入磁盘临时文件的数据量为8KB，有助于平衡磁盘I/O操作。 这个配置实例展示了如何使用上述参数来优化Nginx作为反向代理时的性能表现。根据实际情况，比如后端服务响应的数据量、服务器硬件资源等，你可以适当调整这些数值以达到最佳效果。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:12","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 对客户端的限制 Nginx 提供了多种配置选项来限制客户端请求，包括对请求体大小、头部大小、超时时间等方面的控制。以下是对你提到的关键词进行补充并给出完整释义，以及一个配置示例。 关键词解释 client_body_buffer_size：设置用于读取客户端请求体（body）的缓冲区大小。如果请求体超过了这个缓冲区大小，数据将被写入磁盘上的临时文件。这对于处理大文件上传等场景非常重要。 client_header_buffer_size：设置用于读取客户端请求头（header）的缓冲区大小。大多数情况下，默认值足以满足需求，但如果客户端发送非常大的请求头（如包含大量Cookie或长URL），则可能需要增加此值。 client_body_temp_path：指定当请求体超过client_body_buffer_size时，Nginx将请求体数据写入磁盘的临时文件存储路径。合理设置可以避免系统默认临时目录空间不足的问题。 client_max_body_size 1000M;：定义允许客户端请求的最大主体大小。如果请求体超过这个值，Nginx会返回413 (Request Entity Too Large)错误。设置为0可禁用检查。 client_body_timeout：设置等待客户端发送请求体的超时时间。如果在设定的时间内没有收到完整的请求体，Nginx将关闭连接。 client_header_timeout：设置等待客户端发送请求头的超时时间。如果在这个时间内没有收到完整的请求头，Nginx也将关闭连接。 配置示例 http { # 设置客户端请求体的缓冲区大小 client_body_buffer_size 16k; # 设置客户端请求头的缓冲区大小 client_header_buffer_size 2k; # 设置当请求体过大时，Nginx写入磁盘的临时文件存储位置 client_body_temp_path /var/lib/nginx/body; # 允许客户端请求的最大主体大小为1000MB client_max_body_size 1000M; # 如果在60秒内没有读取到请求体，就报超时错误 client_body_timeout 60s; # 如果在30秒内没有读取到请求头，就报超时错误 client_header_timeout 30s; server { listen 80; server_name example.com; location / { proxy_pass http://backend_server; } } upstream backend_server { server 192.168.1.1:8080; server 192.168.1.2:8080; } } 补充说明 对于处理大文件上传的场景，除了调整client_body_buffer_size和client_max_body_size外，还需要确保服务器有足够的磁盘空间来存放临时文件，并且正确设置了client_body_temp_path。 client_body_timeout和client_header_timeout可以根据实际网络状况和应用需求进行调整，以平衡性能和用户体验。 考虑安全性，建议根据实际情况合理设置client_max_body_size，防止因过大的请求导致服务器资源耗尽。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:13","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx Gzip压缩 使用示例 http { ... # nginx 动态压缩 和 静态压缩结合使用会更好 # 开启gzip 静态压缩 gzip_static on; # 作为反向代理时，针对上游服务器返回的头信息进行压缩 gzip_proxied expired no-cache no-store private auth; # 开启gzip 动态压缩 gzip on; # 启用gzip压缩的最小文件，小于设置值的文件将不会压缩 gzip_min_length 1k; # gzip 压缩级别，1-9，数字越大压缩的越好，也越占用CPU时间，后面会有详细说明 gzip_comp_level 6; # 进行压缩的文件类型。javascript有多种形式。其中的值可以在 mime.types 文件中找到。 gzip_types text/plain application/javascript application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png application/vnd.ms-fontobject font/ttf font/opentype font/x-woff image/svg+xml; # 是否在http header中添加Vary: Accept-Encoding，建议开启，不配置也就可以 gzip_vary on; # 禁用IE 1-6版本的 gzip压缩，大型网站建议不要配置，建议尽量不要在配置文件中出现正则表达式，少用正则，因为这是影响性能的一个选项 # gzip_disable \"MSIE [1-6]\\.\"; # 设置压缩所需要的缓冲区大小 gzip_buffers 32 4k; # 设置gzip压缩针对的HTTP协议版本 gzip_http_version 1.0; ... server { listen 80; listen [::]:80; ... } } Gzip 动态压缩及缺点 缺点：开启gzip 压缩后，sendfile 零拷贝则会失效 Gzip 静态压缩 静态压缩开始不会导致sendfile 零拷贝失效 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:14","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 配置正向代理与反向代理缓存 正向代理配置参考 $ proxy_pass $scheme://$host$request_uri; $ resolver 8.8.8.8; 反向代理缓存proxy_cache配置示例 http { ... upstream backend { keepalive 1000; # 设置最大空闲连接数为32 keepalive_requests 1000; server 192.168.143.102; } proxy_cache_path /ngx_tmp levels=1:2 keys_zone=test_cache:100m inactive=1d max_size=10g; ... server { listen 80; listen [::]:80; .... location / { proxy_pass http://backend; proxy_http_version 1.1; proxy_set_header Connection \"\"; # proxy 反向代理缓存 add_header Nginx-Cache \"$upstream_cache_status\"; proxy_cache test_cache; proxy_cache_valid 1h; # 到上游服务器取数据多久过期 } } ... } nginx 反向代理缓存proxy_cache 配置的优点 Nginx的proxy_cache配置主要用于实现反向代理缓存，它有多个显著的好处，可以极大地提升Web应用的性能和可靠性： 提高响应速度 减少后端服务器负载：通过缓存静态内容或频繁请求的数据，可以大幅减少后端服务器处理请求的次数，从而提高响应速度。 快速响应用户请求：对于已经缓存的内容，Nginx可以直接从本地磁盘提供服务，而不需要转发到后端服务器进行处理，这大大加快了响应时间。 增强系统的可扩展性和稳定性 平滑流量高峰：在流量高峰期，缓存可以减轻后端服务器的压力，避免因过载导致的服务中断或响应迟缓。 提高可用性：即使后端服务器暂时不可用（例如由于维护或故障），Nginx也可以继续为用户提供缓存的内容，确保服务的连续性。 优化资源利用 节省带宽：缓存机制减少了重复数据传输的需求，有助于节省网络带宽。 降低数据库压力：对于动态内容，如果部分数据可以被缓存（如查询结果），则可以减少对数据库的直接访问次数，降低数据库的压力。 灵活的配置选项 细粒度控制：Nginx提供了丰富的指令来控制缓存行为，比如设置缓存的有效期、如何更新缓存、哪些响应应该被缓存等。 支持条件缓存：可以根据请求方法、响应头等多种条件决定是否缓存响应，使得缓存策略更加灵活和精确。 实现简单且高效 易于部署和管理：与应用程序级别的缓存相比，Nginx的反向代理缓存配置相对简单，容易集成到现有架构中。 高效的存储机制：Nginx采用高效的文件系统存储方式，并支持内存缓存以加速频繁访问的内容。 合理地使用Nginx的proxy_cache不仅可以显著提升用户体验，还能增强整个系统的稳定性和效率。不过需要注意的是，缓存策略的设计应当考虑到数据的一致性和时效性要求，以免影响到用户的体验或业务逻辑的正确执行。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:15","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 配置带宽限制 Nginx 提供了多种方式来限制带宽，以满足不同的需求场景。这通常用于防止服务器过载、保障服务质量或控制内容分发速度等。下面是一些常见的使用场景以及相应的配置示例。 场景一：限制客户端下载速度 在某些情况下，你可能希望限制客户端从你的网站下载文件的最大速度，以确保所有用户都能获得良好的服务体验，或者控制资源的使用情况。 示例配置： server { listen 80; server_name example.com; location /downloads/ { # 使用的是令牌桶算法，限制下载速率为每秒100KB limit_rate 100k; # 可选：设置初始爆发速率，这里为500KB limit_rate_after 500k; root /var/www/downloads; } } 在这个例子中，limit_rate 指令设置了每个响应的最大传输速率（这里是100KB/s），而 limit_rate_after 则指定了在开始限速之前允许传输的数据量（这里是500KB）。这意味着对于前500KB的数据，不会有任何速率限制，之后的速度将被限制在100KB/s。 场景二：基于连接数和请求频率的流量控制 有时，除了限制带宽外，还需要对来自单个IP地址的并发连接数或请求频率进行限制，以防止恶意攻击或滥用资源。 示例配置： http { # 定义一个名为 addr 的限流区，限制每秒请求数不超过20次 limit_req_zone $binary_remote_addr zone=addr:10m rate=20r/s; server { listen 80; server_name example.com; location / { # 应用前面定义的限流区，并允许突发最多5个额外请求 limit_req zone=addr burst=5 nodelay; # 限制每个连接的带宽为50KB/s limit_rate 50k; proxy_pass http://backend; } } } 这里使用了 limit_req_zone 和 limit_req 来限制每个IP地址的请求频率，同时通过 limit_rate 来限制带宽。这样可以有效地保护服务器免受过多请求的影响，同时也控制了数据传输速度。 场景三：针对不同用户组应用不同的带宽限制 有时候需要根据用户的类型（如免费用户与付费用户）提供不同的服务级别，包括不同的下载速度。 示例配置： map $http_x_user_type $download_rate { default \"50k\"; # 默认下载速率为50KB/s premium \"200k\"; # 高级用户的下载速率为200KB/s } server { listen 80; server_name example.com; location /downloads/ { # 根据用户类型动态设置下载速率 limit_rate $download_rate; root /var/www/downloads; } } 在这个配置中，我们使用了 map 指令根据自定义的HTTP头（本例中的 X-User-Type）来决定使用哪个下载速率。这使得可以根据用户的类别灵活地调整服务等级。 这些只是Nginx带宽限制的一些基础应用场景和配置示例。实际部署时，你可能需要根据具体的需求和环境进行适当的调整。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:16","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 并发数限制 Nginx 的并发限制功能主要用于控制来自单一客户端或一组客户端的连接数，以防止服务器过载或被滥用。通过使用 limit_conn_zone 和 limit_conn 指令，可以有效地管理资源并确保服务的稳定性。以下是几个常见的使用场景及相应的配置示例。 场景一：限制单个IP地址的最大并发连接数 在Web服务器上，可能会遇到某些用户同时发起大量请求的情况，这可能导致服务器资源耗尽，影响其他用户的访问体验。通过限制每个IP地址的最大并发连接数，可以有效缓解这种问题。 示例配置： http { # 定义一个名为 addr 的限流区，基于客户端的 IP 地址进行限制 limit_conn_zone $binary_remote_addr zone=addr:10m; server { listen 80; server_name example.com; location / { # 允许每个IP地址最多有10个并发连接 limit_conn addr 10; proxy_pass http://backend; } } } 在这个例子中，limit_conn_zone 使用 $binary_remote_addr 变量作为键（即客户端的IP地址），并在共享内存区域 addr 中保存状态信息，大小为10MB。然后，在具体的 location 块中使用 limit_conn 来指定每个IP地址允许的最大并发连接数为10。 场景二：根据不同的URL路径应用不同的并发限制 有时需要对不同类型的资源应用不同的并发限制策略。例如，对于静态资源和动态内容可能希望设置不同的最大并发连接数。 示例配置： http { # 定义两个限流区，一个基于IP地址，另一个基于URI limit_conn_zone $binary_remote_addr zone=per_ip:10m; limit_conn_zone $request_uri zone=per_uri:10m; server { listen 80; server_name example.com; location /static/ { # 对于静态资源，限制每个IP地址最多有20个并发连接 limit_conn per_ip 20; } location /api/ { # 对于API请求，限制每个URI最多有5个并发连接 limit_conn per_uri 5; proxy_pass http://backend_api; } } } 此配置为 /static/ 路径下的请求设置了基于IP地址的并发限制，而为 /api/ 路径下的请求设置了基于URI的并发限制。 场景三：结合速率限制与并发限制 在一些高流量网站中，不仅需要限制并发连接数，还需要限制请求的速率，以进一步保护后端服务免受突发流量的影响。 示例配置： http { # 定义并发限制区 limit_conn_zone $binary_remote_addr zone=addr:10m; # 定义速率限制区 limit_req_zone $binary_remote_addr zone=req_limit_per_ip:10m rate=5r/s; server { listen 80; server_name example.com; location / { # 设置每个IP地址最多有5个并发连接 limit_conn addr 5; # 设置每个IP地址每秒最多处理5个请求，并允许突发最多10个额外请求 limit_req zone=req_limit_per_ip burst=10 nodelay; proxy_pass http://backend; } } } 这个例子展示了如何同时应用并发限制和速率限制来保护你的Web应用。通过这种方式，既可以控制同一时间内的最大连接数，也能平滑请求到达的速率，避免瞬间的大流量冲击。 以上是关于Nginx并发限制的一些典型应用场景及其配置示例。根据实际情况调整这些参数，可以帮助你更好地管理和优化你的Web服务性能。 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:17","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx 日志管理 应用场景 访问日志 错误日志 日志分隔 ngx_http_log_module 日志内存缓冲区 包日志压缩解压缩与json格式输出样例 示例 http { log_format main '$remote_addr - $remote_user [$time_local] \"$request\" ' '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; # buffer=32k 当日志大小达到32k时会写入至指定的日志文件下 # access_log /var/log/nginx/access.log main buffer=32k gzip=3 flush=1s; access_log /var/log/nginx/access.log main buffer=32k; ... } json格式输出 要在Nginx中配置日志格式为JSON输出，你需要定义一个自定义的日志格式，并使用log_format指令将其设置为JSON结构。以下是一个示例配置，展示了如何设置Nginx以JSON格式记录访问日志。 示例配置 首先，在你的Nginx配置文件（通常是nginx.conf或位于sites-available目录下的某个配置文件）中的http块内添加如下定义： http { # 定义JSON格式的日志 log_format json_combined escape=json '{\"time_local\": \"$time_local\", ' '\"remote_addr\": \"$remote_addr\", ' '\"remote_user\": \"$remote_user\", ' '\"request\": \"$request\", ' '\"status\": $status, ' '\"body_bytes_sent\": $body_bytes_sent, ' '\"request_time\": $request_time, ' '\"http_referer\": \"$http_referer\", ' '\"http_user_agent\": \"$http_user_agent\"}'; server { listen 80; server_name example.com; access_log /var/log/nginx/access.log json_combined; location / { proxy_pass http://backend; } } } 释义 log_format：这里我们定义了一个名为json_combined的日志格式。使用escape=json确保所有特殊字符都被正确转义，避免破坏JSON结构。 字段说明： $time_local: 请求的本地时间。 $remote_addr: 客户端IP地址。 $remote_user: 远程用户（通常用于身份验证；如果没有，则为空）。 $request: 完整的原始请求行。 $status: HTTP响应状态码。 $body_bytes_sent: 发送给客户端的字节数，不包括响应头的大小。 $request_time: 请求处理时间，单位为秒，精度达到毫秒。 $http_referer: 来源页面（即用户是从哪个页面链接过来的）。 $http_user_agent: 用户代理（浏览器类型等信息）。 日志输出样例 基于上述配置，一条典型的Nginx访问日志记录将以如下JSON格式输出到/var/log/nginx/access.log文件中： { \"time_local\": \"12/Feb/2025:17:32:00 +0000\", \"remote_addr\": \"192.168.1.1\", \"remote_user\": \"-\", \"request\": \"GET /index.html HTTP/1.1\", \"status\": 200, \"body_bytes_sent\": 612, \"request_time\": 0.002, \"http_referer\": \"http://example.com/start.html\", \"http_user_agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\" } 这个例子展示了一个HTTP GET请求的日志条目，其中包含了请求的各种详细信息。通过将Nginx的日志格式化为JSON，可以更方便地进行日志分析、监控以及与其他系统集成。 日志切割 日志分隔的必要性 避免单个文件过大：长时间运行会导致日志文件占用过多磁盘空间。 便于归档和排查：按时间或大小分隔日志，方便定位问题。 结合压缩节省空间：压缩旧日志减少存储压力。 使用 logrotate 工具（推荐） logrotate 是Linux系统自带的日志管理工具，支持自动化轮转、压缩和删除旧日志。 步骤： 编辑Nginx的logrotate配置文件 通常路径为 /etc/logrotate.d/nginx，内容如下： /var/log/nginx/*.log { # 匹配Nginx日志路径 daily # 按天轮转 missingok # 日志不存在时不报错 rotate 14 # 保留最近14天的日志 compress # 压缩旧日志（gzip） delaycompress # 延迟压缩前一个轮转的日志（方便排查最新旧日志） notifempty # 空日志不轮转 create 0640 www-data adm # 创建新日志文件并设置权限、所有者 sharedscripts # 所有日志处理完成后执行脚本 postrotate # 轮转后执行的命令 [ -f /var/run/nginx.pid ] \u0026\u0026 kill -USR1 `cat /var/run/nginx.pid` # 通知Nginx重新打开日志 endscript } 测试配置 手动执行轮转并检查结果： logrotate -vf /etc/logrotate.d/nginx # -v: 详细输出，-f: 强制运行 验证日志分隔 检查 /var/log/nginx 目录是否生成类似 access.log.1.gz 的压缩文件，并确认Nginx正常写入新日志。 关键注意事项 权限问题 确保新日志文件所有者（如 www-data）与Nginx运行用户一致。 logrotate 配置中使用 create 指令设置权限。 信号机制 kill -USR1 通知Nginx重新打开日志文件，无需重启服务。 也可用 nginx -s reopen 命令实现相同效果。 轮转周期调整 修改 logrotate 配置中的 daily 为 weekly、monthly 或 size（如 size 100M）。 日志处理 使用 compress 和 delaycompress 控制压缩行为。 通过 rotate 设置保留的旧日志数量。 配置多个目录的方式 # /etc/logrotate.d/nginx /var/log/nginx/*.log { daily rotate 14 compress missingok create 0640 www-data adm postrotate nginx -s reopen endscript } # /etc/logrotate.d/nginx /opt/nginx/logs/*.log /data/logs/nginx/*.log { daily rotate 30 missingok dateext compress delaycompress notifempty sharedscripts postrotate [ -f /opt/nginx/logs/nginx.pid ] \u0026\u0026 kill -USR1 `cat /opt/nginx/logs/nginx.pid` endscript } ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:18","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"Nginx upsream 重试机制 探测上游服务器健康状态 重试机制(被动重试机制和主动重试机制) 状态检查 Nginx的upstream模块用于定义一组服务器，这些服务器可以是提供相同服务的多个后端实例。Nginx能够根据配置将请求分配给这些上游服务器，并且在某些情况下自动进行重试，这有助于提高系统的可用性和可靠性。 被动态式重试机制 当一个请求发送到上游服务器失败时（例如由于超时、连接失败或返回了错误码），Nginx可以根据配置决定是否以及如何重试该请求。这个过程通过proxy_next_upstream指令来控制。它允许你指定在什么条件下Nginx应该尝试转发请求到下一个可用的上游服务器。 可用条件包括 error: 当与服务器建立连接、传递请求或读取响应头时发生错误。 timeout: 超时发生时（由其他指令如proxy_connect_timeout, proxy_read_timeout等定义）。 invalid_header: 服务器返回无效响应头。 http_500, http_502, http_503, http_504: 分别对应HTTP状态码500, 502, 503和504。 non_idempotent: 默认情况下，只有幂等请求（如GET, HEAD）会被重试。设置此选项可允许非幂等请求（如POST）也被重试。 示例配置 下面是一个使用proxy_next_upstream的例子，展示了如何配置Nginx以实现动态式重试机制： http { upstream backend { server backend1.example.com max_fails=5; server backend2.example.com; server backend3.example.com backup; # 备份服务器，仅当前面所有服务器都不可用时使用 } server { listen 80; location / { proxy_pass http://backend; # 配置重试条件 proxy_next_upstream error timeout http_500 http_502 http_503 http_504; # 设置尝试下一个上游服务器前等待的时间 proxy_next_upstream_timeout 60s; # 设置每个请求最多尝试多少次上游服务器 proxy_next_upstream_tries 3; } } } 在这个示例中 upstream 块定义了一个名为backend的组，包含三个后端服务器。最后一个标记为backup，这意味着只有在所有非备份服务器都无法使用时才会尝试联系它。 在location块内，proxy_pass指令指定了要使用的上游服务器组。 proxy_next_upstream指令设置了在遇到错误、超时或收到特定HTTP状态码（500, 502, 503, 504）时尝试下一个上游服务器。 proxy_next_upstream_timeout设定了尝试下一个上游服务器的最大等待时间为60秒。 proxy_next_upstream_tries限制了对每个请求最多尝试3次上游服务器。 通过这种方式配置，Nginx可以在面对网络波动或其他临时性故障时更加健壮，确保用户请求尽可能得到成功处理。这对于构建高可用性的Web应用至关重要。 主动重试机制 主动健康检查使用tengine模块 tengine版本(已应用到淘宝网上了) https://github.com/yaoweibin/nginx_upstream_check_module nginx 商业版插件 https://nginx.org/en/docs/http/ngx_http_upstream_hc_module.html ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:5:19","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"参考 https://nginx.org/en/docs/http/ngx_http_core_module.html#http Nginx Config 可以快速方便获得nginx的配置 ","date":"2022-01-13","objectID":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/:6:0","tags":["Nginx"],"title":"Nginx 常用配置","uri":"/nginx%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE/"},{"categories":["Nginx"],"content":"日常工作中的奇淫技巧 ","date":"2022-01-11","objectID":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:1:0","tags":["Nginx"],"title":"Nginx 日常工作常用脚本","uri":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Nginx"],"content":"日志切割脚本 #!/bin/bash #设置你的日志存放的目录 log_files_path=\"/mnt/usr/logs/\" #日志以年/月的目录形式存放 log_files_dir=${log_files_path}\"backup/\" #设置需要进行日志分割的日志文件名称，多个以空格隔开 log_files_name=(access.log error.log) #设置nginx的安装路径 nginx_sbin=\"/mnt/usr/sbin/nginx -c /mnt/usr/conf/nginx.conf\" #Set how long you want to save save_days=10 ############################################ #Please do not modify the following script # ############################################ mkdir -p $log_files_dir log_files_num=${#log_files_name[@]} #cut nginx log files for((i=0;i\u003c$log_files_num;i++));do mv ${log_files_path}${log_files_name[i]} ${log_files_dir}${log_files_name[i]}_$(date -d \"yesterday\" +\"%Y%m%d\") done $nginx_sbin -s reload ","date":"2022-01-11","objectID":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:1:1","tags":["Nginx"],"title":"Nginx 日常工作常用脚本","uri":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Nginx"],"content":"图片放盗链 server { listen 80; server_name *.phpblog.com.cn; # 图片防盗链 location ~* \\.(gif|jpg|jpeg|png|bmp|swf)$ { valid_referers none blocked server_names ~\\.google\\. ~\\.baidu\\. *.qq.com; if ($invalid_referer){ return 403; } } } ","date":"2022-01-11","objectID":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:1:2","tags":["Nginx"],"title":"Nginx 日常工作常用脚本","uri":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Nginx"],"content":"nginx 访问控制 location ~ \\.php$ { allow 127.0.0.1; #只允许127.0.0.1的访问，其他均拒绝 deny all; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } ","date":"2022-01-11","objectID":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:1:3","tags":["Nginx"],"title":"Nginx 日常工作常用脚本","uri":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Nginx"],"content":"丢弃不受支持的文件扩展名的请求 location ~ \\.(js|css|sql)$ { deny all; } ","date":"2022-01-11","objectID":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:1:4","tags":["Nginx"],"title":"Nginx 日常工作常用脚本","uri":"/nginx%E6%97%A5%E5%B8%B8%E5%B7%A5%E4%BD%9C%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["hugo"],"content":"test test LoveIt提供了admonition shortcode，支持 12 种样式，可以在页面中插入提示的横幅。代码如下 ","date":"2021-09-15","objectID":"/first_post/:1:0","tags":["hugo"],"title":"First_post","uri":"/first_post/"},{"categories":["hugo"],"content":"主题自带的admonition样式 注意\r一个 注意 横幅\r这是一个默认不展开的横幅\r一个 注意 横幅\r摘要\r一个 摘要 横幅\r信息\r一个 信息 横幅\r技巧\r一个 技巧 横幅\r成功\r一个 成功 横幅\r问题\r一个 问题 横幅\r警告\r一个 警告 横幅\r失败\r一个 失败 横幅\r危险\r一个 危险 横幅\rBug\r一个 Bug 横幅\r示例\r一个 示例 横幅\r引用\r一个 引用 横幅\r","date":"2021-09-15","objectID":"/first_post/:2:0","tags":["hugo"],"title":"First_post","uri":"/first_post/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 ContainerCreating 或 Waiting 状态 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 配置错误 检查是否打包了正确的镜像 检查配置了正确的容器参数 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"磁盘爆满 启动 Pod 会调 CRI 接口创建容器，容器运行时创建容器时通常会在数据目录下为新建的容器创建一些目录和文件，如果数据目录所在的磁盘空间满了就会创建失败并报错: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedCreatePodSandBox 2m (x4307 over 16h) kubelet, 10.179.80.31 (combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to create a sandbox for pod \"apigateway-6dc48bf8b6-l8xrw\": Error response from daemon: mkdir /var/lib/docker/aufs/mnt/1f09d6c1c9f24e8daaea5bf33a4230de7dbc758e3b22785e8ee21e3e3d921214-init: no space left on device ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"limit 设置太小或者单位不对 如果 limit 设置过小以至于不足以成功运行 Sandbox 也会造成这种状态，常见的是因为 memory limit 单位设置不对造成的 limit 过小，比如误将 memory 的 limit 单位像 request 一样设置为小 m，这个单位在 memory 不适用，会被 k8s 识别成 byte， 应该用 Mi 或 M。， 举个例子: 如果 memory limit 设为 1024m 表示限制 1.024 Byte，这么小的内存， pause 容器一起来就会被 cgroup-oom kill 掉，导致 pod 状态一直处于 ContainerCreating。 这种情况通常会报下面的 event: Pod sandbox changed, it will be killed and re-created。 kubelet报错 to start sandbox container for pod ... Error response from daemon: OCI runtime create failed: container_linux.go:348: starting container process caused \"process_linux.go:301: running exec setns process for init caused \\\"signal: killed\\\"\": unknown ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"拉取镜像失败 镜像拉取失败也分很多情况，这里列举下: 配置了错误的镜像 Kubelet 无法访问镜像仓库（比如默认 pause 镜像在 gcr.io 上，国内环境访问需要特殊处理） 拉取私有镜像的 imagePullSecret 没有配置或配置有误 镜像太大，拉取超时（可以适当调整 kubelet 的 —image-pull-progress-deadline 和 —runtime-request-timeout 选项） ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"controller-manager 异常 查看 master 上 kube-controller-manager 状态，异常的话尝试重启。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:1:5","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Error 状态 通常处于 Error 状态说明 Pod 启动过程中发生了错误。常见的原因包括： 依赖的 ConfigMap、Secret 或者 PV 等不存在 请求的资源超过了管理员设置的限制，比如超过了 LimitRange 等 违反集群的安全策略 容器无权操作集群内的资源，比如开启 RBAC 后，需要为 ServiceAccount 配置角色绑定 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:2:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 ImagePullBackOff 状态 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"http 类型 registry，地址未加入到 insecure-registry dockerd 默认从 https 类型的 registry 拉取镜像，如果使用 https 类型的 registry，则必须将它添加到 insecure-registry 参数中，然后重启或 reload dockerd 生效。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"https 自签发类型 resitry，没有给节点添加 ca 证书 如果 registry 是 https 类型，但证书是自签发的，dockerd 会校验 registry 的证书，校验成功才能正常使用镜像仓库， 要想校验成功就需要将 registry 的 ca 证书放置到 /etc/docker/certs.d/\u003cregistry:port\u003e/ca.crt 位置。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"私有镜像仓库认证失败 如果 registry 需要认证，但是 Pod 没有配置 imagePullSecret，配置的 Secret 不存在或者有误都会认证失败。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"镜像文件损坏 如果 push 的镜像文件损坏了，下载下来也用不了，需要重新 push 镜像文件 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"镜像拉取超时 如果节点上新起的 Pod 太多就会有许多可能会造成容器镜像下载排队，如果前面有许多大镜像需要下载很长时间，后面排队的 Pod 就会报拉取超时。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:5","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"镜像不存在 kubelet 日志： PullImage \"imroc/test:v0.2\" from image service failed: rpc error: code = Unknown desc = Error response from daemon: manifest for imroc/test:v0.2 not found ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:3:6","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Pending 状态 Pending 状态说明 Pod 还没有被调度到某个节点上，需要看下 Pod 事件进一步判断原因，比如: $ kubectl describe pod tikv-0 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 3m (x106 over 33m) default-scheduler 0/4 nodes are available: 1 node(s) had no available volume zone, 2 Insufficient cpu, 3 Insufficient memory. 下面列举下可能原因和解决方法。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"节点资源不够 节点资源不够有以下几种情况: CPU 负载过高 剩余可以被分配的内存不够 如果判断某个 Node 资源是否足够？ 通过 kubectl describe node 查看 node 资源情况，关注以下信息： Allocatable: 表示此节点能够申请的资源总和 Allocated resources: 表示此节点已分配的资源 (Allocatable 减去节点上所有 Pod 总的 Request) $ sudo kubectl describe node 10.10.192.220|grep Allo -A 6 Allocatable: cpu: 8 ephemeral-storage: 33806352329 hugepages-2Mi: 0 memory: 24543516Ki pods: 110 System Info: -- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 3650m (45%) 50150m (626%) memory 5092Mi (21%) 81290Mi (339%) ephemeral-storage 0 (0%) 0 (0%) 可以看到能够申请的资源总和，当前节点可以创建110个pods，cpu 8核，cpu requests占比45% 前者与后者相减，可得出剩余可申请的资源。如果这个值小于 Pod 的 request，就不满足 Pod 的资源要求，Scheduler 在 Predicates (预选) 阶段就会剔除掉这个 Node，也就不会调度上去 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"不满足 nodeSelector 与 affinity 如果 Pod 包含 nodeSelector 指定了节点需要包含的 label，调度器将只会考虑将 Pod 调度到包含这些 label 的 Node 上，如果没有 Node 有这些 label 或者有这些 label 的 Node 其它条件不满足也将会无法调度。参考官方文档： https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/assign-pod-node/ 如果 Pod 包含 affinity（亲和性）的配置，调度器根据调度算法也可能算出没有满足条件的 Node，从而无法调度。affinity 有以下几类: nodeAffinity: 节点亲和性，可以看成是增强版的 nodeSelector，用于限制 Pod 只允许被调度到某一部分 Node。 podAffinity: Pod 亲和性，用于将一些有关联的 Pod 调度到同一个地方，同一个地方可以是指同一个节点或同一个可用区的节点等。 podAntiAffinity: Pod 反亲和性，用于避免将某一类 Pod 调度到同一个地方避免单点故障，比如将集群 DNS 服务的 Pod 副本都调度到不同节点，避免一个节点挂了造成整个集群 DNS 解析失败，使得业务中断。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Node 存在 Pod 没有容忍的污点 如果节点上存在污点 (Taints)，而 Pod 没有响应的容忍 (Tolerations)，Pod 也将不会调度上去。通过 describe node 可以看下 Node 有哪些 Taints: $ kubectl describe nodes host1 ... Taints: special=true:NoSchedule ... 污点既可以是手动添加也可以是被自动添加 手动添加污点 $ kubectl taint node host1 special=true:NoSchedule node \"host1\" tainted 另外，有些场景下希望新加的节点默认不调度 Pod，直到调整完节点上某些配置才允许调度，就给新加的节点都加上 node.kubernetes.io/unschedulable 这个污点 自动添加污点 如果节点运行状态不正常，污点也可以被自动添加，从 v1.12 开始，TaintNodesByCondition 特性进入 Beta 默认开启，controller manager 会检查 Node 的 Condition，如果命中条件就自动为 Node 加上相应的污点，这些 Condition 与 Taints 的对应关系如下: Conditon Value Taints -------- ----- ------ OutOfDisk True node.kubernetes.io/out-of-disk Ready False node.kubernetes.io/not-ready Ready Unknown node.kubernetes.io/unreachable MemoryPressure True node.kubernetes.io/memory-pressure PIDPressure True node.kubernetes.io/pid-pressure DiskPressure True node.kubernetes.io/disk-pressure NetworkUnavailable True node.kubernetes.io/network-unavailable 解释下上面各种条件的意思: OutOfDisk 为 True 表示节点磁盘空间不够了 Ready 为 False 表示节点不健康 Ready 为 Unknown 表示节点失联，在 node-monitor-grace-period 这么长的时间内没有上报状态 controller-manager 就会将 Node 状态置为 Unknown (默认 40s) MemoryPressure 为 True 表示节点内存压力大，实际可用内存很少 PIDPressure 为 True 表示节点上运行了太多进程，PID 数量不够用了 DiskPressure 为 True 表示节点上的磁盘可用空间太少了 NetworkUnavailable 为 True 表示节点上的网络没有正确配置，无法跟其它 Pod 正常通信 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"kube-scheduler 没有正常运行 检查 maser 上的 kube-scheduler 是否运行正常，异常的话可以尝试重启临时恢复。 $ sudo kubectl -n kube-system get pod|grep kube-scheduler kube-scheduler-10.10.192.220 1/1 Running 183 (40d ago) 424d ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:4:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Terminating 状态 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"磁盘爆满 如果 docker 的数据目录所在磁盘被写满，docker 无法正常运行，无法进行删除和创建操作，所以 kubelet 调用 docker 删除容器没反应，看 event 类似这样： Normal Killing 39s (x735 over 15h) kubelet, 10.179.80.31 Killing container with id docker://apigateway:Need to kill Pod ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"存在 “i” 文件属性 如果容器的镜像本身或者容器启动后写入的文件存在 “i” 文件属性，此文件就无法被修改删除，而删除 Pod 时会清理容器目录，但里面包含有不可删除的文件，就一直删不了，Pod 状态也将一直保持 Terminating，kubelet 报错: Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.922965 14109 remote_runtime.go:250] RemoveContainer \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\" from runtime service failed: rpc error: code = Unknown desc = failed to remove container \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\": Error response from daemon: container 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver \"overlay2\" failed to remove root filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868/diff/usr/bin/bash: operation not permitted Sep 27 14:37:21 VM_0_7_centos kubelet[14109]: E0927 14:37:21.923027 14109 kuberuntime_gc.go:126] Failed to remove container \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\": rpc error: code = Unknown desc = failed to remove container \"19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257\": Error response from daemon: container 19d837c77a3c294052a99ff9347c520bc8acb7b8b9a9dc9fab281fc09df38257: driver \"overlay2\" failed to remove root filesystem: remove /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868/diff/usr/bin/bash: operation not permitted 通过 man chattr 查看 “i” 文件属性描述: A file with the 'i' attribute cannot be modified: it cannot be deleted or renamed, no link can be created to this file and no data can be written to the file. Only the superuser or a process possessing the CAP_LINUX_IMMUTABLE capability can set or clear this attribute. 彻底解决当然是不要在容器镜像中或启动后的容器设置 “i” 文件属性，临时恢复方法： 复制 kubelet 日志报错提示的文件路径，然后执行 chattr -i : chattr -i /data/docker/overlay2/b1aea29c590aa9abda79f7cf3976422073fb3652757f0391db88534027546868/diff/usr/bin/bash 执行完后等待 kubelet 自动重试，Pod 就可以被自动删除了。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"docker 17 的 bug docker hang 住，没有任何响应，看 event: Warning FailedSync 3m (x408 over 1h) kubelet, 10.179.80.31 error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded 怀疑是17版本dockerd的BUG。可通过 kubectl -n cn-staging delete pod apigateway-6dc48bf8b6-clcwk –force –grace-period=0 强制删除pod，但 docker ps 仍看得到这个容器 处置建议： 升级到docker 18. 该版本使用了新的 containerd，针对很多bug进行了修复。 如果出现terminating状态的话，可以提供让容器专家进行排查，不建议直接强行删除，会可能导致一些业务上问题。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"存在 Finalizers k8s 资源的 metadata 里如果存在 finalizers，那么该资源一般是由某程序创建的，并且在其创建的资源的 metadata 里的 finalizers 加了一个它的标识，这意味着这个资源被删除时需要由创建资源的程序来做删除前的清理，清理完了它需要将标识从该资源的 finalizers 中移除，然后才会最终彻底删除资源。比如 Rancher 创建的一些资源就会写入 finalizers 标识。 处理建议：kubectl edit 手动编辑资源定义，删掉 finalizers，这时再看下资源，就会发现已经删掉了。通过prometheus-operator 创建prometheus时，最后无法删除namespace 也是此原因。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:5:4","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 一直处于 Unknown 状态 通常是节点失联，没有上报状态给 apiserver，到达阀值后 controller-manager 认为节点失联并将其状态置为 Unknown。 可能原因: 节点高负载导致无法上报 节点宕机 节点被关机 网络不通 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:6:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 健康检查失败 Kubernetes 健康检查包含就绪检查(readinessProbe)和存活检查(livenessProbe) pod 如果就绪检查失败会将此 pod ip 从 service 中摘除，通过 service 访问，流量将不会被转发给就绪检查失败的 pod pod 如果存活检查失败，kubelet 将会杀死容器并尝试重启 健康检查失败的可能原因有多种，除了业务程序BUG导致不能响应健康检查导致 unhealthy，还能有有其它原因，下面我们来逐个排查。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"健康检查配置不合理 initialDelaySeconds 太短，容器启动慢，导致容器还没完全启动就开始探测，如果 successThreshold 是默认值 1，检查失败一次就会被 kill，然后 pod 一直这样被 kill 重启。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"节点负载高 cpu 占用高（比如跑满）会导致进程无法正常发包收包，通常会 timeout，导致 kubelet 认为 pod 不健康。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"容器内进程端口监听挂掉 使用 netstat -tunlp 检查端口监听是否还在，如果不在了，抓包可以看到会直接 reset 掉健康检查探测的连接: 20:15:17.890996 IP 172.16.2.1.38074 \u003e 172.16.2.23.8888: Flags [S], seq 96880261, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0 20:15:17.891021 IP 172.16.2.23.8888 \u003e 172.16.2.1.38074: Flags [R.], seq 0, ack 96880262, win 0, length 0 20:15:17.906744 IP 10.0.0.16.54132 \u003e 172.16.2.23.8888: Flags [S], seq 1207014342, win 14600, options [mss 1424,nop,nop,sackOK,nop,wscale 7], length 0 20:15:17.906766 IP 172.16.2.23.8888 \u003e 10.0.0.16.54132: Flags [R.], seq 0, ack 1207014343, win 0, length 0 连接异常，从而健康检查失败。发生这种情况的原因可能在一个节点上启动了多个使用 hostNetwork 监听相同宿主机端口的 Pod，只会有一个 Pod 监听成功，但监听失败的 Pod 的业务逻辑允许了监听失败，并没有退出，Pod 又配了健康检查，kubelet 就会给 Pod 发送健康检查探测报文，但 Pod 由于没有监听所以就会健康检查失败。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:7:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod 处于 CrashLoopBackOff 状态 Pod 如果处于 CrashLoopBackOff 状态说明之前是启动了，只是又异常退出了，应用实例状态为CrashLoopBackOff， 表现为实例不断重启，由ready状态变为Complete再变成CrashLoopBackOff。 一般由于应用实例容器异常退出导致的实例异常，需要排查应用日志确定异常退出原因。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:0","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"容器进程主动退出 如果是容器进程主动退出，退出状态码一般在 0-128 之间，除了可能是业务程序 BUG，还有其它许多可能原因 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:1","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"系统OOM 如果发生系统 OOM，可以看到 Pod 中容器退出状态码是 137，表示被 SIGKILL 信号杀死，同时内核会报错: Out of memory: Kill process …。大概率是节点上部署了其它非 K8S 管理的进程消耗了比较多的内存，或者 kubelet 的 –kube-reserved 和 –system-reserved 配的比较小，没有预留足够的空间给其它非容器进程，节点上所有 Pod 的实际内存占用总量不会超过 /sys/fs/cgroup/memory/kubepods 这里 cgroup 的限制，这个限制等于 capacity - “kube-reserved” - “system-reserved”，如果预留空间设置合理，节点上其它非容器进程（kubelet, dockerd, kube-proxy, sshd 等) 内存占用没有超过 kubelet 配置的预留空间是不会发生系统 OOM 的，可以根据实际需求做合理的调整。 ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:2","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"cgroup OOM 如果是 cgroup OOM 杀掉的进程，从 Pod 事件的下 Reason 可以看到是 OOMKilled，说明容器实际占用的内存超过 limit 了，同时内核日志会报: ``。 可以根据需求调整下 limit ","date":"2021-07-28","objectID":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/:8:3","tags":["Kubernetes排障"],"title":"Kubernetes Pod 排障指南","uri":"/kubernetes-pod%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/"},{"categories":["Kubernetes"],"content":"Pod调度 一般情况下我们部署的 Pod 是通过集群的自动调度策略来选择节点的，默认情况下调度器考虑的是资源足够，并且负载尽量平均，但是有的时候我们需要能够更加细粒度的去控制 Pod 的调度，比如我们希望一些机器学习的应用只跑在有 GPU 的节点上；但是有的时候我们的服务之间交流比较频繁，又希望能够将这服务的 Pod 都调度到同一个的节点上。这就需要使用一些调度方式来控制 Pod 的调度了，主要有两个概念：亲和性和反亲和性，亲和性又分成节点亲和性(nodeAffinity)和 Pod 亲和性(podAffinity)。 ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:0:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["Kubernetes"],"content":"nodeSelector 在了解亲和性之前，我们先来了解一个非常常用的调度方式：nodeSelector。我们知道 label 标签是 kubernetes 中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，比如最常见的 Service 对象通过 label 去匹配 Pod 资源，而 Pod 的调度也可以根据节点的 label 来进行调度。 我们可以通过下面的命令查看我们的 node 的 label： $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS master1 Ready control-plane,master 82d v1.22.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= node1 Ready \u003cnone\u003e 82d v1.22.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux node2 Ready \u003cnone\u003e 82d v1.22.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux 现在我们先给节点 node2 增加一个com=schedulernode的标签，命令如下： $ kubectl label nodes node2 com=schedulernode node/node2 labeled 我们可以通过上面的 –show-labels 参数可以查看上述标签是否生效。当节点被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在 Pod 的 spec 字段中添加 nodeSelector 字段，里面是我们需要被调度的节点的 label 标签，比如，下面的 Pod 我们要强制调度到 node2 这个节点上去，我们就可以使用 nodeSelector 来表示了： # node-selector-demo.yaml apiVersion: v1 kind: Pod metadata: labels: app: busybox-pod name: test-busybox spec: containers: - command: - sleep - \"3600\" image: busybox imagePullPolicy: Always name: test-busybox nodeSelector: com: schedulernode 然后我们可以通过 describe 命令查看调度结果： $ kubectl apply -f pod-selector-demo.yaml pod/test-busybox created $ kubectl describe pod test-busybox Name: test-busybox Namespace: default Priority: 0 Node: node2/192.168.31.46 ...... Node-Selectors: com=schedulernode Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u003cunknown\u003e default-scheduler Successfully assigned default/test-busybox to node2 Normal Pulling 13s kubelet, node2 Pulling image \"busybox\" Normal Pulled 10s kubelet, node2 Successfully pulled image \"busybox\" Normal Created 10s kubelet, node2 Created container test-busybox Normal Started 9s kubelet, node2 Started container test-busybox 我们可以看到 Events 下面的信息，我们的 Pod 通过默认的 default-scheduler 调度器被绑定到了 node2 节点。不过需要注意的是nodeSelector 属于强制性的，如果我们的目标节点没有可用的资源，我们的 Pod 就会一直处于 Pending 状态。 通过上面的例子我们可以感受到 nodeSelector 的方式比较直观，但是还够灵活，控制粒度偏大，接下来我们再和大家了解下更加灵活的方式：节点亲和性(nodeAffinity)。 ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:1:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["Kubernetes"],"content":"亲和性和反亲和性调度 前面我们了解了 kubernetes 调度器的调度流程，我们知道默认的调度器在使用的时候，经过了 predicates 和 priorities 两个阶段，但是在实际的生产环境中，往往我们需要根据自己的一些实际需求来控制 Pod 的调度，这就需要用到 nodeAffinity(节点亲和性)、podAffinity(pod 亲和性) 以及 podAntiAffinity(pod 反亲和性)。 亲和性调度可以分成软策略和硬策略两种方式: 软策略就是如果现在没有满足调度要求的节点的话，Pod 就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有的话也无所谓 硬策略就比较强硬了，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然就不干了 对于亲和性和反亲和性都有这两种规则可以设置： preferredDuringSchedulingIgnoredDuringExecution 和requiredDuringSchedulingIgnoredDuringExecution，前面的就是软策略，后面的就是硬策略 ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:2:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["Kubernetes"],"content":"节点亲和性 节点亲和性（nodeAffinity）主要是用来控制 Pod 要部署在哪些节点上，以及不能部署在哪些节点上的，它可以进行一些简单的逻辑组合了，不只是简单的相等匹配。 比如现在我们用一个 Deployment 来管理8个 Pod 副本，现在我们来控制下这些 Pod 的调度，如下例子： # node-affinity-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: node-affinity labels: app: node-affinity spec: replicas: 8 selector: matchLabels: app: node-affinity template: metadata: labels: app: node-affinity spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 name: nginxweb affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: NotIn values: - master1 preferredDuringSchedulingIgnoredDuringExecution: # 软策略 - weight: 1 preference: matchExpressions: - key: com operator: In values: - schedulernode 上面这个 Pod 首先是要求不能运行在 master1 这个节点上，如果有个节点满足 com=schedulernode 的话就优先调度到这个节点上。 由于上面 node02 节点我们打上了 com=schedulernode 这样的 label 标签，所以按要求会优先调度到这个节点来的，现在我们来创建这个 Pod，然后查看具体的调度情况是否满足我们的要求。 $ kubectl apply -f node-affinty-demo.yaml deployment.apps/node-affinity created $ kubectl get pods -l app=node-affinity -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES node-affinity-cdd9d54d9-bgbbh 1/1 Running 0 2m28s 10.244.2.247 node2 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-dlbck 1/1 Running 0 2m28s 10.244.4.16 node1 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-g2jr6 1/1 Running 0 2m28s 10.244.4.17 node1 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-gzr58 1/1 Running 0 2m28s 10.244.1.118 node1 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-hcv7r 1/1 Running 0 2m28s 10.244.2.246 node2 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-kvxw4 1/1 Running 0 2m28s 10.244.2.245 node2 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-p4mmk 1/1 Running 0 2m28s 10.244.2.244 node2 \u003cnone\u003e \u003cnone\u003e node-affinity-cdd9d54d9-t5mff 1/1 Running 0 2m28s 10.244.1.117 node2 \u003cnone\u003e \u003cnone\u003e 从结果可以看出有5个 Pod 被部署到了 node2 节点上，但是可以看到并没有一个 Pod 被部署到 master1 这个节点上，因为我们的硬策略就是不允许部署到该节点上，而 node2 是软策略，所以会尽量满足。这里的匹配逻辑是 label 标签的值在某个列表中，现在 Kubernetes 提供的操作符有下面的几种： In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在 但是需要注意的是如果 nodeSelectorTerms 下面有多个选项的话，满足任何一个条件就可以了；如果 matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod。 ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:3:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["Kubernetes"],"content":"Pod亲和性 Pod 亲和性（podAffinity）主要解决 Pod 可以和哪些 Pod 部署在同一个拓扑域中的问题（其中拓扑域用主机标签实现，可以是单个主机，也可以是多个主机组成的 cluster、zone 等等），而 Pod 反亲和性主要是解决 Pod 不能和哪些 Pod 部署在同一个拓扑域中的问题，它们都是处理的 Pod 与 Pod 之间的关系，比如一个 Pod 在一个节点上了，那么我这个也得在这个节点，或者你这个 Pod 在节点上了，那么我就不想和你待在同一个节点上。 由于我们这里只有一个集群，并没有区域或者机房的概念，所以我们这里直接使用主机名来作为拓扑域，把 Pod 创建在同一个主机上面。 $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS master1 Ready control-plane,master 82d v1.22.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=master1,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers= node1 Ready \u003cnone\u003e 82d v1.22.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node1,kubernetes.io/os=linux node2 Ready \u003cnone\u003e 82d v1.22.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=node2,kubernetes.io/os=linux,com=schedulernode 同样，还是针对上面的资源对象，我们来测试下 Pod 的亲和性： # pod-affinity-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pod-affinity labels: app: pod-affinity spec: replicas: 3 selector: matchLabels: app: pod-affinity template: metadata: labels: app: pod-affinity spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: nginxweb affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 - labelSelector: matchExpressions: - key: app operator: In values: - busybox-pod topologyKey: kubernetes.io/hostname 上面这个例子中的 Pod 需要调度到某个指定的节点上，并且该节点上运行了一个带有 app=busybox-pod 标签的 Pod。我们可以查看有标签 app=busybox-pod 的 pod 列表： $ kubectl get pods -l app=busybox-pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-busybox 1/1 Running 0 27m 10.244.2.242 node2 \u003cnone\u003e \u003cnone\u003e 我们看到这个 Pod 运行在了 node2 的节点上面，所以按照上面的亲和性来说，上面我们部署的3个 Pod 副本也应该运行在 node2 节点上： $ kubectl apply -f pod-affinity-demo.yaml deployment.apps/pod-affinity created $ kubectl get pods -o wide -l app=pod-affinity NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-affinity-587f9b5b58-5nxmf 1/1 Running 0 26s 10.244.2.249 node2 \u003cnone\u003e \u003cnone\u003e pod-affinity-587f9b5b58-m2j7s 1/1 Running 0 26s 10.244.2.248 node2 \u003cnone\u003e \u003cnone\u003e pod-affinity-587f9b5b58-vrd7b 1/1 Running 0 26s 10.244.2.250 node2 \u003cnone\u003e \u003cnone\u003e 如果我们把上面的 test-busybox 和 pod-affinity 这个 Deployment 都删除，然后重新创建 pod-affinity 这个资源，看看能不能正常调度呢： $ kubectl delete -f node-selector-demo.yaml pod \"test-busybox\" deleted $ kubectl delete -f pod-affinity-demo.yaml deployment.apps \"pod-affinity\" deleted $ kubectl apply -f pod-affinity-demo.yaml deployment.apps/pod-affinity created $ kubectl get pods -o wide -l app=pod-affinity NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-affinity-587f9b5b58-bbfgr 0/1 Pending 0 18s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e pod-affinity-587f9b5b58-lwc8n 0/1 Pending 0 18s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e pod-affinity-587f9b5b58-pc7ql 0/1 Pending 0 18s \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e \u003cnone\u003e 我们可以看到都处于 Pending 状态了，这是因为现在没有一个节点上面拥有 app=busybox-pod 这个标签的 Pod，而上面我们的调度使用的是硬策略，所以就没办法进行调度了，大家可以去尝试下重新将 test-busybox 这个 Pod 调度到其他节点上，观察下上面的3个副本会不会也被调度到对应的节点上去。 我们这个地方使用的是 kubernetes.io/hostname 这个拓扑域，意思就是我们当前调度的 Pod 要和目标的 Pod 处于同一个主机上面，因为要处于同一个拓扑域下面，为了说明这个问题，我们把拓扑域改成 beta.kubernetes.io/os，同样的我们当前调度的 Pod 要和目标的 Pod 处于同一个拓扑域中，目标的 Pod 是拥有 beta.kubernetes.io/os=linux 的标签，而我们这里所有节点都有这样的标签，这也就意味着我们所有节点都在同一个拓扑域中，所以我们这里的 Pod 可以被调度到任何一个节点，重新运行上面的 app=busybox-pod 的 Pod，然后再更新下我们这里的资源对象： $ kubectl get pods -o wide -l app=pod-affinity NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-affinity-76c56567c-792n4 1/1 Running 0 2m59s 10.244.2.254 node2 \u003cnone\u003e \u003cnone\u003e pod-affinity-76c56567c-8s2pd 1/1 Running 0 3m53s 10.244.4.18 node1 \u003cnone\u003e \u003cnone\u003e pod-affinity-76c56567c-hx7ck 1/1 Running 0 2m52s 10.244.3.23 node2 \u003cnone\u003e \u003cnone\u003e 可以看到现在是分别运行在2个节点下面的，因为他们都属于 beta.kubernetes.io/os 这个拓扑域。 ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:4:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["Kubernetes"],"content":"Pod反亲和性 Pod 反亲和性（podAntiAffinity）则是反着来的，比如一个节点上运行了某个 Pod，那么我们的模板 Pod 则不希望被调度到这个节点上面去了。我们把上面的 podAffinity 直接改成 podAntiAffinity： # pod-antiaffinity-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: pod-antiaffinity labels: app: pod-antiaffinity spec: replicas: 3 selector: matchLabels: app: pod-antiaffinity template: metadata: labels: app: pod-antiaffinity spec: containers: - name: nginx image: nginx ports: - containerPort: 80 name: nginxweb affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: # 硬策略 - labelSelector: matchExpressions: - key: app operator: In values: - busybox-pod topologyKey: kubernetes.io/hostname 这里的意思就是如果一个节点上面有一个 app=busybox-pod 这样的 Pod 的话，那么我们的 Pod 就别调度到这个节点上面来，上面我们把app=busybox-pod 这个 Pod 固定到了 node2 这个节点上面的，所以正常来说我们这里的 Pod 不会出现在该节点上： $ kubectl apply -f pod-antiaffinity-demo.yaml deployment.apps/pod-antiaffinity created $ kubectl get pods -l app=pod-antiaffinity -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-antiaffinity-84d5bf9df4-9c9qk 1/1 Running 0 73s 10.244.4.19 node1 \u003cnone\u003e \u003cnone\u003e pod-antiaffinity-84d5bf9df4-q6lkm 1/1 Running 0 67s 10.244.3.24 node1 \u003cnone\u003e \u003cnone\u003e pod-antiaffinity-84d5bf9df4-vk9tc 1/1 Running 0 57s 10.244.3.25 node1 \u003cnone\u003e \u003cnone\u003e 我们可以看到没有被调度到 node2 节点上，因为我们这里使用的是 Pod 反亲和性。大家可以思考下，如果这里我们将拓扑域更改成 beta.kubernetes.io/os 会怎么样呢？可以自己去测试下看看。 ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:5:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["Kubernetes"],"content":"污点和容忍 对于 nodeAffinity 无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而污点（Taints）恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度 Pod。 比如用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。我们使用 kubeadm 搭建的集群默认就给 master 节点添加了一个污点标记，所以我们看到我们平时的 Pod 都没有被调度到 master 上去： $ kubectl describe node master1 Name: master1 Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=master1 kubernetes.io/os=linux node-role.kubernetes.io/master= ...... Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false ...... 我们可以使用上面的命令查看 master 节点的信息，其中有一条关于 Taints 的信息：node-role.kubernetes.io/master:NoSchedule，就表示master 节点打了一个污点的标记，其中影响的参数是 NoSchedule，表示 Pod 不会被调度到标记为 taints 的节点，除了 NoSchedule 外，还有另外两个选项： PreferNoSchedule：NoSchedule 的软策略版本，表示尽量不调度到污点节点上去 NoExecute：该选项意味着一旦 Taint 生效，如该节点内正在运行的 Pod 没有对应容忍（Tolerate）设置，则会直接被逐出污点 相比较NoExecute策略，NoSchedule，只会影响新的 Pod 调度，不会将正在运行的Pod进行驱逐 taint 标记节点的命令如下： $ kubectl taint nodes node2 test=node2:NoSchedule node \"node2\" tainted 上面的命名将 node2 节点标记为了污点，影响策略是 NoSchedule，只会影响新的 Pod 调度，如果仍然希望某个 Pod 调度到 taint 节点上，则必须在 Spec 中做出 Toleration 定义，才能调度到该节点，比如现在我们想要将一个 Pod 调度到 master 节点： # taint-demo.yaml apiVersion: apps/v1 kind: Deployment metadata: name: taint labels: app: taint spec: replicas: 3 selector: matchLabels: app: taint template: metadata: labels: app: taint spec: containers: - name: nginx image: nginx ports: - name: http containerPort: 80 tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" 由于 master 节点被标记为了污点，所以我们这里要想 Pod 能够调度到改节点去，就需要增加容忍的声明： tolerations: - key: \"node-role.kubernetes.io/master\" operator: \"Exists\" effect: \"NoSchedule\" 然后创建上面的资源，查看结果： $ kubectl apply -f taint-demo.yaml deployment.apps \"taint\" created $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... taint-845d8bb4fb-57mhm 1/1 Running 0 1m 10.244.4.247 node2 taint-845d8bb4fb-bbvmp 1/1 Running 0 1m 10.244.0.33 master1 taint-845d8bb4fb-zb78x 1/1 Running 0 1m 10.244.4.246 node2 ...... 我们可以看到有一个 Pod 副本被调度到了 master 节点，这就是容忍的使用方法。 对于 tolerations 属性的写法，其中的 key、value、effect 与 Node 的 Taint 设置需保持一致， 还有以下几点说明： 如果 operator 的值是 Exists，则 value 属性可省略 如果 operator 的值是 Equal，则表示其 key 与 value 之间的关系是 equal(等于) 如果不指定 operator 属性，则默认值为 Equal 另外，还有两个特殊值： 空的 key 如果再配合 Exists 就能匹配所有的 key 与 value，也就是是能容忍所有节点的所有 Taints 空的 effect 匹配所有的 effect 最后如果我们要取消节点的污点标记，可以使用下面的命令 $ kubectl taint nodes node2 test- node \"node2\" untainted ","date":"2021-07-16","objectID":"/pod%E8%B0%83%E5%BA%A6/:6:0","tags":["Kubernetes"],"title":"Pod调度","uri":"/pod%E8%B0%83%E5%BA%A6/"},{"categories":["linux"],"content":"awk 是 Linux/Unix 系统中的一个强大的文本处理工具,主要用于处理文本文件和字符串。 ","date":"2021-06-29","objectID":"/awk/:0:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"特性 awk 的一些关键特性: 可以根据字段进行操作,如打印指定列、求列的总和等 支持条件判断和循环语句,可以实现复杂的文本处理 使用正则表达式进行模式匹配和内容提取 内置字符串操作函数,如匹配、替换、截取等 ","date":"2021-06-29","objectID":"/awk/:1:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"适用场景 awk 适用于报表生成、格式转换、数学运算等场景。掌握了 awk,可以大大提高 Bash 脚本的文本处理能力 ","date":"2021-06-29","objectID":"/awk/:2:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"内置函数 内置函数 command 释义 gsub(r,s) 在整个$0中用s替代r 相当于 sed ’s///g' gsub(r,s,t) 在整个t中用s替代r index(s,t) 返回s中字符串t的第一位置 length(s) 返回s长度 match(s,r) 测试s是否包含匹配r的字符串 split(s,a,fs) 在fs上将s分成序列a sprint(fmt,exp) 返回经fmt格式化后的exp sub(r,s) 用$0中最左边最长的子串代替s 相当于 sed ’s///' substr(s,p) 返回字符串s中从p开始的后缀部分 substr(s,p,n) 返回字符串s中从p开始长度为n的后缀部分 ","date":"2021-06-29","objectID":"/awk/:3:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"awk 判断 awk ‘{print ($1\u003e$2)?“第一排”$1:“第二排”$2}’ # 条件判断 括号代表if语句判断 “?“代表then “:“代表else awk ‘{max=($1\u003e$2)? $1 : $2; print max}’ # 条件判断 如果$1大于$2,max值为为$1,否则为$2 awk ‘{if ( $6 \u003e 50) print $1 \" Too high” ;else print “Range is OK”}’ file awk ‘{if ( $6 \u003e 50) { count++;print $3 } else { x+5; print $2 } }’ file ","date":"2021-06-29","objectID":"/awk/:4:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"awk 循环 awk ‘{i = 1; while ( i \u003c= NF ) { print NF, $i ; i++ } }’ file awk ‘{ for ( i = 1; i \u003c= NF; i++ ) print NF,$i }’ file ","date":"2021-06-29","objectID":"/awk/:5:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"示例 awk ‘{print $1, $3}’ file.txt 每行按空格分割成多个字段,输出第1个和第3个字段 awk ‘{sum+=$1*$1} END {print sum}’ file.txt 求第一列的平方和 awk ’length \u003e 80’ file.txt 查找长度大于80的行 awk ‘/regex/ {print $0}’ log.txt 使用正则表达式匹配内容 awk ‘{print $NF}’ file.txt 打印最后一列 awk ‘{print $(NF-1) }’ file.txt 打印倒数第二列 awk ‘{OFS=”#”;$1=$1;print $0}’ file.txt OFS 输出记录分隔符 awk ‘{if(NR==3){print $0} else {print “不是第三行”}}’ file.txt 打印指定行 awk ‘NR==1||NR==3{print $0}’ test.txt 打印第一行或者第三行 awk ‘/Tom/’ file # 打印匹配到得行 awk ‘/^Tom/{print $1}’ # 匹配Tom开头的行 打印第一个字段 awk ‘$1 !~ /ly$/’ # 显示所有第一个字段不是以ly结尾的行 awk ‘$3 \u003c40’ # 如果第三个字段值小于40才打印 awk ‘$4==90{print $5}’ # 取出第四列等于90的第五列 awk ‘/^(no|so)/’ test # 打印所有以模式no或so开头的行 awk ‘$3 * $4 \u003e 500’ # 算术运算(第三个字段和第四个字段乘积大于500则显示) awk ‘{print NR” “$0}’ # 加行号 awk ‘/tom/,/suz/’ # 打印tom到suz之间的行 awk ‘{a+=$1}END{print a}’ # 列求和 awk ‘sum+=$1{print sum}’ # 将$1的值叠加后赋给sum awk ‘{a+=$1}END{print a/NR}’ # 列求平均值 awk ‘!s[$1 $3]++’ file # 根据第一列和第三列过滤重复行 awk -F’[ :\\t]’ ‘{print $1,$2}’ # 以空格、:、制表符Tab为分隔符 awk ‘{print “’\"$a”’”,\"’\"$b\"’\"}’ # 引用外部变量 awk ‘{if(NR==52){print;exit}}’ # 显示第52行 awk ‘/关键字/{a=NR+2}a==NR {print}’ # 取关键字下第几行 awk ‘gsub(/liu/,“aaaa”,$1){print $0}’ # 只打印匹配替换后的行 ll | awk -F’ +| +’ ‘/^$/{print $8}’ # 提取时间,空格不固定 awk ‘{$1=\"\";$2=\"\";$3=\"\";print}’ # 去掉前三列 echo aada:aba|awk ‘/d/||/b/{print}’ # 匹配两内容之一 echo aada:abaa|awk -F: ‘$1~/d/||$2~/b/{print}’ # 关键列匹配两内容之一 echo Ma asdas|awk ‘$1~/^[a-Z][a-Z]$/{print }’ # 第一个域匹配正则 echo aada:aaba|awk ‘/d/\u0026\u0026/b/{print}’ # 同时匹配两条件 awk ’length($1)==“4”{print $1}’ # 字符串位数 awk ‘{if($2\u003e3){system (“touch “$1)}}’ # 执行系统命令 awk ‘{sub(/Mac/,“Macintosh”,$0);print}’ # 用Macintosh替换Mac awk ‘{gsub(/Mac/,“MacIntosh”,$1); print}’ # 第一个域内用Macintosh替换Mac awk -F ’’ ‘{ for(i=1;i\u003cNF+1;i++)a+=$i ;print a}’ # 多位数算出其每位数的总和.比如 1234， 得到 10 awk ‘{ i=$1%10;if ( i == 0 ) {print i}}’ # 判断$1是否整除(awk中定义变量引用时不能带 $ ) awk ‘BEGIN{a=0}{if ($1\u003ea) a=$1 fi}END{print a}’ # 列求最大值 设定一个变量开始为0，遇到比该数大的值，就赋值给该变量，直到结束 awk ‘BEGIN{a=11111}{if ($1\u003ca) a=$1 fi}END{print a}’ # 求最小值 awk ‘{if(A)print;A=0}/regexp/{A=1}’ # 查找字符串并将匹配行的下一行显示出来，但并不显示匹配行 awk ‘/regexp/{print A}{A=$0}’ # 查找字符串并将匹配行的上一行显示出来，但并不显示匹配行 awk ‘{if(!/mysql/)gsub(/1/,“a”);print $0}’ # 将1替换成a，并且只在行中未出现字串mysql的情况下替换 awk ‘BEGIN{srand();fr=int(100*rand());print fr;}’ # 获取随机数 awk ‘{if(NR==3)F=1}{if(F){i++;if(i%7==1)print}}’ # 从第3行开始，每7行显示一次 awk ‘{if(NF\u003c1){print i;i=0} else {i++;print $0}}’ # 显示空行分割各段的行数 echo +null:null |awk -F: ‘$1!~\"^+”\u0026\u0026$2!=“null”{print $0}’ # 关键列同时匹配 awk -v RS=@ ‘NF{for(i=1;i\u003c=NF;i++)if($i) printf $i;print “”}’ # 指定记录分隔符 awk ‘{b[$1]=b[$1]$2}END{for(i in b){print i,b[i]}}’ # 列叠加 awk ‘{ i=($1%100);if ( $i \u003e= 0 ) {print $0,$i}}’ # 求余数 awk ‘{b=a;a=$1; if(NR\u003e1){print a-b}}’ # 当前行减上一行 awk ‘{a[NR]=$1}END{for (i=1;i\u003c=NR;i++){print a[i]-a[i-1]}}’ # 当前行减上一行 awk -F: ‘{name[x++]=$1};END{for(i=0;i\u003cNR;i++)print i,name[i]}’ # END只打印最后的结果,END块里面处理数组内容 awk ‘{sum2+=$2;count=count+1}END{print sum2,sum2/count}’ # $2的总和 $2总和除个数(平均值) awk -v a=0 -F ‘B’ ‘{for (i=1;i\u003cNF;i++){ a=a+length($i)+1;print a }}’ # 打印所以B的所在位置 awk ‘BEGIN{ “date” | getline d; split(d,mon) ; print mon[2]}’ file # 将date值赋给d，并将d设置为数组mon，打印mon数组中第2个元素 awk ‘BEGIN{info=“this is a test2010test!\";print substr(info,4,10);}’ # 截取字符串(substr使用) awk ‘BEGIN{info=“this is a test2010test!\";print index(info,“test”)?“ok”:“no found”;}’ # 匹配字符串(index使用) awk ‘BEGIN{info=“this is a test2010test!\";print match(info,/[0-9]+/)?“ok”:“no found”;}’ # 正则表达式匹配查找(match使用) awk ‘{for(i=1;i\u003c=4;i++)printf $i\"“FS; for(y=10;y\u003c=13;y++) printf $y\"“FS;print “”}’ # 打印前4列和后4列 awk ‘BEGIN{for(n=0;n++\u003c9;){for(i=0;i++\u003cn;)printf i\"x\"n”=“i*n” “;print “”}}’ # 乘法口诀 awk ‘BEGIN{info=“this is a test”;split(info,tA,” “);print length(tA);for(k in tA){print k,tA[k];}}’ # 字符串分割(split使用) awk ‘{if (system (“grep “$2” tmp/* \u003e /dev/null 2\u003e\u00261”) == 0 ) {print $1,“Y”} else {print $1,“N”} }’ a # 执行系统命令判断返回状态 awk ‘{for(i=1;i\u003c=NF;i++) a[i,NR]=$i}END{for(i=1;i\u003c=NF;i++) {for(j=1;j\u003c=NR;j++) printf a[i,j] \" “;print “”}}’ # 将多行转多列 常用示例 #删除temp文件的重复行 awk '!($0 in array) { array[$0]; print }' temp #查看最长使用的10个unix命令 awk '{print $1}' ~/.bash_","date":"2021-06-29","objectID":"/awk/:6:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"思维导图 awk\r","date":"2021-06-29","objectID":"/awk/:7:0","tags":["linux文本三剑客"],"title":"文本三剑客之 awk","uri":"/awk/"},{"categories":["linux"],"content":"文本处理三剑客之SED Stream EDitor, 流编辑器 sed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为\"模式空间\"（pattern space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。然后读入下行，执行下一个循环。如果没有使诸如‘D’的特殊命令，那会在两个循环之间清空模式空间，但不会清空保留空间。这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 功能：主要用来自动编辑一个或多个文件,简化对文件的反复操作,编写转换程序等 参考： http://www.gnu.org/software/sed/manual/sed.html 简单的说sed主要是用来编辑文件，那么可以从四个角度来学这个命令，增删查改 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:0:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"初体验 在行首添加一行 sed '1i\\AAA' aa.txt 在行尾添加一行 sed '$a\\AAA' aa.txt ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:1:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"基础 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"用法 用法： sed [option]... 'script' inputfile... 常用选项： -n 不输出模式空间内容到屏幕，即不自动打印 -e 多点编辑 -f /PATH/SCRIPT_FILE 从指定文件中读取编辑脚本 -r 支持使用扩展正则表达式 -i.bak 备份文件并原处编辑 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:1","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"地址定界 (1) 不给地址：对全文进行处理 (2) 单地址： #：指定的行，$：最后一行 /pattern/：被此处模式所能够匹配到的每一行 (3) 地址范围： #,# #,+# /pat1/,/pat2/ #,/pat1/ (4) ~：步进 1~2 奇数行 2~2 偶数行 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:2","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"编辑命令 命令 说明 d 删除模式空间匹配的行，并立即启用下一轮循环 p 打印当前模式空间内容，追加到默认输出之后 a [\\]text 在指定行后面追加文本，支持使用\\n实现多行追加 i [\\]text 在行前面插入文本 c [\\]text 替换行为单行或多行文本 w /path/file 保存模式匹配的行至指定文件 r /path/file 读取指定文件的文本至模式空间中匹配到的行后 = 为模式空间中的行打印行号 ! 模式空间中匹配行取反处理 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:3","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"查找替换 s/// 查找替换,支持使用其它分隔符，s@@@，s### 替换标记： g 行内全局替换 p 显示替换成功的行 w /PATH/FILE 将替换成功的行保存至文件中 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:4","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"示例 命令 介绍 cp /etc/passwd /tmp/passwd 样本文件 sed ‘2p’ /tmp/passwd 打印第二行 sed -n ‘2p’ /tmp/passwd 打印第二行静默输出第二行 sed -n ‘1,4p’ /tmp/passwd 静默输出一至四行 sed -n ‘/root/p’ /tmp/passwd 静默输出匹配包含root字符串的行 sed -n ‘2,/root/p’ /tmp/passwd 从2行开始，静默输出匹配包含root字符串的行 sed -n ‘/^$/=’ file 显示空行行号 sed -n -e ‘/^$/p’ -e ‘/^$/=’ file sed ‘/root/a\\superman’ /tmp/passwd 在匹配root字符串行后追加superman sed ‘/root/i\\superman’ /tmp/passwd 在匹配root字符串行前追加superman sed ‘/root/c\\superman’ /tmp/passwd 重写匹配root字符串的行 sed ‘/^$/d’ file 清空文件 sed ‘1,10d’ file 删除文件的1-10行 nl /tmp/passwd sed ‘2,5d’ nl /tmp/passwd sed ‘2a tea’ sed ’s/test/mytest/g’ example 文件替换 sed -n ’s/root/\u0026superman/p’ /tmp/passwd 在匹配的单词后添加内容 sed -n ’s/root/superman\u0026/p’ /tmp/passwd 在匹配的单词前添加内容 sed -e ’s/dog/cat/’ -e ’s/hi/lo/’ pets 多个正则匹配 sed –i ’s/dog/cat/g’ pets 替换生效 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:2:5","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"总结 添加使用append,insert命令，删除使用delete,修改使用copy,subsitute命令,查看print 结合-n 静默输出，可以覆盖百分之八十以上的场景 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:3:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"高级编辑命令 P： 打印模式空间开端至\\n内容，并追加到默认输出之前 h: 把模式空间中的内容覆盖至保持空间中 H：把模式空间中的内容追加至保持空间中 g: 从保持空间取出数据覆盖至模式空间 G：从保持空间取出内容追加至模式空间 x: 把模式空间中的内容与保持空间中的内容进行互换 n: 读取匹配到的行的下一行覆盖至模式空间 N：读取匹配到的行的下一行追加至模式空间 d: 删除模式空间中的行 D：如果模式空间包含换行符，则删除直到第一个换行符的模式空间中的文本，并不会读取新的输入行，而使用合成的模式空间重新启动循环。如果模式空间 不包含换行符，则会像发出d命令那样启动正常的新循环 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:4:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"练习 1、删除centos7系统/etc/grub2.cfg文件中所有以空白开头的行行首的空白字符 2、删除/etc/fstab文件中所有以#开头，后面至少跟一个空白字符的行的行首的#和空白字符 3、在centos6系统/root/install.log每一行行首增加#号 4、在/etc/fstab文件中不以#开头的行的行首增加#号 5、处理/etc/fstab路径,使用sed命令取出其目录名和基名 6、利用sed 取出ifconfig命令中本机的IPv4地址 7、统计centos安装光盘中Package目录下的所有rpm文件的以.分隔倒数第二个字段的重复次数 8、统计/etc/init.d/functions文件中每个单词的出现次数，并排序（用grep和sed两种方法分别实现） 9、将文本文件的n和n+1行合并为一行，n为奇数行 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:5:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"思维导图 sed\r","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:6:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"参考 linux-sed-command sed常用命令 ","date":"2021-06-26","objectID":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/:7:0","tags":["linux文本三剑客"],"title":"文本三剑客之 sed","uri":"/%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bsed/"},{"categories":["linux"],"content":"为什么使用Linux grep命令? Grep是一个非常有用的命令行工具，可以用来在文件中查找指定的文本模式。通过使用grep命令，你可以快速定位和提取包含特定模式的行，从而加快查找和处理文本数据的效率。这是一个在日常工作中经常需要使用的任务，因此grep命令在Linux中非常流行。 ","date":"2021-06-20","objectID":"/grep/:1:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"Linux grep命令是什么？ Grep是一个在Linux和其他类Unix系统上可用的命令行实用程序，用于搜索和匹配文本。它的名字来自于全局正则表达式（global regular expression print），它的主要功能是根据给定的模式搜索文件中的文本，并打印匹配的行。grep命令支持使用简单的文本模式或正则表达式进行搜索，并且可以通过命令选项进行进一步的定制。 ","date":"2021-06-20","objectID":"/grep/:2:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"如何使用Linux grep命令？ 要使用grep命令，在命令行中输入grep，后跟要搜索的模式和要搜索的文件的路径。下面是一些常用的grep命令选项和示例： 示例列表 命令 解释 grep -i “pattern” file.txt 忽略大小写进行搜索 grep -r “pattern” directory/ 递归搜索子目录 grep -E “pattern” file.txt 使用正则表达式进行搜索 grep -A 2 “pattern” file.txt 显示匹配模式之前或之后的文本行 grep -C 2 “pattern” file.txt 输出匹配模式的上下文行 grep -i “pattern” file.txt 忽略大小写 grep -w “pattern” file.txt 精确匹配 grep -e hello -e world file.txt 多个关键词匹配 grep -v “pattern” file.txt 反向查找，不包含某个关键词的行 grep -lr “pattern” file.txt 递归匹配哪些文件名包含匹配的关键词 grep -o “pattern” file.txt 只输出匹配的内容 grep\r","date":"2021-06-20","objectID":"/grep/:3:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"最佳实践 ","date":"2021-06-20","objectID":"/grep/:4:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"搜索文件中最后匹配的一个关键词的上下200行 $ tac file.txt | grep -m 1 \"/enforcementGroupInfo/editPassword\" -B 200 -A 200 | tac \u003e /tmp/1.txt # or $ tac file.txt | grep -m 1 \"/enforcementGroupInfo/editPassword\" -C 200 | tac \u003e /tmp/1.txt 命令解释： tac file.txt - 反向读取文件内容（从最后一行开始） grep -m 1 - 只匹配第一个出现的模式（即原文件最后一个匹配项） -B 200 -A 200 - 显示匹配行前后的200行内容 -C 200 - 输出匹配模式的上下文行 tac - 再次反向输出，恢复原始行顺序 \u003e /tmp/1.txt - 将结果重定向到目标文件 备选方案（适用于大文件） 如果文件非常大，可以使用更节省内存的方法： # 获取最后一个匹配行号 line=$(grep -n \"/enforcementGroupInfo/editPassword\" file.txt | tail -1 | cut -d: -f1) # 提取该行上下200行内容 [ -n \"$line\" ] \u0026\u0026 sed -n \"$((line\u003e200 ? line-200 : 1)),$((line+200))p\" file.txt \u003e /tmp/1.txt 验证结果 检查输出文件内容是否正确： wc -l /tmp/1.txt # 应该最多401行（200+1+200） tail -n 10 /tmp/1.txt | grep \"/enforcementGroupInfo/editPassword\" # 确认包含关键词 注意事项 如果文件中没有匹配项，/tmp/1.txt 会是空文件 如果匹配项在文件开头，前200行会从第1行开始 如果匹配项在文件结尾，后200行会到文件末尾结束 对于二进制文件，添加 -a 选项：grep -a ","date":"2021-06-20","objectID":"/grep/:4:1","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["linux"],"content":"参考 是真的很详细了！Linux中的Grep命令使用实例 ","date":"2021-06-20","objectID":"/grep/:5:0","tags":["linux文本三剑客"],"title":"文本三剑客之 grep","uri":"/grep/"},{"categories":["Kubernetes"],"content":"K8s Service有四种类型 Service Headless Service NodePort Service LoadBalancer Service Service 如果不指定则为默认类型 ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:0:0","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"Service ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:1:0","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"Service是什么？ Service服务可以为一组具有相同功能的容器应用提供一个统一的入口地址。 ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:1:1","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"Service可以用来做什么？ 我们都知道Pod在摧毁重建时ip地址是会动态变化的，这样通过客户端直接访问不合适了，这时候就可以选择使用服务来和Pod建立连接，通过标签选择器进行适配。这样就能有效的解决了Pod ip地址动态变换的问题了。 ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:1:2","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"Headless Service headless service作为service的一种类型，它又解决了什么问题？headless service 顾名思义无头服务。 ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:2:0","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"为什么需要无头服务？ 客户端想要和指定的的Pod直接通信 并不是随机选择 开发人员希望自己控制负载均衡的策略，不使用Service提供的默认的负载均衡的功能，或者应用程序希望知道属于同组服务的其它实例。 ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:2:1","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"Headless Service使用场景 有状态应用，例如数据库 例如主节点可以对数据库进行读写操作，而其它的两个工作节点只能读，在这里客户端就没必要指定pod服务的集群地址，直接指定数据库Pod ip地址即可，这里需要绑定dns，客户端访问dns，dns会自动返回pod IP地址列表 Service\r","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:2:2","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"总结 无头服务不需要指定集群地址 无头服务适用有状态应用例如数据库 无头服务dns查询会返回pod列表，开发人员可以自定义负载均衡策略 普通Service可以通过负载均衡路由到不同的容器应用 ","date":"2021-06-13","objectID":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/:3:0","tags":["Kubernetes"],"title":"K8s默认Service和Headless Service的区别","uri":"/k8s%E9%BB%98%E8%AE%A4service%E5%92%8Cheadless-service%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["Kubernetes"],"content":"资源对象概述 Kubernetes中的基本概念和术语大多是围绕资源对象（Resource Object）来说的，而资源对象在总体上可分为以下两类。 某种资源的对象，例如节点（Node）、Pod、服务（Service）、存储卷（Volume） 与资源对象相关的事物与动作，例如标签（Label）、注解（Annotation）、命名空间（Namespace）、部署（Deployment）、HPA、PVC。 集群类 集群（Cluster）表示一个由Master和Node组成的Kubernetes集群。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:0:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Master Master指的是集群的控制节点。在每个Kubernetes集群中都需要有一个或一组被称为Master的节点，来负责整个集群的管理和控制。Master通常占据一个独立的服务器（在高可用部署中建议至少使用3台服务器），是整个集群的\"大脑\"，如果它发生宕机或者不可用，那么对集群内容器应用的管理都将无法实施。 在Master上运行着以下关键进程。 Kubernetes API Server（kube-apiserver）：提供HTTP RESTfulAPI接口的主要服务，是Kubernetes里对所有资源进行增、删、改、查等操作的唯一入口，也是集群控制的入口进程。 Kubernetes Controller Manager（kube-controller-manager）：Kubernetes里所有资源对象的自动化控制中心，可以将其理解为资源对象的\"大总管\"。 Kubernetes Scheduler（kube-scheduler）：负责资源调度（Pod调度）的进程，相当于公交公司的调度室。 另外，在Master上通常还需要部署etcd服务 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:1:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Node Kubernetes集群中除Master外的其他服务器被称为Node，Node在较早的版本中也被称为Minion。与Master一样，Node可以是一台物理主机，也可以是一台虚拟机。Node是Kubernetes集群中的工作负载节点，每个Node都会被Master分配一些工作负载（Docker容器），当某个Node宕机时，其上的工作负载会被Master自动转移到其他Node上。 在每个Node上都运行着以下关键进程。 kubelet：负责Pod对应容器的创建、启停等任务，同时与Master密切协作，实现集群管理的基本功能。 kube-proxy：实现Kubernetes Service的通信与负载均衡机制的服务。 容器运行时（如Docker）：负责本机的容器创建和管理。Node可以在运行期间动态增加到Kubernetes集群中，前提是在这个Node上已正确安装、配置和启动了上述关键进程。在默认情况下，kubelet会向Master注册自己，这也是Kubernetes推荐的Node管理方式。一旦Node被纳入集群管理范畴，kubelet进程就会定时向Master汇报自身的情报，例如操作系统、主机CPU和内存使用情况，以及当前有哪些Pod在运行等，这样Master就可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。而某个Node在超过指定时间不上报信息时，会被Master判定为\"失联\"，该Node的状态就被标记为不可用（NotReady），Master随后会触发\"工作负载大转移\"的自动流程。 应用类 Kubernetes中属于应用类的概念和相应的资源对象类型最多，所以应用类也是需要重点学习的一类。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:2:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Service与Pod 应用类相关的资源对象主要是围绕Service（服务）和Pod这两个核心对象展开的。 一般说来，Service指的是无状态服务，通常由多个程序副本提供服务，在特殊情况下也可以是有状态的单实例服务，比如MySQL这种数据存储类的服务。与我们常规理解的服务不同，Kubernetes里的Service具有一个全局唯一的虚拟ClusterIP地址，Service一旦被创建，Kubernetes就会自动为它分配一个可用的ClusterIP地址，而且在Service的整个生命周期中，它的ClusterIP地址都不会改变，客户端可以通过这个虚拟IP地址+服务的端口直接访问该服务，再通过部署Kubernetes集群的DNS服务，就可以实现Service Name（域名）到ClusterIP地址的DNS映射功能，我们只要使用服务的名称（DNS名称）即可完成到目标服务的访问请求。“服务发现\"这个传统架构中的棘手问题在这里首次得以完美解决，同时，凭借ClusterIP地址的独特设计，Kubernetes进一步实现了Service的透明负载均衡和故障自动恢复的高级特性。 通过分析、识别并建模系统中的所有服务为微服务——Kubernetes Service，我们的系统最终由多个提供不同业务能力而又彼此独立的微服务单元组成，服务之间通过TCP/IP进行通信，从而形成强大又灵活的弹性网格，拥有强大的分布式能力、弹性扩展能力、容错能力，程序架构也变得简单和直观许多 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:3:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Pod 为什么Kubernetes会设计出一个全新的Pod概念并且Pod有这样特殊的组成结构？原因如下。 为多进程之间的协作提供一个抽象模型，使用Pod作为基本的调度、复制等管理工作的最小单位，让多个应用进程能一起有效地调度和伸缩。 Pod里的多个业务容器共享Pause容器的IP，共享Pause容器挂接的Volume，这样既简化了密切关联的业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题。 Kubernetes为每个Pod都分配了唯一的IP地址，称之为Pod IP，一个Pod里的多个容器共享Pod IP地址。Kubernetes要求底层网络支持集群内任意两个Pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术实现，例如Flannel、OpenvSwitch等，因此我们需要牢记一点：在Kubernetes里，一个Pod里的容器与另外主机上的Pod容器能够直接通信。 Pod其实有两种类型：普通的Pod及静态Pod（Static Pod）。后者比较特殊，它并没被存放在Kubernetes的etcd中，而是被存放在某个具体的Node上的一个具体文件中，并且只能在此Node上启动、运行。而普通的Pod一旦被创建，就会被放入etcd中存储，随后被Kubernetes Master调度到某个具体的Node上并绑定（Binding），该Pod被对应的Node上的kubelet进程实例化成一组相关的Docker容器并启动。在默认情况下，当Pod里的某个容器停止时，Kubernetes会自动检测到这个问题并且重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，就会将这个Node上的所有Pod都重新调度到其他节点上。 Pod、容器与Node的关系 Pod、容器与Node的关系\r","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:4:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Label与标签选择器 Label（标签）是Kubernetes系统中的另一个核心概念，相当于我们熟悉的\"标签”。一个Label是一个key=value的键值对，其中的key与value由用户自己指定。Label可以被附加到各种资源对象上，例如Node、Pod、Service、Deployment等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上。Label通常在资源对象定义时确定，也可以在对象创建后动态添加或者删除。我们可以通过给指定的资源对象捆绑一个或多个不同的Label来实现多维度的资源分组管理功能，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作，例如，部署不同版本的应用到不同的环境中，以及监控、分析应用（日志记录、监控、告警）等。一些常用的Label示例如下。 版本标签：release：stable和release：canary。 环境标签：environment：dev、environment：qa和environment：production。 架构标签：tier：frontend、tier：backend和tier：middleware。 分区标签：partition：customerA和partition：customerB。 质量管控标签：track：daily和track：weekly Label也是Pod的重要属性之一，其重要性仅次于Pod的端口，我们几乎见不到没有Label的Pod Service很重要的一个属性就是标签选择器，如果我们不小心把标签选择器写错了，就会出现指鹿为马的闹剧。如果恰好匹 配到了另一种Pod实例，而且对应的容器端口恰好正确，服务可以正常连接，则很难排查问题，特别是在有众多Service的复杂系统中。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:5:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Pod与Deployment 前面提到，大部分Service都是无状态的服务，可以由多个Pod副本实例提供服务。通常情况下，每个Service对应的Pod服务实例数量都是固定的，如果一个一个地手工创建Pod实例，就太麻烦了，最好是用模板的思路，即提供一个Pod模板（Template），然后由程序根据我们指定的模板自动创建指定数量的Pod实例。这就是Deployment这个资源对象所要完成的事情了。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:6:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Service的ClusterIP地址 既然每个Pod都会被分配一个单独的IP地址，而且每个Pod都提供了一个独立的Endpoint（Pod IP+containerPort）以被客户端访问，那么现在多个Pod副本组成了一个集群来提供服务，客户端如何访问它们呢？传统的做法是部署一个负载均衡器（软件或硬件），为这组Pod开启一个对外的服务端口如8000端口，并且将这些Pod的Endpoint列表加入8000端口的转发列表中，客户端就可以通过负载均衡器的对外IP地址+8000端口来访问此服务了。Kubernetes也是类似的做法，Kubernetes内部在每个Node上都运行了一套全局的虚拟负载均衡器，自动注入并自动实时更新集群中所有Service的路由表，通过iptables或者IPVS机制，把对Service的请求转发到其后端对应的某个Pod实例上，并在内部实现服务的负载均衡与会话保持机制。不仅如此，Kubernetes还采用了一种很巧妙又影响深远的设计——ClusterIP地址。我们知道，Pod的Endpoint地址会随着Pod的销毁和重新创建而发生改变，因为新Pod的IP地址与之前旧Pod的不同。Service一旦被创建，Kubernetes就会自动为它分配一个全局唯一的虚拟IP地址——ClusterIP地址，而且在Service的整个生命周期内，其ClusterIP地址不会发生改变，这样一来，每个服务就变成了具备唯一IP地址的通信节点，远程服务之间的通信问题就变成了基础的TCP网络通信问题。 之所以说ClusterIP地址是一种虚拟IP地址，原因有以下几点。 ClusterIP地址仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配IP地址（来源于ClusterIP地址池），与Node和Master所在的物理网络完全无关。 因为没有一个\"实体网络对象\"来响应，所以ClusterIP地址无法被Ping通。ClusterIP地址只能与Service Port组成一个具体的服务访问端点，单独的ClusterIP不具备TCP/IP通信的基础。 ClusterIP属于Kubernetes集群这个封闭的空间，集群外的节点要访问这个通信端口，则需要做一些额外的工作 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:7:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"Service的外网访问问题 前面提到，服务的ClusterIP地址在Kubernetes集群内才能被访问，那么如何让集群外的应用访问我们的服务呢？这也是一个相对复杂的问题。要弄明白这个问题的解决思路和解决方法，我们需要先弄明白 Kubernetes的三种IP，这三种IP分别如下。 Node IP：Node的IP地址。 Pod IP：Pod的IP地址。 Service IP：Service的IP地址。 首先，Node IP是Kubernetes集群中每个节点的物理网卡的IP地址，是一个真实存在的物理网络，所有属于这个网络的服务器都能通过这个网络直接通信，不管其中是否有部分节点不属于这个Kubernetes集群。这也表明Kubernetes集群之外的节点访问Kubernetes集群内的某个节点或者TCP/IP服务时，都必须通过Node IP通信。 其次，Pod IP是每个Pod的IP地址，在使用Docker作为容器支持引擎的情况下，它是Docker Engine根据docker0网桥的IP地址段进行分配的，通常是一个虚拟二层网络。前面说过，Kubernetes要求位于不同Node上的Pod都能够彼此直接通信，所以Kubernetes中一个Pod里的容器访问另外一个Pod里的容器时，就是通过Pod IP所在的虚拟二层网络进行通信的，而真实的TCP/IP流量是通过Node IP所在的物理网卡流出的。 在Kubernetes集群内，Service的ClusterIP地址属于集群内的地址，无法在集群外直接使用这个地址。为了解决这个问题，Kubernetes首先引入了NodePort这个概念，NodePort也是解决集群外的应用访问集群内服务的直接、有效的常见做法 NodePort的实现方式是，在Kubernetes集群的每个Node上都为需要外部访问的Service开启一个对应的TCP监听端口，外部系统只要用任意一个Node的IP地址+NodePort端口号即可访问此服务，在任意Node上运行netstat命令，就可以看到有NodePort端口被监听。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:8:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"NodePort 存在的一问题引出了Ingress对象 NodePort的确功能强大且通用性强，但也存在一个问题，即每个Service都需要在Node上独占一个端口，而端口又是有限的物理资源，那能不能让多个Service共用一个对外端口呢？这就是后来增加的Ingress资源对象所要解决的问题。在一定程度上，我们可以把Ingress的实现机制理解为基于Nginx的支持虚拟主机的HTTP代理。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:9:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"有状态的应用集群 我们知道，Deployment对象是用来实现无状态服务的多副本自动控制功能的，那么有状态的服务，比如ZooKeeper集群、MySQL高可用集群（3节点集群）、Kafka集群等是怎么实现自动部署和管理的呢？这个问题就复杂多了，这些一开始是依赖StatefulSet解决的，但后来发现对于一些复杂的有状态的集群应用来说，StatefulSet还是不够通用和强大，所以后面又出现了Kubernetes Operator。 我们先说说StatefulSet。StatefulSet之前曾用过PetSet这个名称，很多人都知道，在IT世界里，有状态的应用被类比为宠物（Pet），无状态的应用则被类比为牛羊，每个宠物在主人那里都是\"唯一的存在\"，宠物生病了，我们是要花很多钱去治疗的，需要我们用心照料，而无差别的牛羊则没有这个待遇。总结下来，在有状态集群中一般有如下特殊共性。 每个节点都有固定的身份ID，通过这个ID，集群中的成员可以相互发现并通信。 集群的规模是比较固定的，集群规模不能随意变动。 集群中的每个节点都是有状态的，通常会持久化数据到永久存储中，每个节点在重启后都需要使用原有的持久化数据。 集群中成员节点的启动顺序（以及关闭顺序）通常也是确定的。 如果磁盘损坏，则集群里的某个节点无法正常运行，集群功能受损 StatefulSet从本质上来说，可被看作Deployment/RC的一个特殊变种，它有如下特性。 StatefulSet里的每个Pod都有稳定、唯一的网络标识，可以用来发现集群内的其他成员。假设StatefulSet的名称为kafka，那么第1个Pod叫kafka-0，第2个叫kafka-1，以此类推。 StatefulSet控制的Pod副本的启停顺序是受控的，操作第n个Pod时，前n-1个Pod已经是运行且准备好的状态。 StatefulSet里的Pod采用稳定的持久化存储卷，通过PV或PVC来实现，删除Pod时默认不会删除与StatefulSet相关的存储卷（为了保证数据安全）。 StatefulSet除了要与PV卷捆绑使用，以存储Pod的状态数据，还要与Headless Service配合使用，即在每个StatefulSet定义中都要声明它属于哪个Headless Service。StatefulSet在Headless Service的基础上又为StatefulSet控制的每个Pod实例都创建了一个DNS域名，这个域名的格式 如下： ${podname}.${headless service name} StatefulSet的建模能力有限，面对复杂的有状态集群时显得力不从心，所以就有了后来的Kubernetes Operator框架和众多的Operator实现了。需要注意的是，Kubernetes Operator框架并不是面向普通用户的，而是面向Kubernetes平台开发者的。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:10:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"批处理应用 除了无状态服务、有状态集群、常见的第三种应用，还有批处理应用。批处理应用的特点是一个或多个进程处理一组数据（图像、文件、视频等），在这组数据都处理完成后，批处理任务自动结束。为了支持这类应用，Kubernetes引入了新的资源对象——Job。 Jobs控制器提供了两个控制并发数的参数：completions和parallelism，completions表示需要运行任务数的总数，parallelism表示并发运行的个数，例如设置parallelism为1，则会依次运行任务，在前面的任务运行后再运行后面的任务。Job所控制的Pod副本是短暂运行的，可以将其视为一组容器，其中的每个容器都仅运行一次。当Job控制的所有Pod副本都运行结束时，对应的Job也就结束了。Job在实现方式上与Deployment等副本控制器不同，Job生成的Pod副本是不能自动重启的，对应Pod副本的restartPolicy都被设置为Never，因此，当对应的Pod副本都执行完成时，相应的Job也就完成了控制使命。后来，Kubernetes增加了CronJob，可以周期性地执行某个任务。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:11:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"应用的配置问题 通过前面的学习，我们初步理解了三种应用建模的资源对象，总结如下。 无状态服务的建模：Deployment。 有状态集群的建模：StatefulSet。 批处理应用的建模：Job。 在进行应用建模时，应该如何解决应用需要在不同的环境中修改配置的问题呢？这就涉及ConfigMap和Secret两个对象。 ConfigMap顾名思义，就是保存配置项（key=value）的一个Map，如果你只是把它理解为编程语言中的一个Map，那就大错特错了。ConfigMap是分布式系统中\"配置中心\"的独特实现之一。我们知道，几乎所有应用都需要一个静态的配置文件来提供启动参数，当这个应用是一个分布式应用，有多个副本部署在不同的机器上时，配置文件的分发就成为一个让人头疼的问题，所以很多分布式系统都有一个配置中心组件，来解决这个问题。但配置中心通常会引入新的API，从而导致应用的耦合和侵入。 用户将配置文件的内容保存到ConfigMap中，文件名可作为key，value就是整个文件的内容，多个配置文件都可被放入同一个ConfigMap。 在建模用户应用时，在Pod里将ConfigMap定义为特殊的Volume进行挂载。在Pod被调度到某个具体Node上时，ConfigMap里的配置文件会被自动还原到本地目录下，然后映射到Pod里指定的配置目录下，这样用户的程序就可以无感知地读取配置了。 在ConfigMap的内容发生修改后，Kubernetes会自动重新获取ConfigMap的内容，并在目标节点上更新对应的文件。 接下来说说Secret。Secret也用于解决应用配置的问题，不过它解决的是对敏感信息的配置问题，比如数据库的用户名和密码、应用的数字证书、Token、SSH密钥及其他需要保密的敏感配置。对于这类敏感信息，我们可以创建一个Secret对象，然后被Pod引用。Secret中的数据要求以BASE64编码格式存放。注意，BASE64编码并不是加密的，在Kubernetes 1.7版本以后，Secret中的数据才可以以加密的形式进行保存，更加安全。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:12:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"应用的运维问题 最后说说与应用的自动运维相关的几个重要对象。 首先就是HPA（Horizontal Pod Autoscaler），如果我们用Deployment来控制Pod的副本数量，则可以通过手工运行kubectl scale命令来实现Pod扩容或缩容。如果仅仅到此为止，则显然不符合谷歌对Kubernetes的定位目标——自动化、智能化。在谷歌看来，分布式系统要能够根据当前负载的变化自动触发水平扩容或缩容，因为这一过程可能是频繁发生、不可预料的，所以采用手动控制的方式是不现实的，因此就有了后来的HPA这个高级功能。我们可以将HPA理解为Pod横向自动扩容，即自动控制Pod数量的增加或减少。通过追踪分析指定Deployment控制的所有目标Pod的负载变化情况，来确定是否需要有针对性地调整目标Pod的副本数量，这是HPA的实现原理。Kubernetes内置了基于Pod的CPU利用率进行自动扩缩容的机制，应用开发者也可以自定义度量指标如每秒请求数，来实现自定义的HPA功能。 存储类 存储类的资源对象主要包括Volume、Persistent Volume、PVC和StorageClass。 首先看看基础的存储类资源对象——Volume（存储卷） Volume是Pod中能够被多个容器访问的共享目录。Kubernetes中的Volume概念、用途和目的与Docker中的Volume比较类似，但二者不能等价。首先，Kubernetes中的Volume被定义在Pod上，被一个Pod里的多个容器挂载到具体的文件目录下；其次，Kubernetes中的Volume与Pod的生命周期相同，但与容器的生命周期不相关，当容器终止或者重启时，Volume中的数据也不会丢失；最后，Kubernetes支持多种类型的Volume，例如GlusterFS、Ceph等分布式文件系统。Volume的使用也比较简单，在大多数情况下，我们先在Pod上声明一个Volume，然后在容器里引用该Volume并将其挂载（Mount）到容器里的某个目录下。举例来说，若我们要给之前的Tomcat Pod增加一个名为datavol的Volume，并将其挂载到容器的某个路径/mydata-data目录下，则只对Pod的定义文件做下修正即可。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:13:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"emptyDir 一个emptyDir是在Pod分配到Node时创建的。从它的名称就可以看出，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为这是Kubernetes自动分配的一个目录，当Pod从Node上移除时，emptyDir中的数据也被永久移除。emptyDir的一些用途如下。 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留。 长时间任务执行过程中使用的临时目录。 一个容器需要从另一个容器中获取数据的目录（多容器共享目录）。 在默认情况下，emptyDir使用的是节点的存储介质，例如磁盘或者网络存储。还可以使用emptyDir.medium属性，把这个属性设置为\"Memory\"，就可以使用更快的基于内存的后端存储了。需要注意的是，这种情况下的emptyDir使用的内存会被计入容器的内存消耗，将受到资源限制和配额机制的管理。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:14:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"hostPath hostPath为在Pod上挂载宿主机上的文件或目录，通常可以用于以下几方面。 在容器应用程序生成的日志文件需要永久保存时，可以使用宿主机的高速文件系统对其进行存储。 需要访问宿主机上Docker引擎内部数据结构的容器应用时，可以通过定义hostPath为宿主机/var/lib/docker目录，使容器内部的应用可以直接访问Docker的文件系统。 在使用这种类型的Volume时，需要注意以下几点。 在不同的Node上具有相同配置的Pod，可能会因为宿主机上的目录和文件不同，而导致对Volume上目录和文件的访问结果不一致。 如果使用了资源配额管理，则Kubernetes无法将hostPath在宿主机上使用的资源纳入管理。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:15:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"公有云Volume 公有云提供的Volume类型包括谷歌公有云提供的GCEPersistentDisk、亚马逊公有云提供的AWS Elastic Block Store（EBSVolume）等。当我们的Kubernetes集群运行在公有云上或者使用公有云厂家提供的Kubernetes集群时，就可以使用这类Volume。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:16:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Kubernetes"],"content":"其他类型的Volume iscsi：将iSCSI存储设备上的目录挂载到Pod中。 nfs：将NFS Server上的目录挂载到Pod中。 glusterfs：将开源GlusterFS网络文件系统的目录挂载到Pod中。 rbd：将Ceph块设备共享存储（Rados Block Device）挂载到Pod中。 gitRepo：通过挂载一个空目录，并从Git库克隆（clone）一个git repository以供Pod使用。 configmap：将配置数据挂载为容器内的文件。 secret：将Secret数据挂载为容器内的文件。 安全类 安全始终是Kubernetes发展过程中的一个关键领域。 从本质上来说，Kubernetes可被看作一个多用户共享资源的资源管理系统，这里的资源主要是各种Kubernetes里的各类资源对象，比如Pod、Service、Deployment等。只有通过认证的用户才能通过Kubernetes的API Server查询、创建及维护相应的资源对象，理解这一点很关键。 Kubernetes里的用户有两类：我们开发的运行在Pod里的应用；普通用户，如典型的kubectl命令行工具，基本上由指定的运维人员（集群管理员）使用。在更多的情况下，我们开发的Pod应用需要通过API Server查询、创建及管理其他相关资源对象，所以这类用户才是Kubernetes的关键用户。为此，Kubernetes设计了Service Account这个特殊的资源对象，代表Pod应用的账号，为Pod提供必要的身份认证。在此基础上，Kubernetes进一步实现和完善了基于角色的访问控制权限系统——RBAC（Role-Based Access Control）。 在默认情况下，Kubernetes在每个命名空间中都会创建一个默认的名称为default的Service Account，因此Service Account是不能全局使用的，只能被它所在命名空间中的Pod使用。通过以下命令可以查看集群中的所有Service Account： sudo kubectl get sa -A Service Account是通过Secret来保存对应的用户（应用）身份凭证的，这些凭证信息有CA根证书数据（ca.crt）和签名后的Token信息（Token）。在Token信息中就包括了对应的Service Account的名称，因此API Server通过接收到的Token信息就能确定Service Account的身份。在默认情况下，用户创建一个Pod时，Pod会绑定对应命名空间中的default这个Service Account作为其\"公民身份证\"。当Pod里的容器被创建时，Kubernetes会把对应的Secret对象中的身份信息（ca.crt、Token等）持久化保存到容器里固定位置的本地文件中，因此当容器里的用户进程通过Kubernetes提供的客户端API去访问API Server时，这些API会自动读取这些身份信息文件，并将其附加到HTTPS请求中传递给API Server以完成身份认证逻辑。在身份认证通过以后，就涉及\"访问授权\"的问题，这就是RBAC要解决的问题了。 首先我们要学习的是Role这个资源对象，包括Role与ClusterRole两种类型的角色。角色定义了一组特定权限的规则，比如可以操作某类资源对象。局限于某个命名空间的角色由Role对象定义，作用于整个Kubernetes集群范围内的角色则通过ClusterRole对象定义。 在RoleBinding中使用subjects（目标主体）来表示要授权的对象，这是因为我们可以授权三类目标账号：Group（用户组）、User（某个具体用户）和Service Account（Pod应用所使用的账号）。 在安全领域，除了以上针对API Server访问安全相关的资源对象，还有一种特殊的资源对象——NetworkPolicy（网络策略），它是网络安全相关的资源对象，用于解决用户应用之间的网络隔离和授权问题。NetworkPolicy是一种关于Pod间相互通信，以及Pod与其他网络端点间相互通信的安全规则设定。 NetworkPolicy资源使用标签选择Pod，并定义选定Pod所允许的通信规则。在默认情况下，Pod间及Pod与其他网络端点间的访问是没有限制的，这假设了Kubernetes集群被一个厂商（公司/租户）独占，其中部署的应用都是相互可信的，无须相互防范。但是，如果存在多个厂商共同使用一个Kubernetes集群的情况，则特别是在公有云环境中，不同厂商的应用要相互隔离以增加安全性，这就可以通过NetworkPolicy来实现了。 ","date":"2021-06-13","objectID":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/:17:0","tags":["Kubernetes"],"title":"Kubernetes的基本概念和术语","uri":"/kubernetes%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E5%92%8C%E6%9C%AF%E8%AF%AD/"},{"categories":["Influxdb"],"content":"InfluxDB 基础了解 ","date":"2021-05-28","objectID":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/:1:0","tags":["Influxdb"],"title":"Influxdb 添加鉴权","uri":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/"},{"categories":["Influxdb"],"content":"介绍 一个开源的时间序列数据库 ","date":"2021-05-28","objectID":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/:1:1","tags":["Influxdb"],"title":"Influxdb 添加鉴权","uri":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/"},{"categories":["Influxdb"],"content":"特性 1、内置HTTP-API,无需编写服务端代码来启动和运行。 2、数据可以被标记，允许非常灵活的查询 3、类似SQL一样的查询语言 4、简单的安装和管理， 快速的获取和输出数据 5、它的目标是符合实时查询， 这意味着每一个数据点都会被索引，并且可以立即在小于100ms的查询中获得 docker 创建容器实例 docker run --name=cm2-influxdb -d -p 8530:8083 -p 8586:8086 \\ -v /opt/influxdata:/var/lib/influxdb \\ -m 10g \\ --restart=always \\ influxdb:1.8 ","date":"2021-05-28","objectID":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/:1:2","tags":["Influxdb"],"title":"Influxdb 添加鉴权","uri":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/"},{"categories":["Influxdb"],"content":"配置鉴权 ","date":"2021-05-28","objectID":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/:2:0","tags":["Influxdb"],"title":"Influxdb 添加鉴权","uri":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/"},{"categories":["Influxdb"],"content":"添加鉴权配置 [http] enabled = true bind-address = \":8086\" auth-enabled = true log-enabled = true ping-auth-enabled = true 重启influxdb服务 未添加鉴权 $ curl -G http://172.16.24.220:8586/query --data-urlencode \"q=SHOW DATABASES\" {\"results\":[{\"statement_id\":0,\"series\":[{\"name\":\"databases\",\"columns\":[\"name\"],\"values\":[[\"host_metrics\"],[\"vnf_metrics\"],[\"events\"],[\"alarm\"],[\"custom_metrics\"],[\"_internal\"],[\"k8s_metrics\"],[\"mw_metrics\"],[\"bss_metrics\"],[\"application_metrics\"],[\"oss_metrics\"],[\"itracing_metrics\"],[\"prometheus_metrics\"],[\"dcm_metrics\"]]}]}]} 添加鉴权未创建用户,提示需要创建鉴权用户或者关闭鉴权 curl -G http://172.16.17.82:8586/query --data-urlencode \"q=SHOW DATABASES\" [lushuan@220 nms-influxdb]$ curl -G http://172.16.24.220:8586/query --data-urlencode \"q=SHOW DATABASES\" {\"error\":\"error authorizing query: create admin user first or disable authentication\"} 登录influxdb 容器创建用户 docker exec -it cm2-influxdb sh \u003e influx \u003e CREATE USER admin WITH PASSWORD 'password' WITH ALL PRIVILEGES 再次测试 $ curl -G http://172.16.24.220:8586/query -u admin:abc@123A --data-urlencode \"q=SHOW DATABASES\" {\"results\":[{\"statement_id\":0,\"series\":[{\"name\":\"databases\",\"columns\":[\"name\"],\"values\":[[\"host_metrics\"],[\"vnf_metrics\"],[\"events\"],[\"alarm\"],[\"custom_metrics\"],[\"_internal\"],[\"k8s_metrics\"],[\"mw_metrics\"],[\"bss_metrics\"],[\"application_metrics\"],[\"oss_metrics\"],[\"itracing_metrics\"],[\"prometheus_metrics\"],[\"dcm_metrics\"]]}]}]} ","date":"2021-05-28","objectID":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/:2:1","tags":["Influxdb"],"title":"Influxdb 添加鉴权","uri":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/"},{"categories":["Influxdb"],"content":"参考 influxdb 官网 ","date":"2021-05-28","objectID":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/:3:0","tags":["Influxdb"],"title":"Influxdb 添加鉴权","uri":"/influxdb-%E9%85%8D%E7%BD%AE%E9%89%B4%E6%9D%83/"},{"categories":["Kubernetes"],"content":"背景 通过 kubeadm 安装k8s v1.20 集群报错 操作系统环境信息 $ cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"18.04.5 LTS (Bionic Beaver)\" ID=ubuntu ID_LIKE=debian PRETTY_NAME=\"Ubuntu 18.04.5 LTS\" VERSION_ID=\"18.04\" HOME_URL=\"https://www.ubuntu.com/\" SUPPORT_URL=\"https://help.ubuntu.com/\" BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\" PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\" VERSION_CODENAME=bionic UBUNTU_CODENAME=bionic kubeadm init 安装报错信息 [kubelet-check] It seems like the kubelet isn't running or healthy. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused. [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get \"http://localhost:10248/healthz\": dial tcp 127.0.0.1:10248: connect: connection refused. Unfortunately, an error has occurred: timed out waiting for the condition This error is likely caused by: - The kubelet is not running - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled) If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands: - 'systemctl status kubelet' - 'journalctl -xeu kubelet' Additionally, a control plane component may have crashed or exited when started by the container runtime. To troubleshoot, list all containers using your preferred container runtimes CLI. Here is one example how you may list all Kubernetes containers running in docker: - 'docker ps -a | grep kube | grep -v pause' Once you have found the failing container, you can inspect its logs with: - 'docker logs CONTAINERID' ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:1:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"排查思路 查看官网介绍为 docker 和 kubelet 服务中的 cgroup 驱动不一致，有两种方法 方式一：驱动向 docker 看齐 方式二：驱动为向 kubelet 看齐 如果docker 不方便重启则统一向 kubelet看齐，并重启对应的服务即可 ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:2:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"解决方式 ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:3:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"docker 配置文件 这里采取的是方式二，docker 默认驱动为 cgroupfs ,只需要添加 \"exec-opts\": [ \"native.cgroupdriver=systemd\" ], 修改后配置文件 $ cat /etc/docker/daemon.json { \"exec-opts\": [ \"native.cgroupdriver=systemd\" ], \"bip\":\"172.12.0.1/24\", \"registry-mirrors\": [ \"http://docker-registry-mirror.kodekloud.com\" ] } 重启docker systemctl restart docker ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:3:1","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"kublete 配置文件 grep 截取一下,可以看得出来kubelet默认 cgoup 驱动为systemd $ cat /var/lib/kubelet/config.yaml |grep group cgroupDriver: systemd 重启kubelet （optional） systemctl restart kubelet ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:3:2","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kubernetes"],"content":"参考 配置cgroup驱动 Docker中的Cgroup Driver:Cgroupfs 与 Systemd 为什么要修改docker的cgroup driver ","date":"2021-05-28","objectID":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/:4:0","tags":["Kubernetes排障"],"title":"Kubeadm 安装k8s集群报错提示 kubelet 未运行","uri":"/kubeadm%E5%AE%89%E8%A3%85k8s%E9%9B%86%E7%BE%A4%E6%8A%A5%E9%94%99%E6%8F%90%E7%A4%BAkubelet%E6%9C%AA%E8%BF%90%E8%A1%8C/"},{"categories":["Kafka"],"content":"kafka 简介 Kafka 被称为下一代分布式消息系统，是非营利性组织ASF(Apache Software Foundation，简称为ASF)基金会中的一个开源项目，比如HTTP Server、Hadoop、ActiveMQ、Tomcat等开源软件都属于Apache基金会的开源软件，类似的消息系统还有RbbitMQ、ActiveMQ、ZeroMQ，最主要的优势是其具备分布式功能、并且结合zookeeper可以实现动态扩容。 kafka 竞品比较\r","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:1:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"名字的由来 kafka的诞生，是为了解决linkedin的数据管道问题，起初linkedin采用了ActiveMQ来进行数据交换，大约是在2010年前后，那时的ActiveMQ还远远无法满足linkedin对数据传递系统的要求，经常由于各种缺陷而导致消息阻塞或者服务无法正常访问，为了能够解决这个问题，linkedin决定研发自己的消息传递系统，当时linkedin的首席架构师jay kreps便开始组织团队进行消息传递系统的研发；由于jay kreps非常喜欢franz kafka,并且觉得kafka这个名字很酷，因此取了个和消息传递系统完全不相干的名称kafka，该名字并没有特别的含义。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:1:1","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Kafka适合什么样的场景? 它可以用于两大类别的应用: 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。 (相当于message queue) 构建实时流式应用程序，对这些流数据进行转换或者影响。 (就是流处理，通过kafka stream topic和topic之间内部进行变化) ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:1:2","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"kafka 优势 吞吐量高，性能好 伸缩性好，支持在线水平扩展 容错性和可靠性 与大数据生态紧密结合，可无缝对接hadoop、stream,spark等 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:1:3","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"发行版本 Confluent Platform Cloudera kafka 偏大数据解决方案 hortonworks kafka 偏大数据解决方案 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:1:4","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"消息模型 JMS 规范 队列-点对点 主题-发布订阅 Apache ActiveMQ AMQP(协议) AMQP模型 队列 信箱 绑定 特点：支持事务，数据一致性高，多用于银行，金融行业 Pivotal RabbitMQ Spring AMQP与Spring JMS MQTT ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:2:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Kafka中的相关概念 在Kafka集群(Cluster)中，一个Kafka节点就是一个Broker，消息由Topic来承载，可以存储在1个或多个Partition中。发布消息的应用为Producer、消费消息的应用为Consumer，多个Consumer可以促成Consumer Group共同消费一个Topic中的消息。 概念/对象 简单说明 Broker Kafka节点 Topic 主题，用来承载消息 Partition 分区，用于主题分片存储 Producer 生产者，向主题发布消息的应用 Consumer 消费者，从主题订阅消息的应用 Consumer Group 消费者组，由多个消费者组成 kafka-concepts\r上图中包含了2个Producer（生产者），一个Topic（主题），3个Partition（分区），3个Replica（副本），3个Broker（Kafka实例或节点），一个Consumer Group（消费者组），其中包含3个Consumer（消费者）。下面我们逐一介绍这些概念。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Producer（生产者） 生产者，顾名思义，就是生产东西的，也就是发送消息的，生产者每发送一个条消息必须有一个Topic（主题），也可以说是消息的类别，生产者源源不断的向kafka服务器发送消息。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:1","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Topic（主题） 每一个发送到Kafka的消息都有一个主题，也可叫做一个类别，类似我们传统数据库中的表名一样，比如说发送一个主题为order的消息，那么这个order下边就会有多条关于订单的消息，只不过kafka称之为主题，都是一样的道理。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:2","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Partition（分区） 生产者发送的消息数据Topic会被存储在分区中，这个分区的概念和ElasticSearch中分片的概念是一致的，都是想把数据分成多个块，好达到我们的负载均衡，合理的把消息分布在不同的分区上，分区是被分在不同的Broker上也就是服务器上，这样我们大量的消息就实现了负载均衡。每个Topic可以指定多个分区，但是至少指定一个分区。每个分区存储的数据都是有序的，不同分区间的数据不保证有序性。因为如果有了多个分区，消费数据的时候肯定是各个分区独立开始的，有的消费得慢，有的消费得快肯定就不能保证顺序了。那么当需要保证消息的顺序消费时，我们可以设置为一个分区，只要一个分区的时候就只能消费这个一个分区，那自然就保证有序了。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:3","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Replica（副本） 副本就是分区中数据的备份，是Kafka为了防止数据丢失或者服务器宕机采取的保护数据完整性的措施，一般的数据存储软件都应该会有这个功能。假如我们有3个分区，由于不同分区中存放的是部分数据，所以为了全部数据的完整性，我们就必须备份所有分区。这时候我们的一份副本就包括3个分区，每个分区中有一个副本，两份副本就包含6个分区，一个分区两份副本。Kafka做了副本之后同样的会把副本分区放到不同的服务器上，保证负载均衡。讲到这我们就可以看见，这根本就是传统数据库中的主从复制的功能，没错，Kafka会找一个分区作为主分区（leader）来控制消息的读写，其他的（副本）都是从分区（follower），这样的话读写可以通过leader来控制，然后同步到副本上去，保证的数据的完整性。如果有某些服务器宕机，我们可以通过副本恢复数据，也可以暂时用副本中的数据来使用。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:4","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Broker（实例或节点） 这个就好说了，意思就是Kafka的实例，启动一个Kafka就是一个Broker，多个Brokder构成一个Kafka集群，这就是分布式的体现，服务器多了自然吞吐率效率啥的都上来了。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:5","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Consumer Group（消费者组）和 Consumer（消费者） Consume消费者来读取Kafka中的消息，可以消费任何Topic的数据，多个Consume组成一个消费者组，一般的一个消费者必须有一个组（Group）名，如果没有的话会被分一个默认的组名。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:3:6","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"kafka 集群架构 Kafka是作为一个集群建立和运行的——一组服务器或代理管理两种类型的客户端，生产者和消费者(一般Pub/Sub术语中的发布者和订阅者)之间的通信。 根据硬件特性的不同，即使一个broker代理也足以形成一个每秒处理数万甚至数十万个事件的集群。但是为了获得高可用性和防止数据丢失，建议至少设置三个broker代理。 Kafka-cluster-architecture\r集群架构和相关重要组件 Kafka-cluster-architecture-v2\r集群中的一个代理被自动选为控制器。它负责诸如故障监视之类的管理任务。为了协调集群内的代理，Kafka使用了一个单独的服务——Apache ZooKeeper。 Kafka-cluster-architecture\r上图该kafka集群A,该集群有8个实例broker节点，集群中的主题有8个分区(p0-p7),副本因子是3，也就是说每份数据存3份，每个分区都有1个leader 和2个follwer,以第一个broker为例，该broker 有3个分区，p1 分区为leader,p1分区上的所有读写请求都是由这个broker 进行处理的，p0和p2 分区是follwer,因此该broker 只负责p0和p2 从p1 中同步数据，而不处理这两个follwer 分区的读写请求。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:4:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"环境搭建-本地伪分布式安装 一台主机部署3个broker的伪分布式集群实例演示，演示版本为 kafka_2.12-2.1.0,下载地址，Kafka启动方式有Zookeeper和Kraft，两种方式只能选择其中一种启动，不能同时使用。这里选择的是zookeeper cd /usr/local tar -zxvf kafka_2.12-2.1.0.tgz cd kafka_2.12-2.1.0 mkdir etc cp config/zookeeper.properties etc/ # config/server.properties 是 kafka broker实例的配置文件，这里要搭建一个三节点的集群所以需要拷贝3份 cp config/server.properties etc/server-0.properties cp config/server.properties etc/server-1.properties cp config/server.properties etc/server-2.properties 修改broker 配置文件server-0.properties的配置，分别是2 处 listeners=PLAINTEXT://:9092 监听端口注释去掉 log.dirs=/tmp/kafka-logs 进行区分 log.dirs=/tmp/kafka-logs-0 修改broker 配置文件server-1.properties的配置，分别是3 处 broker.id=1 防止和 server-0 重复 listeners=PLAINTEXT://:9093 监听端口注释去掉,并更改为9093 log.dirs=/tmp/kafka-logs 进行区分 log.dirs=/tmp/kafka-logs-1 修改broker 配置文件server-2.properties的配置，分别是3 处 broker.id=2 防止和 server-0 重复 listeners=PLAINTEXT://:9094 监听端口注释去掉,并更改为9094 log.dirs=/tmp/kafka-logs 进行区分 log.dirs=/tmp/kafka-logs-2 检查下修改配置 root@k8s-master01:/usr/local/kafka_2.12-2.1.0/etc# cat server-*.properties |grep -e broker.id -e listeners= -e log.dirs=|grep -v \\# broker.id=0 listeners=PLAINTEXT://:9092 log.dirs=/tmp/kafka-logs broker.id=1 listeners=PLAINTEXT://:9093 log.dirs=/tmp/kafka-logs-1 broker.id=2 listeners=PLAINTEXT://:9094 log.dirs=/tmp/kafka-logs-2 启动实例 cd /usr/local/kafka_2.12-2.1.0/bin # 先启动zookeeper ./zookeeper-server-start.sh ../etc/zookeeper.properties # 再启动kafka 的三个实例，新开一个会话以后台进程的方式启动 ./kafka-server-start.sh -daemon ../etc/server-0.properties ./kafka-server-start.sh -daemon ../etc/server-1.properties ./kafka-server-start.sh -daemon ../etc/server-2.properties # 查看服务是否正常启动，查看端口监听 $ netstat -anp|grep -e 9092 -e 9093 -e 9094 tcp6 0 0 :::9094 :::* LISTEN 34724/java tcp6 0 0 :::9092 :::* LISTEN 33537/java tcp6 0 0 :::9093 :::* LISTEN 34355/java 备注：启动Kafka本地环境需JDK 8+以上 创建主题测试部署的集群,创建一个test 名称的topic root@k8s-master01:/usr/local/kafka_2.12-2.1.0/bin# ./kafka-topics.sh --zookeeper localhost:2181 --create --topic test --partitions 3 --replication-factor 2 Created topic \"test\". root@k8s-master01:/usr/local/kafka_2.12-2.1.0/bin# ./kafka-topics.sh --zookeeper localhost:2181 --describe --topic test Topic:test PartitionCount:3 ReplicationFactor:2 Configs: Topic: test Partition: 0 Leader: 1 Replicas: 1,0 Isr: 1,0 Topic: test Partition: 1 Leader: 2 Replicas: 2,1 Isr: 2,1 Topic: test Partition: 2 Leader: 0 Replicas: 0,2 Isr: 0,2 创建好了主题topic 就可以使用生产者和消费者进行消息的发送和读取了。通过kafka-console-consumer.sh脚本模拟消费，会出现一个光标，等待显示消息 root@k8s-master01:/usr/local/kafka_2.12-2.1.0/bin# ./kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --topic test 提供一个kafka-console-producer.sh脚本进行发送消息message root@k8s-master01:/usr/local/kafka_2.12-2.1.0/bin# ./kafka-console-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --topic test \u003emessage ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:5:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"监听器和内外部网络 总结一下kafka server.proerties 必要配置项 broker.id logs.dirs zookeeper.connect 监听器 listeners: 指定broker 启动时本机的监听名称、端口 listeners=PLAINTEXT://:9092 协议：PLAINTEXT listeners=PLAINTEXT://192.168.1.11:9092 协议：SSL listeners=PLAINTEXT://hostname:9092 协议：SASL_PLAINTEXT listeners=PLAINTEXT://0.0.0.0:9092 协议：SASL_SSL listeners: 指定broker 启动时的本机监听端口，给服务器使用 advertised.listeners: 对外发布的访问IP和端口，注册到zookeeper 中，给客户端(client)使用 更多详细配置请参考KAFAK 配置内外网分流，实现同时支持内网，外网，其他网络 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:5:1","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"kafka 消息模型 点对点 发布订阅 消息传播语义 至少一次 最多一次 精确一次 消费者模型\r分区是最小的并行单位 一个消费者可以消费多个分区 一个分区可以被多个消费组里消费者消费 注意：一个分区不能同时被一个消费者组里的多个消费者进行消费 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:6:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"点对点模式 点对点模式\r生产者发送一条消息到queue，只有一个消费者能收到。 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:6:1","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"发布订阅模式 发布订阅模式\r发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。这里的订阅者也就是消费者，且每个消费者都属于不同的消费者组 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:6:2","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"消息传播语义 最多一次——消息可能会丢失，永远不重复发送 最少一次——消息不会丢失，但是可能会重复 精确一次——保障消息被传递到服务器且在服务端不重复 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:6:3","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"Kafka API Producer API Allows applications to send streams of data to topics in the Kafka cluster. Consumer API Permits applications to read data streams from topics in the Kafka cluster. Streams API Acts as a stream processor, transforming data streams from input to output topics. Connect API Enables the development and running of reusable producers or consumers that connect Kafka topics to existing data system applications. Admin API Supports administrative operations on a Kafka cluster, like creating or deleting topics. Kafka API use cases Real-time analytics, event sourcing, log aggregation, message queuing, and stream processing. Kafka API compatible alternative Redpanda streaming data platform. Kafka API提供了一个编程接口，允许应用程序实时生成、消费和处理记录流。它作为Kafka服务器和用户之间的通信点，以低延迟的规模处理实时数据馈送。这些属性使它成为涉及实时分析、事件溯源和许多其他数据密集型操作的用例的首选工具。 Kafka API 架构 Kafka API 架构\r","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:7:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"序列化 序列化与反序列化 常用消息格式 csv 适合简单的消息 json 1. 可读性高 2. 占用空间大 序列化消息 1. avro hadoop、hive 支持好 2. protobuf Avro与Schema 序列化是指将对象以二进制的方式在网络之间传输或者保存到文件中，并可以根据特定的规则进行还原 序列化的好处 节省空间，提高网络传输效率 跨平台 跨语言 序列化和反序列化形象解释 kafka-serdes\r关于序列化和反序列参考 ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:8:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["Kafka"],"content":"参考 https://kafka1x.apachecn.org/intro.html https://kafka.apache.org/ https://developer.confluent.io/quickstart/kafka-on-confluent-cloud/ https://developer.confluent.io/what-is-apache-kafka/ https://kafka.apache.org/downloads https://www.cnblogs.com/xxeleanor/p/15018271.html https://www.altexsoft.com/blog/apache-kafka-pros-cons/ kafka的发行版版本选择 https://www.infoq.cn/article/apache-kafka/ https://www.cloudduggu.com/kafka/architecture/ ","date":"2021-05-19","objectID":"/kafka%E4%BB%8B%E7%BB%8D/:9:0","tags":["Kafka"],"title":"Apache kafka 介绍","uri":"/kafka%E4%BB%8B%E7%BB%8D/"},{"categories":["linux"],"content":"打算给一台服务器做逻辑卷分区，发现磁盘未做分区，且磁盘空间余量较大，本篇记录一下 fdisk 磁盘分区划分的过程 fdisk\r","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:0:0","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"事情的起因 通过pvcreate 创建逻辑卷组提示报错 报错信息 警告\rCan’t open /dev/sda1 exclusively. Mounted filesystem?\r通过 fdisk -l 查看该分区为boot 分区无法创建逻辑卷。fdisk -l 可以用来查看磁盘的所有分区 $ fdisk -l Disk /dev/sda: 1000.2 GB, 1000171331584 bytes, 1953459632 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 262144 bytes / 262144 bytes Disk label type: dos Disk identifier: 0x0003eb01 Device Boot Start End Blocks Id System /dev/sda1 * 2048 2099199 1048576 83 Linux /dev/sda2 2099200 276834303 137367552 8e Linux LVM 从上面的信息可以了解到磁盘/dev/sda划分了两个分区，且扇区大小为512字节，boot 分区大小为(2099199-2048)*512/1024/1024=1G, 且通过pvdisplay查看该/dev/sda2 分区为131G，已经全部用完。总的磁盘有1000.2 GB 可用，抛出boot /dev/sda1分区和/dev/sda2 的 131GB空间，还剩800多GB的磁盘空间未使用。闲言少叙，开始通过fdisk划分新的磁盘分区/dev/sda3 主分区和扩展分区 fdisk\r","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:1:0","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"划分磁盘分区 ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:2:0","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"首先了解什么是 fdisk 命令 fdisk 的意思是 固定磁盘(Fixed Disk) 或 格式化磁盘(Format Disk)，它是命令行下允许用户对分区进行查看、创建、调整大小、删除、移动和复制的工具。它支持 MBR、Sun、SGI、BSD 分区表，但是它不支持 GUID 分区表（GPT）。它不是为操作大分区设计的。 fdisk 允许我们在每块硬盘上创建最多四个主分区。它们中的其中一个可以作为扩展分区，并下设多个逻辑分区。1-4 扇区作为主分区被保留，逻辑分区从扇区 5 开始。 ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:2:1","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"查看分区 fdisk 查看特定分区 fdisk -l /dev/sda fdisk交互指令说明 命令 说明 是否常用 a 设置可引导标记 否 b 编辑bsd磁盘标签 否 c 设置DOS操作系统兼容标记 否 d 删除一个分区 注：这是删除一个分区的动作 是 l 显示已知的分区类型。82 为Linux swap分区，83为Linux分区 注：l是列出分区类型，以供我们设置相应分区的类型； 是 m m 是列出帮助信息 是 n 添加一个分区 是 o 建立空白DOS分区表 否 p p列出分区表 是 q 不保存退出 是 s 新建空白SUN磁盘标签 否 t 改变一个分区的系统ID 是 u 改变显示记录单位 否 v 验证分区表 否 w 把分区表写入硬盘并退出 是 x 扩展应用，专家功能 否 关注一下常用命令即可 ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:2:2","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"创建扩展分区 列出当前操作硬盘的分区情况，用 p $ fdisk /dev/sda Welcome to fdisk (util-linux 2.23.2). Changes will remain in memory only, until you decide to write them. Be careful before using the write command. Command (m for help): n Partition type: p primary (2 primary, 0 extended, 2 free) e extended Select (default e): p 新建磁盘分区，用 n,在创建的时候会提示选择是使用主分区还是逻辑分区，见扩展一。 直接下一步会提示创建分区的大小，指定分区大小时直接默认起始扇区后，在last Sector size 处直接+400GB，最后 选择 w进行保存，这个不要忘记。这个时候分区已经分配好了，但是还不能使用，因为没有对该分区进行格式化操作，指定分区文件系统。 ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:3:0","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"指定分区文件系统并生效分区 选择不同文件系统的性能也不通，选择合适的文件系统对分区进行格式，在格式化文件系统前先更新分区表。重新扫描分区表需要借助partprobe命令。 partprobe命令是用于重新扫描系统上的分区表，以便内核能够识别新创建或修改的分区。当您在不重启系统的情况下对磁盘进行了分区或调整分区大小时，可以使用partprobe命令通知内核更新分区信息。 partprobe命令会读取分区表并向内核发送信号，让内核重新加载分区信息。这样，系统就能够识别到最新的分区信息，并使其可用于挂载和访问。 $ partprobe $ cat /proc/partitions major minor #blocks name 8 0 976729816 sda 8 1 1048576 sda1 8 2 137367552 sda2 8 3 412102655 sda3 [root@CENTOS7 temp]# mkfs.ext3 /dev/sda3 注意：partprobe命令需要root权限才能执行。在执行partprobe之后，您可以使用相应的命令（例如fdisk或lsblk）来检查分区是否已成功更新。 格式化完分区表后那么到此就完成了，我的目标是完成硬盘分区来创建物理卷。后续可以参考逻辑卷组管理文章 如： pvcreate /dev/sda3 vgcreate vgdata /dev/sda3 ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:3:1","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"设置开机挂载 设置开机挂载需同步至 /etc/fstab 文件中 示例 mkdir -p /var/lib/kubelet lvcreate -n lvkubelet -L +35G vgdata mkfs.xfs /dev/vgdata/lvkubelet echo \"/dev/mapper/vgdata-lvkubelet /var/lib/kubelet xfs defaults 0 0\" \u003e\u003e /etc/fstab mount -a ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:3:2","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"扩展一：fdisk 新增主分区和扩展分区的主要区别 主分区(Primary Partition)是硬盘上的基本分区,每个硬盘最多可以创建4个主分区。 扩展分区(Extended Partition)是一种特殊的主分区,它不能直接存储数据,仅可用于扩展更多的逻辑分区。每个硬盘只能创建1个扩展分区。 主分区可以直接存储数据,而扩展分区内需要再创建逻辑分区(Logical Partition)才能存储数据。 主分区对文件系统格式化的支持更广泛,可支持各种文件系统。扩展分区和逻辑分区仅支持部分文件系统。 主分区的访问效率稍高一些。扩展分区和逻辑分区的访问会稍微低一些。 如果没有空间限制,建议使用主分区。如果磁盘空间不足,可以使用扩展分区额外增加更多分区。 所以总的来说,主分区更基础和通用,扩展分区可以提供更多额外的分区数。选择时需要根据实际磁盘空间及使用需求来决定。 小结：有主选主，主分区更基础和通用 ","date":"2021-04-29","objectID":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/:3:3","tags":["linux"],"title":"fdisk 磁盘管理详解","uri":"/fdisk%E7%A3%81%E7%9B%98%E7%AE%A1%E7%90%86%E8%AF%A6%E8%A7%A3/"},{"categories":["linux"],"content":"背景 部署 k8s 时需要对磁盘进行分区划分，正常是集成是直接给创建好的，现场在沟通过程中只提供了一块磁盘。 本篇文章记录一下磁盘分区划分的原理和步骤，尽量选择大白话。 lvm\r","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:1:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"lvm 介绍 LVM——全称Logical Volume Manager，可以将多个硬盘和硬盘分区做成一个逻辑卷，并把这个逻辑卷作为一个整体来统一管理，动态对分区进行扩缩空间大小，提高磁盘管理灵活性。 在安装CentOS 7的过程中选择自动分区时，默认就是以LVM的方案安装的系统。 但是/boot分区必须独立出来，不能基于LVM创建。 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:2:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"三个概念 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:3:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"PV(Physical Volume) 物理卷 物理卷，Physical Volume，是LVM机制的基本存储设备，通常对应一个普通分区或是整个硬盘。 创建物理卷时，会在分区或磁盘头部创建一个用于记录LVM属性的保留区块，并把存储空间分割成默认大小为4MB的基本单元（Physical Extend，PE），从而构成物理卷。 普通分区先转换分区类型为8e；整块硬盘，可以将所有的空间划分为一个主分区再做调整 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:3:1","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"VG(Volume Group) 卷组 卷组，Volume Group，是由一个或多个物理卷组成的一个整体。可以动态添加、移除物理卷，创建时可以指定PE大小 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:3:2","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"LV(Logical Volume) 逻辑卷 逻辑卷，Logical Volume，建立在卷组之上，与物理卷没有直接关系。格式化后，即可挂载使用。 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:3:3","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"三者关系 lvm\r","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:4:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"lvm特点 LVM最大的特点就是可以对磁盘进行动态管理。因为逻辑卷的大小是可以动态调整的，而且不会丢失现有的数据。我们如果新增加了硬盘，其也不会改变现有上层的逻辑卷。作为一个动态磁盘管理机制，逻辑卷技术大大提高了磁盘管理的灵活性！ ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:5:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"工作原理 物理磁盘被格式化为PV，空间被划分为一个个的PE 不同的PV加入到同一个VG中，不同PV的PE全部进入到了VG的PE池内 LV基于PE创建，大小为PE的整数倍，组成LV的PE可能来自不同的物理磁盘 LV现在就直接可以格式化后挂载使用了 LV的扩充缩减实际上就是增加或减少组成该LV的PE数量，其过程不会丢失原始数据 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:6:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"讲个故事 上面工作原理可以做一个类比，集成提供的挂载磁盘可以理解为一堆小麦，pv的作用是将这一堆小麦打磨成面粉， 不管挂载多少个磁盘pv都可以做成面粉，一堆一堆的面粉(pv格式化的各个磁盘)可以加入到一个VG中， 这个VG可以理解为一个供销社将所有的面粉进行了纳管，当然也可以不同的面粉划分给不同的供销社, 意思就是四块磁盘A、B、C、D 通过打面机pv做成了四份面粉，A、C份面粉卖给了大脚供销社VG-DJ,C、D两份面粉卖给了小脚供销社 VG-XJ,现在亚洲舞王尼古拉斯赵四想要吃馒头，就选择了去大脚供销社去购买面粉，打算购买50斤面粉。这50斤面粉供销提供的lv, lv 一小份的面粉是来自于vg供销社而不是pv 面粉制造机器。下面会给出示例从小麦生产到赵四购买面粉。 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:7:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"操作 lvm\r","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:8:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"分区操作 1、 查看磁盘 通过fdisk -l可以看出集成提供了一块600多G的磁盘 /dev/xvdc $ fdisk -l .... Disk /dev/xvdc: 644.2 GB, 644245094400 bytes, 1258291200 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 2、 创建物理卷 pv 将小麦加工成面粉，原理是物理磁盘被格式化为PV，空间被划分为一个个的PE $ pvcreate /dev/xvdc Physical volume \"/dev/xvdc\" successfully created 3、 创建逻辑卷组 将面粉运输至大脚供销社vgdata,vgdata 是自定义的逻辑卷组名 $ vgcreate vgdata /dev/xvdc Volume group \"vgdata\" successfully created $ vgs VG #PV #LV #SN Attr VSize VFree centos 1 3 0 wz--n- \u003c99.00g 0 datavg 1 1 0 wz--n- \u003c400.00g 4.00m vgdata 1 0 0 wz--n- \u003c600.00g \u003c600.00g 4、创建逻辑卷 可以看的出来逻辑卷的创建是来自于逻辑卷组vg,后面的扩容也是从逻辑卷组进行扩容，这个步骤可以比喻为赵四去大脚超市 购买了35斤的面粉 mkdir -p /var/lib/kubelet lvcreate -n lvkubelet -L +35G vgdata mkfs.xfs /dev/vgdata/lvkubelet echo \"/dev/mapper/vgdata-lvkubelet /var/lib/kubelet xfs defaults 0 0\" \u003e\u003e /etc/fstab mount -a 查看逻辑卷是否挂载成功 $ df -h|grep kubelet /dev/mapper/vgdata-lvkubelet 35G 35M 35G 1% /var/lib/kubelet ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:8:1","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"相关操作命令 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:9:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"创建逻辑卷 Linux 系统使用逻辑卷来模拟物理分区，并在其中保存文件系统。 选 项 长选项名 描 述 -c –chunksize 指定快照逻辑卷的单位大小 -C –contiguous 设置或重置连续分配策略 -i –stripes 指定条带数 -I –stripesize 指定每个条带的大小 -l –extents 指定分配给新逻辑卷的逻辑区段数，或者要用的逻辑区段的百分比 -L –size 指定分配给新逻辑卷的硬盘大小 -M –persistent 让次设备号一直有效 -n –name 指定新逻辑卷的名称 -p –permission 为逻辑卷设置读/写权限 -r –readahead 设置预读扇区数 -R –regionsize 指定将镜像分成多大的区 -s snapshot 创建快照逻辑卷 -Z –zero 将新逻辑卷的前1 KB数据设置为零 -m –mirrors 创建逻辑卷镜像 空 –minor 指定设备的次设备号 参数很多，其实常用的就几个 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:9:1","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"指定逻辑卷大小 如逻辑卷组 Vol1 lvcreate -n lvtest -L +32G Vol1 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:9:2","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"创建文件系统 此时只有逻辑卷没有文件系统，需要给逻辑卷指定文件系统 这里的文件系统指定的是ext4，公司生产环境中使用的是 xfs 文件系统如mkfs.xfs /dev/vgdata/lvkubelet，以下示例使用的是ext4文件系统，不同文件系统的差别请自行了解，这里不做说明 sudo mkfs.ext4 /dev/Vol1/lvtest mke2fs 1.41.12 (17-May-2010) Filesystem label= OS type: Linux Block size=4096 (log=2) Fragment size=4096 (log=2) Stride=0 blocks, Stripe width=0 blocks 131376 inodes, 525312 blocks 26265 blocks (5.00%) reserved for the super user First data block=0 Maximum filesystem blocks=541065216 17 block groups 32768 blocks per group, 32768 fragments per group 7728 inodes per group Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912 Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 28 mounts or 180 days, whichever comes first.Use tune2fs -c or -i to override. ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:10:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"目录挂载 在创建了文件系统后需要将该逻辑卷挂载到指定的虚拟目录下，就跟他是物理分区一样 sudo mount /dev/Vol1/lvtest /mnt/my_partition 命令解释是将逻辑卷挂载到指定虚拟目录上 设置挂载永久生效，否则机器重启后会丢失掉 注意修改名称 echo \"/dev/mapper/$vgname-lvdocker /var/lib/docker xfs defaults 0 0\" \u003e\u003e /etc/fstab ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:10:1","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"修改LVM 命 令 功 能 vgchange 激活和禁用卷组 vgremove 删除卷组 vgextend 将物理卷加到卷组中 vgreduce 从卷组中删除物理卷 lvextend 增加逻辑卷的大小 lvreduce 减小逻辑卷的大小 通过以上命令可以完全控制你的LVM环境 如扩容 扩容前先取消挂载 umount /mnt/my_partition # 对/dev/Vol1/lvtest 增加1G lvextend -L +1G /dev/Vol1/lvtest -r # 重新挂载设备，并查看挂载状态 mount -a df -h ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:10:2","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"LVM 命令汇总 功能 pv命令 vg命令 lv命令 扫描 pvscan vgscan lvscan Create创建 pvcreate vgcreate lvcreate Display显示 pvdisplay vgdisplay lvdisplay Remove移除 pvremove vgremove lvremove Extend扩展 - vgextend lvextend Reduce减少 - vgreduce lvreduce 关于收缩 LVM 逻辑卷，可以参考上面的命令lvreduce。这会涉及到数据安全性问题，谨慎使用 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:11:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"小结 就是三步走，创建物理卷，创建卷组，创建逻辑卷，然后格式化并挂载使用。扩容的话，没有挂载使用，就正常扩，然后格式化挂载，已经挂载使用了，扩容后在线调整生效。 ","date":"2021-03-18","objectID":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/:12:0","tags":["linux"],"title":"lvm 逻辑卷组管理","uri":"/lvm%E9%80%BB%E8%BE%91%E5%8D%B7%E7%BB%84%E7%AE%A1%E7%90%86/"},{"categories":["linux"],"content":"简介 之所以能用到这个命令，是由于很多 linux 命令不支持用管道传递参数，xargs 可以理解为参数转换器，例如 #错误指令 find /sbin -perm +700 | ls -l #正确指令 find /sbin -perm +700 |xargs ls -l 通常Linux命令可以用|首尾相连，上一个命令的 stdout 连接到下一个命令的 stdin。但是有些命令，比如ls、rm等，是从命令行参数接受输入的。这时候如果想把上一个命令的输出传给它们，就不好办了。所以就有了xargs。 简单而言，xargs可以把从 stdin 接受到的输入，按空白符或加车符分隔开，然后依次作为参数去调用xargs后面的命令 xargs 默认的分隔符：空格 或 回车 ","date":"2021-02-28","objectID":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/:1:0","tags":["linux"],"title":"xargs原理及使用","uri":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/"},{"categories":["linux"],"content":"参数使用 想把所有.jpg文件删除，当然你可以 rm *.jpg，但是如果要递归操作所有子目录下的文件呢？ 可以这样： find . -name \"*.jpg\" | xargs rm 这样，所有被find找到的文件名，都会作为参数来调用rm命令了 对于大多数情况，这一行命令没有问题，但是如有些文件名中包含空格，就会有问题了。 xargs默认以空白符分隔接受到的输入，所以一个含有空格的文件名会被当做多个参数，分别传给rm。所以在处理文件名这类命令时，通常要这样 find . -name \"*.jpg\" -print0 | xargs -0 rm 这里的 -print0 是告诉find命令，在每个输出后面以’\\0’作为结束。-0是告诉xargs，使用’\\0’来分隔输入，而不是空白符。这样就避免出现问题了。 下面再考虑另一种情况，假设不是删除，而是想把符合要求的文件名都添加上后缀.bak怎么办？这时候需要这样 find . -name \"*.jpg\" -print0 | xargs -0 -I {} mv {} {}.bak 常用命令 其中的-I {} (initial arguments)是告诉xargs，后面的命令中，用{}表示占位符，将会被实际的参数替代。这样就行了。 其他有用的参数还有： -n 用于指定每次传递几个参数 -d 用于指定切分输入内容时，具体的分隔符 -a file 从文件中读入作为sdtin $ cat 1.txt aaa bbb ccc ddd a b $ xargs -a 1.txt echo aaa bbb ccc ddd a b -e flag 注意有的时候可能会是-E，flag必须是一个以空格分隔的标志，当xargs分析到含有flag这个标志的时候就停止 $ xargs -E 'ddd' -a 1.txt echo aaa bbb ccc $ cat 1.txt |xargs -E 'ddd' echo aaa bbb ccc -n num 后面加次数，表示命令在执行的时候一次用的argument的个数，默认是用所有的。 $ cat 1.txt |xargs -n 2 echo aaa bbb ccc ddd a b -p 操作具有可交互性，每次执行comand都交互式提示用户选择，当每次执行一个argument的时候询问一次用户 $ cat 1.txt |xargs -p echo echo aaa bbb ccc ddd a b ?...y aaa bbb ccc ddd a b $ cat 1.txt |xargs -p echo echo aaa bbb ccc ddd a b ?...n -t 表示先打印命令，然后再执行 $ cat 1.txt |xargs -t echo echo aaa bbb ccc ddd a b aaa bbb ccc ddd a b 占位 -i 或者是-I，这得看linux支持了，将xargs的每项名称，一般是一行一行赋值给{}，可以用{}代替。小写的-i带参数时和大写的-I是一模一样的，小写的-i可以不带参数，这时候相当于大写的-I {}。 不过手册里面不建议使用小写的-i，可能会有什么问题 xargs加上-I (这里手册建议使用大写的-I)后就可以用 {}表示管道传过来的参数放到该位置 $ ls 1.txt 2.txt 3.txt log.xml $ ls *.txt | xargs -t -i mv {} {}.bak mv 1.txt 1.txt.bak mv 2.txt 2.txt.bak mv 3.txt 3.txt.bak $ ls 1.txt.bak 2.txt.bak 3.txt.bak log.xml 空参数传递 -r no-run-if-empty 如果没有要处理的参数传递给xargs xargs 默认是带空参数运行一次，如果你希望无参数时，停止 xargs，直接退出，使用 -r 选项即可，其可以防止xargs 后面命令带空参数运行报错 $ echo \"\"|xargs -t mv mv mv: missing file operand Try `mv --help' for more information. $ echo \"\"|xargs -t -r mv #直接退出 -s num xargs后面那个命令的最大命令行字符数(含空格) -L 从标准输入一次读取num行送给Command命令 ，-l和-L功能一样 $ cat 1.txt.bak aaa bbb ccc ddd a b ccc dsds $ cat 1.txt.bak |xargs -L 4 echo aaa bbb ccc ddd a b ccc dsds $ cat 1.txt.bak |xargs -L 1 echo aaa bbb ccc ddd a b ccc dsds -d delim 分隔符，默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符 $ cat 1.txt.bak aaa@ bbb ccc@ ddd a b $ cat 1.txt.bak |xargs -d '@' echo aaa bbb ccc ddd a b ","date":"2021-02-28","objectID":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/:2:0","tags":["linux"],"title":"xargs原理及使用","uri":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/"},{"categories":["linux"],"content":"xargs 结合find 使用 举例 $find . -name \"install.log\" -print | cat ./install.log #显示从管道传来的内容，仅仅作为字符串来处理 $find . -name \"install.log\" -print | xargs cat aaaaaa #将管道传来的内容作为文件，交给cat执行。也就是说，该命令执行的是如果存在install.log，那么就打印出这个文件的内容。 来看看xargs命令是如何同find命令一起使用的，并给出一些例子。 1、在当前目录下查找所有用户具有读、写和执行权限的文件，并收回相应的写权限： # find . -perm -7 -print | xargs chmod o-w 2、查找系统中的每一个普通文件，然后使用xargs命令来测试它们分别属于哪类文件 # find . -type f -print | xargs file ./liyao: empty 3、尝试用rm 删除太多的文件，你可能得到一个错误信息：/bin/rm Argument list too long. 用xargs 去避免这个问题 $find ~ -name ‘*.log’ -print0 | xargs -i -0 rm -f {} 4、查找所有的jpg 文件，并且压缩它 # find / -name *.jpg -type f -print | xargs tar -cvzf images.tar.gz 5、拷贝所有的图片文件到一个外部的硬盘驱动 # ls *.jpg | xargs -n1 -i cp {} /external-hard-drive/directory ","date":"2021-02-28","objectID":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/:3:0","tags":["linux"],"title":"xargs原理及使用","uri":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/"},{"categories":["linux"],"content":"总结 命令 解释 echo -n “hello#word” |xargs -d “#” 适用#号进行拆分 echo “hello word”|xargs -n 1 一次传递一个参数 echo -n “hello#word”|xargs -d “#” -t 直接输出打印命令无用户交互 echo -n “hello#word”|xargs -d “#” -p 用户交互选择是否打印命令并且输出打印命令 echo -n “hello#word”|xargs -I {} echo {} 占位符initial arguments echo -n “”|xargs -r echo 小写r(no run if empty),如果传递的参数为空则不执行 ","date":"2021-02-28","objectID":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/:4:0","tags":["linux"],"title":"xargs原理及使用","uri":"/xargs%E5%8E%9F%E7%90%86%E5%8F%8A%E4%BD%BF%E7%94%A8/"},{"categories":["MySQL"],"content":"背景 数据是企业生产的重中之重，保护数据安全，防止数据丢失很关键 ","date":"2020-08-17","objectID":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/:1:0","tags":["MySQL"],"title":"MySQL 数据库备份和还原","uri":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/"},{"categories":["MySQL"],"content":"使用 mysqldump 进行备份 备份的方案场景，列举部分场景 只备份某一个库，不要里面的表 备份所有的库，和所有的表，但是不要表中的数据，只备份它的结构(创建的SQL) 只要某一个库，以及这个库中的所有表，以及所有数据 只要某一个库，里面的某一张表 mysqldump 相关命令\rmysqldump客户端可用来转储数据库或搜集数据库进行备份或将数据转移到另一个SQL服务器(不一定是一个MySQL服务器)。转储包含创建表和/或装载表的SQL语句。 如果你在服务器上进行备份，并且表均为MyISAM表，应考虑使用mysqlhotcopy，因为可以更快地进行备份和恢复。参见8.9节，“mysqlhotcopy：数据库备份程序”。 有3种方式来调用mysqldump： shell\u003e mysqldump [options] db_name [tables] shell\u003e mysqldump [options] —database DB1 [DB2 DB3…] shell\u003e mysqldump [options] –all–database 如果没有指定任何表或使用了—database或–all–database选项，则转储整个数据库。 要想获得你的版本的mysqldump支持的选项，执行mysqldump —help。 如果运行mysqldump没有–quick或–opt选项，mysqldump在转储结果前将整个结果集装入内存。如果转储大数据库可能会出现问题。该选项默认启用，但可以用–skip-opt禁用。 如果使用最新版本的mysqldump程序生成一个转储重装到很旧版本的MySQL服务器中，不应使用–opt或-e选项。 mysqldump支持下面的选项： —help，-？ 显示帮助消息并退出。 –add-drop–database 在每个CREATE DATABASE语句前添加DROP DATABASE语句。\r–add-drop-tables 在每个CREATE TABLE语句前添加DROP TABLE语句。\r–add-locking 用LOCK TABLES和UNLOCK TABLES语句引用每个表转储。重载转储文件时插入得更快。参见7.2.16节，“INSERT语句的速度”。\r–all–database，-A 转储所有数据库中的所有表。与使用---database选项相同，在命令行中命名所有数据库。\r–allow-keywords 允许创建关键字列名。应在每个列名前面加上表名前缀。\r—comments[={0|1}] 如果设置为 0，禁止转储文件中的其它信息，例如程序版本、服务器版本和主机。--skip—comments与---comments=0的结果相同。 默认值为1，即包括额外信息。\r–compact 产生少量输出。该选项禁用注释并启用--skip-add-drop-tables、--no-set-names、--skip-disable-keys和--skip-add-locking选项。\r–compatible=name 产生与其它数据库系统或旧的MySQL服务器更兼容的输出。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options或者no_field_options。要使用几个值，用逗号将它们隔开。这些值与设置服务器SQL模式的相应选项有相同的含义。参见5.3.2节，“SQL服务器模式”。\r该选项不能保证同其它服务器之间的兼容性。它只启用那些目前能够使转储输出更兼容的SQL模式值。例如，--compatible=oracle 不映射Oracle类型或使用Oracle注释语法的数据类型。\r–complete-insert，-c 使用包括列名的完整的INSERT语句。\r–compress，-C 压缩在客户端和服务器之间发送的所有信息（如果二者均支持压缩）。\r–create-option 在CREATE TABLE语句中包括所有MySQL表选项。\r—database，-B 转储几个数据库。通常情况，mysqldump将命令行中的第1个名字参量看作数据库名，后面的名看作表名。使用该选项，它将所有名字参量看作数据库名。CREATE DATABASE IF NOT EXISTS db_name和USE db_name语句包含在每个新数据库前的输出中。\r—debug[=debug_options]，-# [debug_options] 写调试日志。debug_options字符串通常为'd:t:o,file_name'。\r–default-character-set=charset 使用charsetas默认字符集。参见5.10.1节，“数据和排序用字符集”。如果没有指定，mysqldump使用utf8。\r–delayed-insert 使用INSERT DELAYED语句插入行。\r–delete-master-logs 在主复制服务器上，完成转储操作后删除二进制日志。该选项自动启用--master-data。\r–disable-keys，-K 对于每个表，用/*!40000 ALTER TABLE tbl_name DISABLE KEYS */;和/*!40000 ALTER TABLE tbl_name ENABLE KEYS */;语句引用INSERT语句。这样可以更快地装载转储文件，因为在插入所有行后创建索引。该选项只适合MyISAM表。\r–extended-insert，-e 使用包括几个VALUES列表的多行INSERT语法。这样使转储文件更小，重载文件时可以加速插入。\r–fields-terminated-by=…，–fields-enclosed-by=…，–fields-optionally-enclosed-by=…，–fields-escaped-by=…，–行-terminated-by=… 这些选项结合-T选项使用，与LOAD DATA INFILE的相应子句有相同的含义。参见13.2.5节，“LOAD DATA INFILE语法”。\r–first-slave，-x 不赞成使用，现在重新命名为--lock-all-tables。\r–flush-logs，-F 开始转储前刷新MySQL服务器日志文件。该选项要求RELOAD权限。请注意如果结合--all--database(或-A)选项使用该选项，根据每个转储的数据库刷新日志。例外情况是当使用--lock-all-tables或--master-data的时候：在这种情况下，日志只刷新一次，在所有 表被锁定后刷新。如果你想要同时转储和刷新日志，应使用--flush-logs连同--lock-all-tables或--master-data。\r–force，-f 在表转储过程中，即使出现SQL错误也继续。 –host=host_name，-h host_name 从给定主机的MySQL服务器转储数据。默认主机是localhost。 –hex-blob 使用十六进制符号转储二进制字符串列(例如，‘abc’ 变为0x616263)。影响到的列有BINARY、VARBINARY、BLOB。 –lock-all-tables，-x 所有数据库中的所有表加锁。在整体转储过程中通过全局读锁定来实现。该选项自动关闭–single-transaction和–lock-tables。 –lock-tables，-l 开始转储前锁定所有表。用READ LOCAL锁定表以允许并行插入MyISAM表。对于事务表例如InnoDB和BDB，–single-transaction是一个更好的选项，因为它不根本需要锁定表。 请注意当转储多个数据库时，–lock-tables分别为每个数据库锁定表。因此，该选项不能保证转储文件中的表在数据库之间的逻辑一致性。不同数据库表的转储状态可以完全不同。 –master-data[=value] 该选项将二进制日志的位置和文件名写入到输出中。该选项要求有RELOAD权限，并且必须启用二进制日志。如果该选项值等于1，位置和文件名被写入CHANGE MASTER语句形式的转储输出，如果你使用该SQL转储主服务器以设置从服务器，从服务器从主服务器二进制日志的正确位置开始。如果选项值等于2，CHANGE MASTER语句被写成SQL注释。如果value被省略，这是默认动作。 –master-data 选项启用–lock-all-tables，除非还指定–single-transaction(在这种情况下，只在刚开始转储时短时间获得全局读锁定。又见–single-transaction。在任何一种情况下，日志相关动作发生在转储时。该选项自动关闭–lock-tables。 –no-create-db，-n 该选项禁用CREATE DATABASE /*!32312 IF NOT EXISTS*/ db_name语句，如果给出---database或--all--database选项，则包含到输出中。\r–no-create-info，-t 不写重新创建每个转储表的CREATE TABLE语句。\r–no-data，-d 不写表的任何行信息。如果你只想转储表的结构这很有用。\r–opt 该选项是速记；等同于指定 --add-drop-tables--add-locking --create-option --disable-keys--extended-insert ","date":"2020-08-17","objectID":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/:2:0","tags":["MySQL"],"title":"MySQL 数据库备份和还原","uri":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/"},{"categories":["MySQL"],"content":"数据备份的各个场景 场景一：备份所有数据库实例 mysqldump -u root -p --all-databases\u003e all_databases.sql 场景二：导出某个或某几个数据库 数据库为myblog mysqldump -u root -p --databases test mysql \u003e db_test_and_mysql.sql 场景三：导出一张表 数据库为myblog，表为wp_users mysqldump -u dbadmin -p myblog wp_users\u003e blog_users.sql 场景四：导出整个数据库结构 导出一个数据库结构 -d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table mysqldump -u dbadmin -p -d --add-drop-table myblog \u003e blog_struc.sql 场景五：导出数据库中的一张表结构 数据库为myblog，表为wp_users mysqldump -u dbadmin -p -d --add-drop-table myblog wp_users\u003e blog_users_struc.sql 场景六：导出数据库一个表数据,不包括表结构 数据库为test，表为person mysqldump -u root -p -t test person \u003e order.sql ","date":"2020-08-17","objectID":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/:2:1","tags":["MySQL"],"title":"MySQL 数据库备份和还原","uri":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/"},{"categories":["MySQL"],"content":"数据还原 数据导出就有导入还原,库实例为test,将备份的数据进行还原 mysql -h127.0.0.1 -uroot -P3306 -p test \u003c order.sql ","date":"2020-08-17","objectID":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/:2:2","tags":["MySQL"],"title":"MySQL 数据库备份和还原","uri":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/"},{"categories":["MySQL"],"content":"通过 crontab 定时备份指定数据库脚本参考 脚本验证适用环境Centos7.x #!/bin/bash #set -e # 备份并清理，将mysql 表数据保留21天 dateDir=$(date +%Y%m%d)\"_all\" backUpFolder=/zpaasssd/zcm/mysql_171_grafana_dashboard_bkp host=10.10.168.67 port=20821 username=\"root\" password=\"password\" main(){ mkdir ${backUpFolder}/${dateDir} #rm -rf ${backUpFolder}/$(date -d \"-21 day\" +%Y%m%d)\"_all\".tar.gz if [[ -f ${backUpFolder}/$(date -d \"-21 day\" +%Y%m%d)\"_all\".tar.gz ]];then rm -rf ${backUpFolder}/$(date -d \"-21 day\" +%Y%m%d)\"_all\".tar.gz fi mysqldump -h $host -P $port -u ${username} -p${password} --databases zcm_grafana --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/zcm_grafana.sql mysqldump -h $host -P $port -u ${username} -p${password} --databases zcm_grafpub --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/zcm_grafpub.sql #mysqldump -h $host -P $port -u ${username} -p${password} --databases zcm_nms --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/zcm_nms.sql mysqldump -h $host -P $port -u ${username} -p${password} --databases zcm_task --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/zcm_task.sql mysqldump -h $host -P $port -u ${username} -p${password} --databases zcm_dialing --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/zcm_dialing.sql #mysqldump -h $host -P $port -u ${username} -p${password} --databases zcm_idocs --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/zcm_idocs.sql mysqldump -h $host -P $port -u ${username} -p${password} --databases nms_sla --default-character-set utf8 \u003e ${backUpFolder}/${dateDir}/nms_sla.sql #压缩备份文件,清理原始文件 cd ${backUpFolder} tar zcvf ${dateDir}.tar.gz ${dateDir} rm -rf ${backUpFolder}/${dateDir} # 解压 #tar zxvf /zpaasssd/zcm/mysql_171_grafana_dashboard_bkp.tar.gz echo success } main ","date":"2020-08-17","objectID":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/:3:0","tags":["MySQL"],"title":"MySQL 数据库备份和还原","uri":"/mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E5%92%8C%E8%BF%98%E5%8E%9F/"},{"categories":["Ansible"],"content":"1. Ansible 简介 Ansible是一个开源的IT自动化运维工具，用于配置管理、应用部署、任务执行和持续交付。其设计目标是简单易用，以帮助系统管理员更轻松地管理服务器基础设施。Ansible通过YAML（YAML Ain’t Markup Language）格式的清晰易懂的文本定义配置（称为Playbook），这种格式有助于用户快速编写和阅读任务及配置信息。 以下是Ansible的主要特点： 简单易用：Ansible无需复杂的环境搭建和编程语言知识，采用YAML语法编写任务，具有高度可读性。部署在目标主机上不需要安装Agent，因此不会增加额外负担。 代理无需安装：Ansible通过SSH协议与目标主机通信，在目标主机上无需安装额外的代理程序，减轻了系统负担。 幂等性：Ansible任务的执行具有幂等性，即同一任务被多次执行时，结果相同且不会产生副作用。这使得Ansible在多次执行任务时，始终保持系统的稳定。 模块化：Ansible自带数百个可用于各种任务的模块，如文件管理、软件包安装、系统服务管理、网络设备配置等。用户还可以编写自定义模块以实现特定功能。 配置管理：Ansible Playbook中的变量、模板和条件处理等功能，使得配置文件可以轻松实现参数化，同时满足多种环境和主机组的需求。 任务编排：使用Ansible Playbook，用户可以编排一系列任务并按顺序执行。 社区支持与生态系统：Ansible有一个庞大且积极的社区，提供大量的模块、插件、教程和技术支持。此外，Ansible已被许多知名企业采用，持续地完善和拓展功能。 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:1:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"1.1 使用场景 Ansible作为一个功能强大的自动化运维工具，应用广泛，以下是一些典型的使用场景： 系统配置管理：使用Ansible，可以轻松地对目标系统进行配置，例如设置用户、组、文件权限、网络配置等。通过YAML格式的Playbook来管理配置，便于维护及版本控制。 软件包安装与升级：使用Ansible的软件包管理模块安装、升级或移除软件包。例如，可以针对大量服务器的软件升级、你也可以统一规划软件包的安装和版本。 一键部署应用与服务：通过Ansible Playbook实现应用程序和服务的快速、自动化部署。例如，部署Web服务、数据库、负载均衡器等。 持续集成与持续部署：将Ansible与持续集成工具（如Jenkins）结合，实现代码自动构建、测试和部署。可以大大提高软件开发和交付的效率。 定时任务管理：使用Ansible管理定时任务，例如创建、修改、删除Linux系统中的crontab任务。 网络设备管理：使用Ansible管理网络设备，包括路由器、交换机和防火墙等。可以快速地执行配置更改、故障排查和性能数据采集等任务。 扩展基础设施：适用于云计算环境中的基础设施管理，可以快速创建、配置和销毁虚拟机或容器，以满足不断变化的需求。 安全性管理：使用Ansible进行安全加固和审计，例如管理SSL证书、防火墙规则、OS安全补丁等。 监控与告警：通过Ansible部署和配置各种监控工具（如Nagios、Zabbix等），实现对服务器、应用程序和网络设备的监控与告警。 跨平台管理：Ansible支持多种操作系统，可以在Linux、Windows、MacOS等系统上实现统一的配置管理和任务执行。 综上所述，Ansible凭借其灵活性和可扩展性，可广泛应用于各种场景。根据具体需求，用户可以开发自定义模块和插件，进一步实现自动化运维的高效性。 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:1:1","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"1.2 工作原理 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:1:2","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"1.3 命令执行过程 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:1:3","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"1.4 ansible 管理命令 Ansible命令行执行方式有Ad-Hoc、Ansible-playbook两种方式： Ad-Hoc主要用于临时命令的执行。 Ansibel-playbook可以理解为Ad-Hoc的集合，通过一定的规则编排在一起。 两者的操作也极其简便，且提供了如with_items、failed_when、changed_when、until、ignore_errors等丰富的逻辑条件和Dry-run的Check Mode。但在Chceck Mode下并不真正执行命令，即将执行的操作不会对端服务器产生任何影响，只模拟命令的执行过程是否能正常执行。 Ansible管理命令有： ansible：这个命令是日常工作中使用率非常高的命令之一，主要用于临时一次性操作。 ansible-doc：Ansible模块文档说明，针对每个模块都有详细的用法说明和应用案例介绍。 ansible-galaxy：可以简单的理解为Github或PIP的功能，是Ansible官方一个分享role的功能平台。可以通过ansible-galaxy命令很简单的实现role的分享和安装。 ansible-playbook：是日常应用中使用频率最高的命令，其工作机制是，通过读取预先编写好的playbook文件实现批量管理。 ansible-pull：Ansible的另一种工作模式（pull模式），Ansible默认使用push模式。 ansible-vault：主要用于配置文件加密。 ansible-console：让用户可以在ansible-console虚拟出来的终端上像Shell一样使用Ansible内置的各种命令。 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:1:4","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"2. install 部署 dns resolve(可选) install ansible 安装 $ yum install epel-release -y $ yum install ansible -y 检测部署是否完成 rpm -ql ansible 列出所有文件 rpm -qc ansible 查看配置文件 ansible --help 查看ansible帮助 ansible-doc -l 查看ansible所有模块 ansible-doc -s yum 查看ansible yum模块,并查看其功能 注意： 部分yum方式安装ansible可能会出错，可以选择采用rpm进行安装 采坑-rpm安装的，运行会提示缺少argparse modules，用yum install python-argparse安装即可 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:2:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"2.1 免密配置(可选) $ ssh-keygen -t dsa $ ssh-copy-id 192.168.143.102 $ ssh 192.168.143.102 Last login: Tue Jan 7 11:44:30 2025 from 192.168.143.1 $ ip a|grep 192.168.143.102 inet 192.168.143.102/24 brd 192.168.143.255 scope global ens32 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:2:1","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"3. Ansible 基础 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:3:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"3.1 配置主机清单 root@localhost:~$ vim /etc/ansible/hosts ... ## db-[99:101]-node.example.com 192.168.143.102 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:3:1","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"3.2 测试 ansible 模块 指定ping模块 $ ansible localhost -m ping localhost | SUCCESS =\u003e { \"changed\": false, \"ping\": \"pong\" } root@localhost:~$ ansible 192.168.143.102 -m ping 192.168.143.102 | SUCCESS =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": false, \"ping\": \"pong\" } # 如果目标主机未设置免密登录，并设置简洁输出 root@localhost:~$ ansible 192.168.143.102 -m ping -o -u root -k SSH password: 192.168.143.102 | SUCCESS =\u003e {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/bin/python\"}, \"changed\": false, \"ping\": \"pong\"} ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:3:2","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"4. Inventory 主机清单 增加主机组 root@localhost:~$ vim /etc/ansible/hosts ... ## db-[99:101]-node.example.com [webserver] host1 host2 host3 host4 测试组 $ ansible webserver -m ping -o -u root -k SSH password: 192.168.143.102 | SUCCESS =\u003e {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/bin/python\"}, \"changed\": false, \"ping\": \"pong\"} 增加用户名和密码 root@localhost:~$ vim /etc/ansible/hosts ... ## db-[99:101]-node.example.com [webserver2] 192.168.143.102 ansible_ssh_user='root' ansible_ssh_pass='123456' 测试 root@localhost:~$ ansible webserver2 -m ping -o 192.168.143.102 | SUCCESS =\u003e {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/bin/python\"}, \"changed\": false, \"ping\": \"pong\"} 增加端口 root@localhost:~$ vim /etc/ansible/hosts ... ## db-[99:101]-node.example.com [webserver3] 192.168.143.102 ansible_ssh_user='root' ansible_ssh_pass='123456' ansible_ssh_port='22' 测试 root@localhost:~$ ansible webserver3 -m ping -o 192.168.143.102 | SUCCESS =\u003e {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/bin/python\"}, \"changed\": false, \"ping\": \"pong\"} 组：变量 root@localhost:~$ vim /etc/ansible/hosts ... ## db-[99:101]-node.example.com [webserver] host1 ansible_ssh_port='2222' # 定制化端口 host2 host3 host4 [webserver:vars] ansible_ssh_user='root' ansible_ssh_pass='123456' 子分组 root@localhost:~$ vim /etc/ansible/hosts ... ## db-[99:101]-node.example.com [tomcaterver] host1 host2 [mysqlterver] host3 host4 [productserver:children] 测试 $ ansible productserver -m ping -o 自定义主机列表 $ ansible -i /etc/ansible/hosts webserver -m ping -o 192.168.143.102 | SUCCESS =\u003e {\"ansible_facts\": {\"discovered_interpreter_python\": \"/usr/bin/python\"}, \"changed\": false, \"ping\": \"pong\"} Ansible 主机清单参数说明 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:4:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"6. Add-Hoc 点对点模式 内部指令常用模块 shell copy user 软件包管理 服务模块 文件模块 收集模块 fetch cron group script unarchive copy模块案例 $ touch /tmp/1.txt $ ansible-doc copy $ ansible webserver -m copy -a 'src=/tmp/1.txt dest=/tmp/11.txt owner=root group=root mode=777' 192.168.143.102 | CHANGED =\u003e { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"checksum\": \"da39a3ee5e6b4b0d3255bfef95601890afd80709\", \"dest\": \"/tmp/11.txt\", \"gid\": 0, \"group\": \"root\", \"md5sum\": \"d41d8cd98f00b204e9800998ecf8427e\", \"mode\": \"0777\", \"owner\": \"root\", \"secontext\": \"unconfined_u:object_r:admin_home_t:s0\", \"size\": 0, \"src\": \"/root/.ansible/tmp/ansible-tmp-1736236277.53-3361-267624272356035/source\", \"state\": \"file\", \"uid\": 0 } ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:5:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"7. Role 角色扮演 Role则是在ansible中，playbooks的目标组织结构。将代码或文件进行模块化，成为roles的文件目录组织结构，易读，代码可重用，层次清晰。 自动化运维面试的时候问到ansible的会比较多一些，ansible问的更多的便是playbook了。 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"1. 目录结构 Ansible 的 Role 是一种将 Playbook 中的任务、变量、模板和文件组织在一起的方法，使得 Playbook 更加模块化、可重用和易于管理。每个 Role 通常对应一个特定的功能或服务（如 web server, database 等）。以下是一个典型的 Ansible Role 目录结构： roles/ └── myrole ├── defaults # 定义默认变量，这些变量可以在外部被覆盖。 │ └── main.yml # 默认变量文件，通常用于设置可选配置项。 ├── files # 存放需要复制到远程主机上的静态文件。 ├── handlers # 定义可以由任务触发的动作（如重启服务）。 │ └── main.yml # 处理程序文件，定义了当某些条件满足时执行的操作。 ├── meta # 包含角色元数据，如依赖关系、平台兼容性等。 │ └── main.yml # 元数据文件，指定角色依赖和其他属性。 ├── README.md # 说明文档，描述角色的功能、使用方法和注意事项。 ├── tasks # 角色的主要任务列表，定义要执行的具体操作。 │ └── main.yml # 必须存在的入口文件，包含了一系列的任务定义。其它目录和文件为可选 ├── templates # 存放 Jinja2 模板文件，用于生成配置文件等文本内容。 ├── tests # 测试用例目录，通常包括测试 Playbook 和库存文件。 │ ├── inventory # 测试环境的库存文件，定义了测试目标主机。 │ └── test.yml # 测试 Playbook 文件，用于验证角色是否按预期工作。 └── vars # 定义不可覆盖的变量，通常用于强制性的配置选项。 └── main.yml # 变量文件，包含了角色所需的固定配置值。 各个目录和文件的作用 defaults/: 此目录下的 main.yml 文件包含了该角色的默认变量，这些变量可以很容易地被用户自定义或覆盖。它们通常是可选的配置项，提供了合理的默认值。 files/: 这个目录用来存放那些不需要经过模板处理就可以直接复制到远程节点上的静态文件。 handlers/: 在这里定义的是处理程序，它们是一些特殊的任务，只有当其他任务通知它们时才会被执行。常见的例子是重启服务，因为这通常不应该在每次运行时都发生，而是在配置更改后才需要。 meta/: 包含有关角色的元数据信息，比如它可能依赖的其他角色、适用的操作系统类型等。这对于确保角色正确安装其依赖项非常重要。 README.md: 提供关于角色的详细信息，例如如何使用它、它的功能是什么以及任何重要的提示或警告。 tasks/: 这是角色的核心部分，main.yml 文件中列出了所有要执行的任务。这是当你将角色添加到 Playbook 中时会被调用的地方。 templates/: 如果你需要创建基于模板的文件（如配置文件），那么这些模板应该放在这个目录下。Ansible 使用 Jinja2 模板引擎来渲染这些模板，并根据需要插入变量值。 tests/: 该目录包含了用于测试角色的资源，包括一个模拟的库存文件和一个或多个测试 Playbook。这对于开发过程中进行单元测试和集成测试非常有用。 vars/: 此目录中的 main.yml 文件定义了角色所需的变量，这些变量应该是不可轻易覆盖的，因为它们代表了角色内部的关键配置。 通过这样的组织方式，Ansible 的 Roles 不仅易于理解，而且也促进了代码的重用性和模块化设计。 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:1","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"2. 编写任务 创建一个 Role 你可以手动创建上述目录结构，或者使用 ansible-galaxy 命令来自动创建。以下是使用 ansible-galaxy 创建 Role 的步骤： 打开命令行工具。 使用 ansible-galaxy role init 来创建一个新的 Role。例如，如果你想创建一个名为 myrole 的 Role，你会运行： root@localhost:/etc/ansible$ ansible-galaxy role init myrole^C root@localhost:/etc/ansible$ tree roles/ roles/ └── myrole ├── defaults │ └── main.yml ├── files ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── README.md ├── tasks │ └── main.yml ├── templates ├── tests │ ├── inventory │ └── test.yml └── vars └── main.yml 9 directories, 8 files ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:2","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"3. 准备配置文件 files 目录下是存放静态文件，templates 目录下是存放模板文件，文件中设置占位符一般是结合vars 变量和setup模块进行组合使用。 在files 目录下创建一个静态文件，通过处理程序将其拷贝到远程主机 $ cd files $ touch ansible.txt 在templates 目录下创建一个模板文件 $ cd templates $ vim ansible-templates.txt the ntp_port is {{ ntp_port }} ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:3","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"4. 编写变量 变量结合模板文件一起使用，方便动态修改 $ cd vars $ vim main.yml ntp_port: 8888 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:4","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"5. 编写处理程序 处理程序tasks可以理解为playbook的导演 $ cd tasks $ vim main.yml - name: \"Tasks in role demo\" debug: msg=\"Simple role example\" - name: \"copy file to dest host\" copy: src: ansible.txt dest: /tmp/ansible.txt mode: 0644 - name: \"template file to dest host\" template: src: ansible-template.txt dest: /tmp/ansible-template.txt owner: root group: root mode: 0644 ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:5","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"6. 剧本编写 site.yaml 可以理解为剧本，可以设置每个role的执行顺序，例如冯小刚导演的电影1942的剧本就是刘震云编剧的 $ cd /etc/ansible/role $ vim site.yaml --- # This playbook install base_vm - hosts: nodes #gather_facts: yes pre_tasks: - name: pre task shell: echo \"hello\" in pre_tasks roles: - role: myrole #when: \"ansible_os_family == 'RedHat'\" # 条件判断可以将其注释掉，结合gather_facts一起使用 tasks: - name: tasks in site.yaml debug: msg=\"this is task in site.yaml\" post_tasks: - name: post task shell: echo \"goodbye\" in post_tasks ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:6","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"7. 测试 对playbook进行语法测试 $ ansible-playbook site.yaml --syntax-check playbook: site.yaml # 查看输出的细节 $ ansible-playbook -i hosts site.yml --verbose # 查看该脚本影响哪些主机 $ ansible-playbook -i hosts site.yml --list-hosts # 并行执行脚本 $ ansible-playbook site.yml -f 10 # Dry run $ ansible-playbook site.yml --check ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:6:7","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["Ansible"],"content":"参考 Ansible 社区文档 Ansible中文权威指南 自动化运维-Ansible03-Ansible常用命令和模块 github官网 ansible examples 运维自动化之ANSIBLE Ansible Galaxy ","date":"2020-06-27","objectID":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/:7:0","tags":["Ansible"],"title":"Ansible 部署及快速入门","uri":"/ansible%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"categories":["learning method"],"content":"\r摘要\r欲罢不能的学习需要一个良好的学习情绪和专注力。 下面主要是学习节奏的控制 改变学习心态，而不是在电脑旁坐了12小时，其实只是学了2个小时\r","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:0:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"1. 如何选择学习时间 选择在早上和下午学习，尽量不要选择在晚上学习(保护发际线)。 在睡前大量用脑，会让脑袋即紧张有疲劳，还会影响睡眠质量。 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:1:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"2.把时间分成几段 假设你今天早上打算学6个小时，可以安排早晨2.5小时， 下午安排2小时，晚上安排1.5小时。把尽量多的任务放到早上和中午去完成。 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:2:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"3. 吃饭的时候保持悠闲和放松 吃不好心情会变差 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:3:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"4. 继续打碎时间 把两个小时左右的时间砍成不足一个小时的区间。学一个小时，就放松 十分钟，这十分钟一定要把自己的注意力转移到别的事情上。看点不相干的书啊，回个微信啥的 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:4:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"5. 转换地方 在一个地方学的久了会让自己感觉时间过得很慢,不要在一个地方呆太久，多踩几个作案地点 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:5:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"6. 换材料 假设学习了一个小时英语，下一个小时就学别的东西，换材料有助于减轻对学习的厌恶感和疲劳度，主要是为了维持新鲜感 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:6:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["learning method"],"content":"7. 中午或者下午休息时间千万不要去学习。 不学习也是伟大计划中的一部分,要和学习一样认真执行，不好好吃饭想学习的事，会让你情绪很紧绷。 整个学习的方法就是保持时间的超级集中点，切换打断带来轻松感，空隙娱乐带来 奖励感，减少体感时间的长度来减少心理压力。 另外还有一点我认为最重要的是保持专注。 ","date":"2020-06-19","objectID":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/:7:0","tags":["learning method"],"title":"欲罢不能的学习方法策略","uri":"/%E6%AC%B2%E7%BD%A2%E4%B8%8D%E8%83%BD%E7%9A%84%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E7%AD%96%E7%95%A5/"},{"categories":["Ansible"],"content":"Ansible 中文权威指南 Ansible 知识图谱\r","date":"2020-05-17","objectID":"/ansible%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/:0:0","tags":["Ansible"],"title":"Ansible 知识图谱","uri":"/ansible%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"},{"categories":["Shell"],"content":"在大部份的 UNIX 系统，三种著名且广被支持的 shell 是 Bourne shell（AT\u0026T shell，在 Linux 下是 BASH）、C shell（Berkeley shell，在 Linux 下是 TCSH）和 Korn shell（Bourne shell 的超集）。 这三种 shell 在交谈（interactive）模式下的表现相当类似，但作为命令文件语言时，在语法和执行效率上就有些不同了。 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:0:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"bash Bourne shell 是标准的 UNIX shell，以前常被用来做为管理系统之用。大部份的系统管理命令文件，例如 rc start、stop 与 shutdown 都是 Bourne shell 的命令档，且在单一使用者模式（single user mode）下以 root 签入时它常被系统管理者使用。 Bourne shell 是由 AT\u0026T 发展的，以简洁、快速著名。 Bourne shell 提示符号的默认值是 $。 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:1:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"csh C shell 是柏克莱大学（Berkeley）所开发的，且加入了一些新特性，如命令列历程（history）、别名（alias）、内建算术、档名完成（filename completion）、和工作控制（job control）。 对于常在交谈模式下执行 shell 的使用者而言，他们较喜爱使用 C shell；但对于系统管理者而言，则较偏好以 Bourne shell 来做命令档，因为 Bourne shell 命令档比 C shell 命令档来的简单及快速。C shell 提示符号的默认值是 %。 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:2:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"ksh Korn shell 是 Bourne shell 的超集（superset），由 AT\u0026T 的 David Korn 所开发。它增加了一些特色，比 C shell 更为先进。Korn shell 的特色包括了可编辑的历程、别名、函式、正规表达式万用字符（regular expression wildcard）、内建算术、工作控制（job control）、共作处理（coprocessing）、和特殊的除错功能。Bourne shell 几乎和 Korn shell 完全向上兼容（upward compatible），所以在 Bourne shell 下开发的程序仍能在 Korn shell 上执行。Korn shell 提示符号的默认值也是 $。 在 Linux 系统使用的 Korn shell 叫做 pdksh，它是指 Public Domain Korn Shell。除了执行效率稍差外，Korn shell 在许多方面都比 Bourne shell 为佳；但是，若将 Korn shell 与 C shell 相比就很困难，因为二者在许多方面都各有所长，就效率和容易使用上看，Korn shell 是优于 C shell，相信许多使用者对于 C Shell 的执行效率都有负面的印象。 在 shell 的语法方面，Korn shell 是比较接近一般程序语言，而且它具有子程序的功能及提供较多的资料型态。至于 Bourne shell，它所拥有的资料型态是三种 shell 中最少的，仅提供字符串变量和布尔型态。在整体考量下 Korn shell 是三者中表现最佳者，其次为 C shell，最后才是 Bourne shell，但是在实际使用中仍有其它应列入考虑的因素，如速度是最重要的选择时，很可能应该采用 Bourne shell，因它是最基本的 shell，执行的速度最快。 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:3:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"tcsh tcsh 是近几年崛起的一个免费软件（Linux 下的 C shell 其实就是使用 tcsh）执行，它虽然不是 UNIX 的标准配备，但是从许多地方您都可以下载到它。如果您是 C shell 的拥护者，笔者建议不妨试试 tcsh，因为您至少可以将它当作是 C shell 来使用。如果您愿意花点时间学习，您还可以享受许多它新增的优越功能，例如： tcsh 提供了一个命令列（command line）编辑程序。 提供了命令列补全功能。 提供了拼字更正功能。它能够自动检测并且更正在命令列拼错的命令或是单字。 危险命令侦测并提醒的功能，避免您一个不小心执行了rm* 这种杀伤力极大的命令。 提供常用命令的快捷方式（shortcut）。 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:4:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"ash 一个简单的轻量级的 Shell，占用资源少，适合运行于低内存环境，但是与下面讲到的 bash shell 完全兼容 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:5:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"Shell分类总结 在大部份的 UNIX 系统，三种著名且广被支持的 shell 是 Bourne shell（AT\u0026T shell，在 Linux 下是 BASH）、C shell（Berkeley shell，在 Linux 下是 TCSH）和 Korn shell（Bourne shell 的超集）。 这三种 shell 在交谈（interactive）模式下的表现相当类似，但作为命令文件语言时，在语法和执行效率上就有些不同了。 ","date":"2020-04-28","objectID":"/ch0_shell%E5%88%86%E7%B1%BB/:6:0","tags":["Shell"],"title":"shell 分类","uri":"/ch0_shell%E5%88%86%E7%B1%BB/"},{"categories":["Shell"],"content":"对于shell脚本而言，有些内容是专门用于处理参数的，它们都有特定的含义，例如 /home/shouwang/test.sh para1 para2 para3 $0 $1 $2 $3 其中$0代表了执行的脚本名，$1，$2分别代表了第一个，第二个参数。除此之外，还有一些其他的默认变量，例如： 符号 释义 $# 代表脚本后面跟的参数个数，前面的例子中有3个参数 $@ 代表了所有参数，并且可以被遍历 $* 代表了所有参数，且作为整体，和$@ 很像，但是有区别 $$ 代表了当前脚本的进程ID $? 代表了上一条命令的退出状态 ","date":"2020-04-27","objectID":"/ch1_%E5%85%A5%E5%8F%82%E5%92%8C%E9%BB%98%E8%AE%A4%E5%8F%98%E9%87%8F/:0:0","tags":["Shell"],"title":"shell 入参和默认变量","uri":"/ch1_%E5%85%A5%E5%8F%82%E5%92%8C%E9%BB%98%E8%AE%A4%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"给变量赋值，使用等号即可，但是等号两边千万不要有空格，等号右边有空格的字符串也必须用引号引起来 ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:0:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"变量定义 para1=\"hello world\" #字符串直接赋给变量para1 #unset用于取消变量 unset para1 定义变量类型： declare 或 typeset -r 只读(readonly一样) -i 整形 -a 数组 -f 函数 -x export declare -i n=0 ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:1:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"系统变量 command 释义 $0 脚本启动名(包括路径) $n 第n个参数,n=1,2,…9 $* 所有参数列表(不包括脚本本身) $@ 所有参数列表(独立字符串) $# 参数个数(不包括脚本本身) $$ 当前程式的PID $! 执行上一个指令的PID $? 执行上一个指令的返回值 ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:2:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"变量引用技巧 command 释义 ${name:+value} 如果设置了name,就把value显示,未设置则为空 ${name:-value} 如果设置了name,就显示它,未设置就显示value ${name:?value} 未设置提示用户错误信息value ${name:=value} 如未设置就把value设置并显示\u003c写入本地中\u003e ${#A} 可得到变量中字节数 ${A:4:9} 取变量中第4位到后面9位 ${A:(-1)} 倒叙取最后一个字符 ${A/www/http} 取变量并且替换每行第一个关键字 ${A//www/http} 取变量并且全部替换每行关键字 定义一个变量：file=/dir1/dir2/dir3/my.file.txt command 释义 ${file#*/} 去掉第一条 / 及其左边的字串：dir1/dir2/dir3/my.file.txt ${file##*/} 去掉最后一条 / 及其左边的字串：my.file.txt ${file#*.} 去掉第一个 . 及其左边的字串：file.txt ${file##*.} 去掉最后一个 . 及其左边的字串：txt ${file%/*} 去掉最后条 / 及其右边的字串：/dir1/dir2/dir3 ${file%%/*} 去掉第一条 / 及其右边的字串：(空值) ${file%.*} 去掉最后一个 . 及其右边的字串：/dir1/dir2/dir3/my.file ${file%%.*} 去掉第一个 . 及其右边的字串：/dir1/dir2/dir3/my 说明 # 是去掉左边(在键盘上 # 在 $ 之左边) % 是去掉右边(在键盘上 % 在 $ 之右边) 单一符号是最小匹配﹔两个符号是最大匹配 ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:3:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"操作变量 echo \"para1 is $para1\" #将会输出 para1 is hello world # or echo \"para1 is ${para1}!\" #将会输出 para1 is hello world! ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:4:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"变量替代 foo=\"I'm a cat.\" echo ${foo/cat/dog} # 打印 \"I'm a dog.\" 使用两个反斜线进行全力替换 foo=\"I'm a cat, and she's cat.\" echo ${foo/cat/dog} # 打印 \"I'm a dog, and she's a cat.\" echo ${foo//cat/dog} # 打印 \"I'm a dog, and she's a dog.\" 打印时不会修改变量 foo=\"hello\" echo ${foo/hello/goodbye} # 打印 \"goodbye\" echo $foo # 仍然打印 \"hello\" ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:4:1","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"变量匹配删除 没有替换的字符串则直接删除 foo=\"I like meatballs.\" echo ${foo/balls} # 打印 I like meat. ${name#pattern}操作删除${name}匹配模式的最短前缀，而##删除最长前缀： minipath=\"/usr/bin:/bin:/sbin\" echo ${minipath#/usr} # 打印 /bin:/bin:/sbin echo ${minipath#*/bin} # 打印 :/bin:/sbin echo ${minipath##*/bin} # 打印 :/sbin ,就是*/bin 匹配的/bin及之前的字符串一并删除 运算符%是相同的，只不过匹配的是后缀而不是前缀： minipath=\"/usr/bin:/bin:/sbin\" echo ${minipath%/usr*} # 打印 nothing echo ${minipath%/bin*} # 打印 /usr/bin: echo ${minipath%%/bin*} # 打印 /usr ,就是%/bin 匹配的/bin及之后的字符串一并删除 分片切割 string=\"I'm a fan of dogs.\" echo ${string:6:3} # 打印 fan ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:4:2","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"是否存在默认值 username=shuan_lu unset username echo ${username-default} # 打印 default username=admin echo ${username-default} # 打印 admin 对于测试是否设置了变量的操作，可以通过添加冒号（\":\"）来强制检查变量是否已设置且是否为空： foo=\"\" bar=\"\" echo ${foo-123} # 打印 nothing echo ${bar:-456} # 打印 456 运算符=（或:=）类似于运算符-，只是如果变量没有值，它也会设置变量： unset cache echo ${cache:=1024} # 打印 1024 echo $cache # 打印 1024 echo ${cache:=2048} # 打印 1024 echo $cache # 打印 1024 ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:5:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"单引号和双引号 shell 会忽略单引号中所有的特殊字符，其中的所有内容都会被当作一个元素 双引号几乎与单引号相似。这里之所以说“几乎”是因为他们也会忽略所有特殊字符，除了： 美元符号：$ 反引号：` 反斜杠：\\ world=Earth foo='Hello, $world!' bar=\"Hello, $world\" echo $foo # 打印 Hello, $world! echo $bar # 打印 Hello, Earth ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:6:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"总结 command 释义 A=“a b c def” 将字符串复制给变量 A=cmd 将命令结果赋给变量 A=$(cmd) 将命令结果赋给变量 eval a=$$a 间接调用 i=2\u0026\u0026echo $((i+3)) 计算后打印新变量结果 i=2\u0026\u0026echo $[i+3] 计算后打印新变量结果 a=$((2\u003e6?5:8)) 判断两个值满足条件的赋值给变量,类似三目运算符 $1 $2 $* 位置参数 *代表所有 env 查看环境变量 env grep “name” set 查看环境变量和本地变量 read name 输入变量 readonly name 把name这个变量设置为只读变量,不允许再次设置 readonly 查看系统存在的只读文件 export name 变量name由本地升为环境 export name=“RedHat” 直接定义name为环境变量 export Stat$nu=2222 变量引用变量赋值 unset name 变量清除 export -n name 去掉只读变量 shift 用于移动位置变量,调整位置变量,使$3的值赋给$2.$2的值赋予$1 name + 0 将字符串转换为数字 number \" \" 将数字转换成字符串 a=‘ee’;b=‘a’;echo ${!b} 间接引用name变量的值 : ${a=“cc”} 如果a有值则不改变,如果a无值则赋值a变量为cc ","date":"2020-04-26","objectID":"/ch2_%E5%8F%98%E9%87%8F/:7:0","tags":["Shell"],"title":"shell 变量","uri":"/ch2_%E5%8F%98%E9%87%8F/"},{"categories":["Shell"],"content":"在shell中执行命令通常只需要像在终端一样执行命令即可，不过，如果想要命令结果打印出来的时候，这样的方式就行不通了。因此，shell的命令方式常有 ","date":"2020-04-26","objectID":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/:0:0","tags":["Shell"],"title":"shell 命令执行","uri":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"方式一 a=`ls` #`是左上角～键，不是单引号 ","date":"2020-04-26","objectID":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/:1:0","tags":["Shell"],"title":"shell 命令执行","uri":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"方式二 或者使用$，后面括号内是执行的命令 echo \"current path is $(pwd)\" # ","date":"2020-04-26","objectID":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/:2:0","tags":["Shell"],"title":"shell 命令执行","uri":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"方式三 计算 另外，前面两种方式对于计算表达式也是行不通的，而要采取下面的方式： echo \"1+1=$((1+1))\" #打印：1+1=2 ","date":"2020-04-26","objectID":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/:3:0","tags":["Shell"],"title":"shell 命令执行","uri":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"方式四 命令赋值给变量 a=\"ls\" echo \"$($a)\" ","date":"2020-04-26","objectID":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/:4:0","tags":["Shell"],"title":"shell 命令执行","uri":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"方式五 如果字符串时多条命令的时候，上面的方式又不可行了，而要采用下面的方式 a=\"ls;pwd\" echo \"$(eval $a)\" ","date":"2020-04-26","objectID":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/:5:0","tags":["Shell"],"title":"shell 命令执行","uri":"/ch3_%E5%91%BD%E4%BB%A4%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"一般说明，如果命令执行成功，则其返回值为0，否则为非0，因此，可以通过下面的方式判断上条命令的执行结果： ","date":"2020-04-25","objectID":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/:0:0","tags":["Shell"],"title":"shell 条件分支","uri":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/"},{"categories":["Shell"],"content":"if 分支 if [ $? -eq 0 ] then echo \"success\" elif [ $? -eq 1 ] then echo \"failed,code is 1\" else echo \"other code\" fi ","date":"2020-04-25","objectID":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/:1:0","tags":["Shell"],"title":"shell 条件分支","uri":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/"},{"categories":["Shell"],"content":"多个条件 方式一 if [ 10 -gt 5 -o 10 -gt 4 ];then echo \"10\u003e5 or 10 \u003e4\" fi 方式二 if [ 10 -gt 5 ] || [ 10 -gt 4 ];then echo \"10\u003e5 or 10 \u003e4\" fi 总结： -o or 或者，同|| -a and 与，同\u0026\u0026 ! 非 ","date":"2020-04-25","objectID":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/:1:1","tags":["Shell"],"title":"shell 条件分支","uri":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/"},{"categories":["Shell"],"content":"case 分支 name=\"aa\" case $name in \"aa\") echo \"name is $name\" ;; \"\") echo \"name is empty\" ;; \"bb\") echo \"name is $name\" ;; *) echo \"other name\" ;; esac ```shell 注意： - []前面要有空格，它里面是逻辑表达式 - if elif后面要跟then，然后才是要执行的语句 - 如果想打印上一条命令的执行结果，最好的做法是将 $?赋给一个变量，因为一旦执行了一条命令，$?的值就可能会变。 - case每个分支最后以两个分号结尾，最后是case反过来写，即esac。 ","date":"2020-04-25","objectID":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/:2:0","tags":["Shell"],"title":"shell 条件分支","uri":"/ch4_%E6%9D%A1%E4%BB%B6%E5%88%86%E6%94%AF/"},{"categories":["Shell"],"content":"Bash Shell 字符串操作详解 Bash 中的字符串操作是脚本编写的核心技能之一，涵盖定义、拼接、截取、替换、匹配等场景。以下是常见用法及示例： 1. 字符串基础 1.1 定义字符串 str1=\"Hello World\" # 双引号（允许变量替换和转义） str2='Hello $USER' # 单引号（原样输出，不解析变量） str3=$'Line1\\nLine2' # ANSI-C 引号（支持转义字符） 1.2 字符串拼接 name=\"Alice\" greet=\"Hello, \"$name\"!\" # 直接拼接 echo $greet # 输出: Hello, Alice! # 使用双引号简化 greet=\"Hello, ${name}!\" # 推荐写法 1.3 获取字符串长度 str=\"abcdefg\" echo ${#str} # 输出: 7 2. 字符串截取 2.1 按位置截取 语法 描述 示例（str=“abcdef”） ${str:start} 从 start 开始截取到末尾 ${str:2} → “cdef” ${str:start:length} 从 start 截取 length 长度 ${str:1:3} → “bcd” 2.2 按模式截取 语法 描述 示例（str=“app_log_20231010.txt”） ${str#pattern} 删除开头匹配的最短模式 ${str#*_} → “log_20231010.txt” ${str##pattern} 删除开头匹配的最长模式 ${str##*_} → “20231010.txt” ${str%pattern} 删除结尾匹配的最短模式 ${str%.*} → “app_log_20231010” ${str%%pattern} 删除结尾匹配的最长模式 ${str%%.*} → “app_log” 3. 字符串替换 3.1 简单替换 语法 描述 示例（str=“hello world hello”） ${str/old/new} 替换第一个匹配项 ${str/hello/Hi} → “Hi world hello” ${str//old/new} 替换所有匹配项 ${str//hello/Hi} → “Hi world Hi” 3.2 前缀/后缀替换 语法 描述 示例（str=“error.log”） ${str/#prefix/new} 替换开头的 prefix ${str/#error/access} → “access.log” ${str/%suffix/new} 替换结尾的 suffix ${str/%.log/.txt} → “error.txt” 4. 字符串查找 4.1 查找子字符串位置 str=\"abcdefg\" index=$(expr index \"$str\" \"cde\") # 查找子串起始位置 echo $index # 输出: 3（位置从1开始计数） 4.2 检查是否包含子串 str=\"hello world\" if [[ $str == *\"world\"* ]]; then echo \"包含 'world'\" fi 5. 字符串分割为数组 csv=\"apple,banana,orange\" IFS=',' read -ra arr \u003c\u003c\u003c \"$csv\" # 按逗号分割 echo \"${arr[1]}\" # 输出: banana 6. 字符串比较 操作符 描述 示例 == 或 = 相等 [[ \"abc\" == \"abc\" ]] → true != 不相等 [[ \"abc\" != \"def\" ]] → true \u003e 或 \u003c 按字典序比较 [[ \"apple\" \u003c \"banana\" ]] → true -z 字符串为空 [[ -z \"\" ]] → true -n 字符串非空 [[ -n \"abc\" ]] → true 7. 正则表达式匹配 7.1 使用 =~ 操作符 email=\"user@example.com\" if [[ $email =~ ^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$ ]]; then echo \"有效邮箱地址\" fi 7.2 提取匹配组 str=\"Date: 2023-10-10\" if [[ $str =~ ([0-9]{4}-[0-9]{2}-[0-9]{2}) ]]; then echo \"日期: ${BASH_REMATCH[1]}\" # 输出: 日期: 2023-10-10 fi 8. 高级技巧 8.1 处理带空格的字符串 file_path=\"/path/with spaces/file.txt\" # 双引号保护空格 cp \"$file_path\" /backup/ # 数组存储带空格字符串 files=(\"file1.txt\" \"file two.txt\") for file in \"${files[@]}\"; do echo \"$file\" done 8.2 使用 heredoc 定义多行字符串 cat \u003c\u003cEOF 第一行 第二行 变量替换: $USER EOF 8.3 颜色和格式输出 RED='\\033[0;31m' NC='\\033[0m' # 重置颜色 echo -e \"${RED}错误信息${NC}\" 9. 实用示例 9.1 获取文件名和扩展名 file=\"image.jpg\" filename=\"${file%.*}\" # image extension=\"${file##*.}\" # jpg 9.2 路径处理 path=\"/var/log/app.log\" dirname=\"${path%/*}\" # /var/log basename=\"${path##*/}\" # app.log ","date":"2020-04-25","objectID":"/ch19_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/:0:1","tags":["Shell"],"title":"shell 字符串详解","uri":"/ch19_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/"},{"categories":["Shell"],"content":"总结 核心操作：截取 (${var:start:len})、替换 (${var/old/new})、模式删除 (${var#pattern}) 关键技巧： 使用 [[ ]] 进行字符串比较和正则匹配 用双引号保护含空格的字符串 灵活使用 IFS 分割字符串为数组 注意： Bash 字符串索引从 0 开始，但 ${str:start} 的 start 从 0 开始 避免在 [ ] 中使用 ==（推荐用 [[ ]]） ","date":"2020-04-25","objectID":"/ch19_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/:0:2","tags":["Shell"],"title":"shell 字符串详解","uri":"/ch19_%E5%AD%97%E7%AC%A6%E4%B8%B2%E6%93%8D%E4%BD%9C%E8%AF%A6%E8%A7%A3/"},{"categories":["Shell"],"content":"for 循环 ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:1:0","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"for 循环一 #遍历输出脚本的参数 for i in $@; do echo $i done ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:1:1","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"for 循环方式二 for ((i = 0 ; i \u003c 10 ; i++)); do echo $i done ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:1:2","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"for 循环方式三 for i in {1..5}; do echo \"Welcome $i\" done ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:1:3","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"for 循环方式四 for i in {5..15..3}; do echo \"number is $i\" done 每隔3打印一次，即打印5,8,11,14 示例 for f in *.c do gcc -o ${f%.c} $f done ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:1:4","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"while 循环 while [ \"$ans\" != \"yes\" ] do read -p \"please input yes to exit loop:\" ans done # 只有当ans不是yes时，循环就终止。 # or num=1 while [ $num -lt 10 ] do echo $num ((num=$num+2)) done ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:2:0","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"until 循环 num=1 # 当command不为0时循环 until [ $num -gt 10 ] do echo until $num ((num=$num+2)) done ","date":"2020-04-24","objectID":"/ch5_%E5%BE%AA%E7%8E%AF/:3:0","tags":["Shell"],"title":"shell 循环","uri":"/ch5_%E5%BE%AA%E7%8E%AF/"},{"categories":["Shell"],"content":"为了完成某一功能的程序指令（语句）的集合，称为函数。Shell 函数的本质是一段可以重复使用的脚本代码，这段代码被提前编写好了，放在了指定的位置，使用时直接调取即可 在程序中，编写函数的主要目的是将一个需要很多行代码的复杂问题分解为一系列简单的任务来解决，而且，同一个任务（函数）可以被多次调用，有助于代码重用。 ","date":"2020-04-23","objectID":"/ch6_%E5%87%BD%E6%95%B0/:0:0","tags":["Shell"],"title":"shell 函数","uri":"/ch6_%E5%87%BD%E6%95%B0/"},{"categories":["Shell"],"content":"定义函数 function myfunc() { echo \"hello world $1\" } function myfunc { echo \"hello world $1\" } 或者 myfunc() { echo \"hello world $1\" } 说明 可以省略 function 关键词 如果写了 function 关键字，也可以省略函数名后面的小括号 ","date":"2020-04-23","objectID":"/ch6_%E5%87%BD%E6%95%B0/:1:0","tags":["Shell"],"title":"shell 函数","uri":"/ch6_%E5%87%BD%E6%95%B0/"},{"categories":["Shell"],"content":"函数参数及调用 在 Shell 中，我们定义 函数 时，不像 C 语言、 C++、 Python、 Java 和 Golang 那样，需要传递参数，Shell 中的函数在定义时不能指明参数，但是在调用时却可以传递参数。 函数参数是 Shell 位置参数的一种，在函数内部可以使用 $n 来接收，例如，$1 表示第一个参数，$2 表示第二个参数，依次类推。除了 n，还有另外三个比较重要的变量，# 可以获取传递的参数的个数，$@ 或者 $* 可以一次性获取所有的参数。 #!/bin/bash function show(){ echo \"Name is: $1\" echo \"Site is: $2\" echo \"Age is: $3\" } show shuanlu https://lu_shuan.gitee.io/gitbook 109 ","date":"2020-04-23","objectID":"/ch6_%E5%87%BD%E6%95%B0/:2:0","tags":["Shell"],"title":"shell 函数","uri":"/ch6_%E5%87%BD%E6%95%B0/"},{"categories":["Shell"],"content":"通常函数的return返回值只支持0-255，因此想要获得返回值，可以通过下面的方式。 function myfunc() { local myresult='some value' echo $myresult } val=$(myfunc) #val的值为some value 通过return的方式适用于判断函数的执行是否成功： function myfunc() { #do something return 0 } if myfunc;then echo \"success\" else echo \"failed\" fi ","date":"2020-04-22","objectID":"/ch7_%E8%BF%94%E5%9B%9E%E5%80%BC/:0:0","tags":["Shell"],"title":"shell 返回值","uri":"/ch7_%E8%BF%94%E5%9B%9E%E5%80%BC/"},{"categories":["Shell"],"content":"shell通过#来注释一行内容，前面我们已经看到过了 #!/bin/bash # 这是一行注释 :' 这是 多行 注释 ' ls :\u003c\u003cEOF 这也可以 达到 多行注释 的目的 EOF ","date":"2020-04-21","objectID":"/ch8_%E6%B3%A8%E9%87%8A/:0:0","tags":["Shell"],"title":"shell 注释","uri":"/ch8_%E6%B3%A8%E9%87%8A/"},{"categories":["Shell"],"content":"脚本执行后免不了要记录日志，最常用的方法就是重定向。以下面的脚本为例： #!/bin/bash #test.sh lll #这个命令是没有的，因此会报错 date ","date":"2020-04-20","objectID":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/:0:0","tags":["Shell"],"title":"shell 日志保存","uri":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/"},{"categories":["Shell"],"content":"方式一 将标准输出保存到文件中，打印标准错误： ./test.sh \u003e log.dat 这种情况下，如果命令执行出错，错误将会打印到控制台。所以如果你在程序中调用，这样将不会讲错误信息保存在日志中。 ","date":"2020-04-20","objectID":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/:1:0","tags":["Shell"],"title":"shell 日志保存","uri":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/"},{"categories":["Shell"],"content":"方式二 ./test.sh \u003e log.dat 2\u003e\u00261 标准输出和标准错误都保存到日志文件中 这里的2\u003e\u00261是什么意思？ 表明将./test.sh的输出重定向到log.txt文件中，同时将标准错误也重定向到log.txt文件中。 ","date":"2020-04-20","objectID":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/:2:0","tags":["Shell"],"title":"shell 日志保存","uri":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/"},{"categories":["Shell"],"content":"方式三 保存日志文件的同时，也输出到控制台 ./test.sh |tee log.dat ","date":"2020-04-20","objectID":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/:3:0","tags":["Shell"],"title":"shell 日志保存","uri":"/ch9_%E6%97%A5%E5%BF%97%E4%BF%9D%E5%AD%98/"},{"categories":["Shell"],"content":"常见执行方式 ./test.sh ","date":"2020-04-19","objectID":"/ch10_%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C/:1:0","tags":["Shell"],"title":"shell 脚本执行","uri":"/ch10_%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"其它执行方式 sh test.sh #在子进程中执行 sh -x test.sh #会在终端打印执行到命令，适合调试 source test.sh #test.sh在父进程中执行 . test.sh #不需要赋予执行权限，临时执行 ","date":"2020-04-19","objectID":"/ch10_%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C/:2:0","tags":["Shell"],"title":"shell 脚本执行","uri":"/ch10_%E8%84%9A%E6%9C%AC%E6%89%A7%E8%A1%8C/"},{"categories":["Shell"],"content":"很多时候我们需要获取脚本的执行结果，即退出状态，通常0表示执行成功，而非0表示失败。为了获得退出码，我们需要使用exit #!/bin/bash function myfun() { if [ $# -lt 2 ] then echo \"para num error\" exit 1 fi echo \"ok\" exit 2 } if [ $# -lt 1 ] then echo \"para num error\" exit 1 fi returnVal=`myfun aa` echo \"end shell\" exit 0 ","date":"2020-04-18","objectID":"/ch11_%E8%84%9A%E6%9C%AC%E9%80%80%E5%87%BA%E7%A0%81/:0:0","tags":["Shell"],"title":"shell 脚本退出码","uri":"/ch11_%E8%84%9A%E6%9C%AC%E9%80%80%E5%87%BA%E7%A0%81/"},{"categories":["Ansible"],"content":"问题描述 通过ansible命令直接ping多台机器的网络状态，提示报错 失败\r172.16.24.220 | UNREACHABLE! =\u003e { “changed”: false, “msg”: “Failed to connect to the host via ssh: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password).\\r\\n”, “unreachable”: true }\r","date":"2020-04-17","objectID":"/ansible%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:1:0","tags":["Ansible"],"title":"Ansible 权限认证报错问题记录","uri":"/ansible%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Ansible"],"content":"问题处理 解决方式：单向的ssh验证 ssh-keygen一路回车,主要是用来免密通信的 ssh-copy-id 172.16.24.220 需要输入对应主节的root密码 $ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: 7f:69:87:cf:28:fe:8b:19:55:a7:d0:c9:aa:6d:05:0c root@chm The key's randomart image is: +--[ RSA 2048]----+ | E | | o o . | | + = .| | = o | | S o o | | . + + | | + B . | | .B = | | .+o+.o | +-----------------+ [root@chm log]# [root@chm log]# ssh-copy-id 172.16.24.220 /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key(s) remain to be installed -- if you are prompted now it is to install the new keys root@172.16.24.220's password: Number of key(s) added: 1 Now try logging into the machine, with: \"ssh '172.16.24.220'\" and check to make sure that only the key(s) you wanted were added. ","date":"2020-04-17","objectID":"/ansible%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:2:0","tags":["Ansible"],"title":"Ansible 权限认证报错问题记录","uri":"/ansible%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Ansible"],"content":"验证 再次验证，成功 [root@chm log]# ansible 172.16.24.220 -m ping 172.16.24.220 | SUCCESS =\u003e { \"changed\": false, \"failed\": false, \"ping\": \"pong\" } ","date":"2020-04-17","objectID":"/ansible%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/:3:0","tags":["Ansible"],"title":"Ansible 权限认证报错问题记录","uri":"/ansible%E6%9D%83%E9%99%90%E8%AE%A4%E8%AF%81%E6%8A%A5%E9%94%99%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95/"},{"categories":["Shell"],"content":"要给某个环境变量设置多个值，可以把值放在括号里，值与值之间用空格分隔。 ","date":"2020-04-17","objectID":"/ch12_%E6%95%B0%E7%BB%84/:0:0","tags":["Shell"],"title":"shell 数组","uri":"/ch12_%E6%95%B0%E7%BB%84/"},{"categories":["Shell"],"content":"声明 mytest=(one two three four five) echo ${mytest[2]} # 输出 three echo ${mytest[*]} # 输出 one two three four five ","date":"2020-04-17","objectID":"/ch12_%E6%95%B0%E7%BB%84/:1:0","tags":["Shell"],"title":"shell 数组","uri":"/ch12_%E6%95%B0%E7%BB%84/"},{"categories":["Shell"],"content":"修改数组值 mytest=(one two three four five) mytest[2]=seven ","date":"2020-04-17","objectID":"/ch12_%E6%95%B0%E7%BB%84/:2:0","tags":["Shell"],"title":"shell 数组","uri":"/ch12_%E6%95%B0%E7%BB%84/"},{"categories":["Shell"],"content":"删除数组中的值 mytest=(one two three four five) unset mytest[2] # 删除单个元素，索引下标保留映射的值被置空 echo ${mytest[2]} # 输出为空 unset mytest # 删除整个数组 ","date":"2020-04-17","objectID":"/ch12_%E6%95%B0%E7%BB%84/:3:0","tags":["Shell"],"title":"shell 数组","uri":"/ch12_%E6%95%B0%E7%BB%84/"},{"categories":["Shell"],"content":"示例 docker 中的应用启动后再进行健康检查是一个启动脚本 #!/bin/bash APP=(\"java\" \"health_check\") java=( \"java --add-opens java.base/java.lang=ALL-UNNAMED -DAPP_NAME=zcmMonitor -XX:MaxRAMPercentage=70.0 -Dspring.config.location=/app/conf/monitorConfig.properties -Dlogging.config=/app/conf/logback.xml -cp /app/lib/zcm-monitor-9.1.3-sec-SNAPSHOT.jar:/app/lib/run/* com.lushuan.zsmart.zcm.monitor.App\" # start command \"APP_NAME=zcmMonitor\" # process key \"echo OK\" # liveness check command \"OK\" # liveness successful code \"curl -k -m 10 -s https://127.0.0.1:${NMS_MONITOR_SSL_PORT}/app/health 2\u003e/dev/null\" # readiness check command \"UP\" # readiness successful code ) health_check=( \"sh /app/health_check.sh\" # start command \"health_check.sh\" # process key \"echo OK\" # liveness check command \"OK\" # liveness successful code \"echo OK\" # readiness check command \"OK\" # readiness successful code ) 然后解析该数组配置设置启动顺序 ","date":"2020-04-17","objectID":"/ch12_%E6%95%B0%E7%BB%84/:4:0","tags":["Shell"],"title":"shell 数组","uri":"/ch12_%E6%95%B0%E7%BB%84/"},{"categories":["Shell"],"content":"总结 有时数组变量会让事情很麻烦，所以在shell脚本编程时并不常用。对其他shell而言，数组变 量的可移植性并不好，如果需要在不同的shell环境下从事大量的脚本编写工作，这会带来很多不 便 command 释义 A=(a b c def) 将变量定义为数組 ${#A[*]} 数组个数 ${A[*]} 数组所有元素,大字符串 ${A[@]} 数组所有元素,类似列表可迭代 ${A[2]} 脚本的一个参数或数组第三位 ","date":"2020-04-17","objectID":"/ch12_%E6%95%B0%E7%BB%84/:5:0","tags":["Shell"],"title":"shell 数组","uri":"/ch12_%E6%95%B0%E7%BB%84/"},{"categories":["Shell"],"content":"在shell编程中，字典是一种非常有用的数据结构，用于存储键值对。字典可以用于存储和检索数据，它类似于其他编程语言中的关联数组或哈希表。 在shell中，可以使用关联数组来实现字典。关联数组是一种有序的集合，它将唯一的键映射到值。键可以是字符串或整数，而值可以是任意类型的数据。 字典的用法如下： 创建字典：可以使用关联数组的方式来创建字典，例如： declare -A my_dict my_dict[key1]=value1 my_dict[key2]=value2 访问字典值：可以通过键来访问字典中的值，例如： echo ${my_dict[key1]} # 输出：value1 遍历字典：可以使用循环来遍历字典中的所有键和值，例如： for key in \"${!my_dict[@]}\"; do echo \"Key: $key, Value: ${my_dict[$key]}\" done 修改字典值：可以通过键来修改字典中的值，例如： my_dict[key1]=new_value1 删除键值对：可以使用unset命令来删除字典中的键值对，例如： unset my_dict[key1] 字典长度：可以使用#运算符来获取字典的长度，即键值对的数量，例如： echo ${#my_dict[@]} # 输出字典的长度 字典在shell编程中非常有用，可以用于各种场景，如配置文件解析、数据存储和传递等。它提供了一种便捷的方式来处理和组织数据，提高了编程效率。 ","date":"2020-04-16","objectID":"/ch13_%E5%AD%97%E5%85%B8/:0:0","tags":["Shell"],"title":"shell 字典","uri":"/ch13_%E5%AD%97%E5%85%B8/"},{"categories":["Shell"],"content":"介绍 test 是 Shell 内置命令，用来检测某个条件是否成立。test 通常和 if 语句一起使用，并且大部分 if 语句都依赖 test。 test 命令有很多选项，可以进行数值、字符串和文件三个方面的检测。 Shell test 命令的用法为： test expression 当 test 判断 expression 成立时，退出状态为 0，否则为非 0 值。 test 命令也可以简写为[]，它的用法为： [ expression ] 注意[]和expression之间的空格，这两个空格是必须的，否则会导致语法错误。[]的写法更加简洁，比 test 使用频率高。 test 和 [] 是等价的，后续我们会交替使用 test 和 []，以让读者尽快熟悉。 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:1:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"字符串操作 command 释义 -z “$str1” str1是否为空字符串 -n “$str1” str1是否不是空字符串 “$str1” == “$str2” str1是否与str2相等 “$str1” != “$str2” str1是否与str2不等 “$str1” =~ “str2” str1是否包含str2 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:2:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"整数操作 command 释义 -eq 两数是否相等 -ne 两数是否不等 -gt 前者是否大于后者（greater then） -lt 前面是否小于后者（less than） -ge 前者是否大于等于后者（greater then or equal） -le 前者是否小于等于后者（less than or equal） ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:3:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"文件操作 command 释义 -a 并且，两条件为真 -b 是否块文件 -p 文件是否为一个命名管道 -c 是否字符文件 -r 文件是否可读 -d 是否一个目录 -s 文件的长度是否不为零 -e 文件是否存在 -S 是否为套接字文件 -f 是否普通文件 -x 文件是否可执行，则为真 -g 是否设置了文件的 SGID 位 -u 是否设置了文件的 SUID 位 -G 文件是否存在且归该组所有 -w 文件是否可写，则为真 -k 文件是否设置了的粘贴位 -o 或，一个条件为真 -O 文件是否存在且归该用户所有 ! 取反 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:4:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"示例 command 释义 test 10 -lt 5 判断大小 echo $? 查看上句test命令返回状态 # 结果0为真,1为假 test -n “hello” 判断字符串长度是否为0 [ $? -eq 0 ] \u0026\u0026 echo “success” || exit　# 判断成功提示,失败则退出 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:5:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"判断 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:6:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"整数判断 -eq 两数是否相等 -ne 两数是否不等 -gt 前者是否大于后者（greater then） -lt 前面是否小于后者（less than） -ge 前者是否大于等于后者（greater then or equal） -le 前者是否小于等于后者（less than or equal） ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:6:1","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"字符串判断str1 exp str2： -z “$str1” str1是否为空字符串 -n “$str1” str1是否不是空字符串 “$str1” == “$str2” str1是否与str2相等 “$str1” != “$str2” str1是否与str2不等 “$str1” =~ “str2” str1是否包含str2 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:6:2","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"文件目录判断 -f $filename 是否为文件 -e $filename 是否存在 -d $filename 是否为目录 -s $filename 文件存在且不为空 ! -s $filename 文件是否为空 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:6:3","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"流程控制 break N # 跳出几层循环 continue N # 跳出几层循环，循环次数不变 continue # 重新循环次数不变 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:7:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"示例 #!/bin/bash read age if test $age -le 2; then echo \"婴儿\" elif test $age -ge 3 \u0026\u0026 test $age -le 8; then echo \"幼儿\" elif [ $age -ge 9 ] \u0026\u0026 [ $age -le 17 ]; then echo \"少年\" elif [ $age -ge 18 ] \u0026\u0026 [ $age -le 25 ]; then echo \"成年\" elif test $age -ge 26 \u0026\u0026 test $age -le 40; then echo \"青年\" elif test $age -ge 41 \u0026\u0026 [ $age -le 60 ]; then echo \"中年\" else echo \"老年\" fi ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:8:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"总结 test 命令有点特别==,=~ 只能用来比较字符串，不能用来比较数字，比较数字需要使用 -eq、-gt 等选项；不管是比较字符串还是数字，test 都不支持 \u003e= 和 \u003c=。有经验的程序员需要慢慢习惯 test 命令的这些奇葩用法 ","date":"2020-04-15","objectID":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/:9:0","tags":["Shell"],"title":"shell test 条件判断","uri":"/ch14_test%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"},{"categories":["Shell"],"content":"Shell 参数处理是指在 Shell 脚本中，对命令行传入的参数进行处理和操作的技术。这在编写 Shell 脚本时非常重要，因为它允许脚本根据不同的参数执行不同的操作，增强了脚本的灵活性和适用性。 在 Shell 脚本中，可以使用特殊变量 “$1”、\"$2\"、\"$3\" 等来引用命令行传入的参数，其中 “$1” 表示第一个参数，\"$2\" 表示第二个参数，依此类推。这些参数可以根据脚本需要进行操作，比如作为文件名、目录名、网址等。 以下是一些示例，演示了如何在 Shell 脚本中处理参数： ","date":"2020-04-14","objectID":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/:0:0","tags":["Shell"],"title":"shell 参数处理","uri":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/"},{"categories":["Shell"],"content":"显示脚本名称和参数： #!/bin/bash echo \"脚本名称：$0\" echo \"第一个参数：$1\" echo \"第二个参数：$2\" 运行 ./script.sh argument1 argument2，将会输出： 脚本名称：./script.sh 第一个参数：argument1 第二个参数：argument2 ","date":"2020-04-14","objectID":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/:1:0","tags":["Shell"],"title":"shell 参数处理","uri":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/"},{"categories":["Shell"],"content":"判断参数个数是否正确： #!/bin/bash if [ $# -ne 2 ]; then echo \"需要两个参数\" exit 1 fi echo \"参数个数正确\" 运行 ./script.sh argument1，将会输出：需要两个参数 ","date":"2020-04-14","objectID":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/:2:0","tags":["Shell"],"title":"shell 参数处理","uri":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/"},{"categories":["Shell"],"content":"使用参数作为文件名： #!/bin/bash filename=\"$1\" if [ -f \"$filename\" ]; then echo \"文件存在\" else echo \"文件不存在\" fi 运行 ./script.sh myfile.txt，将会输出：文件存在 ","date":"2020-04-14","objectID":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/:3:0","tags":["Shell"],"title":"shell 参数处理","uri":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/"},{"categories":["Shell"],"content":"循环处理多个参数： #!/bin/bash for arg in \"$@\" do echo \"参数: $arg\" done 运行 ./script.sh argument1 argument2 argument3，将会输出： 参数: argument1 参数: argument2 参数: argument3 ","date":"2020-04-14","objectID":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/:4:0","tags":["Shell"],"title":"shell 参数处理","uri":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/"},{"categories":["Shell"],"content":"特别的参数 注意参数超过个位数时的处理 echo $0 # 打印脚本名称 echo $1 # 打印第一个参数 echo $2 # 打印第二个参数 echo $9 # 打印第九个参数 echo $10 # 打印第一个参数后面跟一个0 echo ${10} # 打印第十个参数 echo $# # 打印参数的数量 这些示例展示了 Shell 参数处理的一些基本用法，可以根据实际需求灵活运用。 ","date":"2020-04-14","objectID":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/:5:0","tags":["Shell"],"title":"shell 参数处理","uri":"/ch15_%E5%8F%82%E6%95%B0%E5%A4%84%E7%90%86/"},{"categories":["Shell"],"content":"方式一 命令expr 命令expr打印算术表达式的结果，但必须小心 * 号 expr 3 + 12 # 打印 15 expr 3 * 12 # (probably) crashes: * 需要进行转义，这行语句会执行报错 expr 3 \\* 12 # 打印 36 ","date":"2020-04-13","objectID":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/:1:0","tags":["Shell"],"title":"shell 算术表达式","uri":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/"},{"categories":["Shell"],"content":"方式二 (( assignable = expression )) (( x = 3 + 12 )); echo $x # 打印 15 (( x = 3 * 12 )); echo $x # 打印 36 ","date":"2020-04-13","objectID":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/:2:0","tags":["Shell"],"title":"shell 算术表达式","uri":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/"},{"categories":["Shell"],"content":"方式三 echo $(( 3 + 12 )) # 打印 15 echo $(( 3 * 12 )) # 打印 36 ","date":"2020-04-13","objectID":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/:3:0","tags":["Shell"],"title":"shell 算术表达式","uri":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/"},{"categories":["Shell"],"content":"指定类型 虽然隐式声明变量是bash中的规范，但可以显式声明变量并为其指定类型 指定整型 declare -i number number=2+4*10 echo $number # 打印 42 another=2+4*10 echo $another # 打印 2+4*10 number=\"foobar\" echo $number # 打印 0 ","date":"2020-04-13","objectID":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/:4:0","tags":["Shell"],"title":"shell 算术表达式","uri":"/ch16_%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%92%8C%E7%AE%97%E6%9C%AF/"},{"categories":["Shell"],"content":"Unix中的每个进程默认都可以访问三个输入/输出通道：STDIN（标准输入）、STDOUT（标准输出）和STDERR（标准错误）。 ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:0:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"打印myfile中包含单词foo的行 grep foo \u003c myfile ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:1:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"文件合并 一般配合文件切割使用 将文件file1、file2 合并至新文件combined cat file1 file2 \u003e combined ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:2:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"文件追加 date \u003e\u003e log ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:3:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"endmarker 要在脚本中从字面上指定STDIN的内容，请使用«endmarker表示法： cat \u003e file1 \u003c\u003cUNTILHERE All of this will be printed out. Since all of this is going into cat on STDIN. UNTILHERE # \u003e\u003e 追加 cat \u003e\u003e file1 \u003c\u003cUNTILHERE All of this will be printed out. Since all of this is going into cat on STDIN. UNTILHERE # 一般使用EOF cat \u003e\u003e file1 \u003c\u003cEOF All of this will be printed out. Since all of this is going into cat on STDIN. EOF ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:4:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"输出错误日志至文件 httpd 2\u003e error.log ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:5:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"chanel 输出类型 STDIN is channel 0, STDOUT is channel 1, while STDERR is channel 2. grep foo nofile 2\u003e\u00261 # errors will appear on STDOUT，既正常输出也输出错误日志 ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:6:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"总结 重定向 command 释义 cmd 1\u003e fiel 把 标准输出 重定向到 file 文件中 cmd \u003e file 2\u003e\u00261 把 标准输出 和 标准错误 一起重定向到 file 文件中 cmd 2\u003e file 把 标准错误 重定向到 file 文件中 cmd 2» file 把 标准错误 重定向到 file 文件中(追加) cmd » file 2\u003e\u00261 把 标准输出 和 标准错误 一起重定向到 file 文件中(追加) cmd \u003c file \u003efile2 cmd 命令以 file 文件作为 stdin(标准输入)，以 file2 文件作为 标准输出 cat \u003c\u003efile 以读写的方式打开 file ","date":"2020-04-12","objectID":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/:7:0","tags":["Shell"],"title":"shell 文件和重定向","uri":"/ch17_%E6%96%87%E4%BB%B6%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91/"},{"categories":["Shell"],"content":"一些生产中常用到的片段脚本 ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:0:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Shell"],"content":"字符串分割模板 IFS 默认是空格进行分割 oldIFS=$IFS IFS=\":\" data=\"大牛:二狗:三驴\" for item in $data; do echo $item done IFS=$oldIFS ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:1:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Shell"],"content":"进程探测 #!/bin/bash PROC_NAME=nms-prometheus ProcNumber=`ps -ef |grep -w $PROC_NAME|grep -v grep|wc -l` if [ $ProcNumber -le 0 ];then echo \"nmsprometheus is not run\" sh /home/zoms/monitor/start.sh else echo \"nmsprometheus is running..\" fi ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:2:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Shell"],"content":"目录循环 #!/bin/sh dirlist=\"ls\" for dir in `$dirlist` do if [ -d $dir ]; then helm install $dir $dir -n zcm9 -f ../values.yaml fi done ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:3:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Shell"],"content":"进程检测 判断进程是否存在，如果不存在则进行手动重启，如果重启还不存在则做另外的动作 #!/bin/sh nginxpid=$(ps -C nginx --no-header|wc -l) #1.判断Nginx是否存活,如果不存活则尝试启动Nginx if [ $nginxpid -eq 0 ];then systemctl start nginx sleep 3 #2.等待3秒后再次获取一次Nginx状态 nginxpid=$(ps -C nginx --no-header|wc -l) #3.再次进行判断, 如Nginx还不存活则停止Keepalived,让地址进行漂移,并退出脚本 if [ $nginxpid -eq 0 ];then systemctl stop keepalived fi fi ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:4:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Shell"],"content":"定时清空文件内容 结合cron 使用 #!/bin/bash logfile=/tmp/`date +%H-%F`.log n=`date +%H` if [ $n -eq 00 ] || [ $n -eq 12 ] then #通过for循环，以find命令作为遍历条件，将目标目录下的所有文件进行遍历并做相应操作 for i in `find /data/log/ -type f` do true \u003e $i done else for i in `find /data/log/ -type f` do du -sh $i \u003e\u003e $logfile done fi ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:5:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":["Shell"],"content":"探测主机端口是否处于监听状态 #!/bin/bash ip_list=\"10.1.1.81 10.1.1.83 10.1.11.159\" port=9100 function main() { for ip in $ip_list do result=`echo -e \"\\n\" |timeout --signal=9 2 telnet $ip $port 2\u003e/dev/null | grep Connected | wc -l` if [ $result -ne 1 ] then echo $ip \u003e\u003e $HOME/ip_refused.txt fi done } main ","date":"2020-04-11","objectID":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/:6:0","tags":["Shell"],"title":"shell 常用脚本","uri":"/ch18_%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC/"},{"categories":[""],"content":"前言 生活可能不像你想象的那么好\r我们在人的一生中最为辉煌的一天，并不是功成名就的那一天，而是从悲叹和绝望中产生对人生挑战的欲望，并且勇敢的迈向这种挑战的那一天。 人生当中成功只是一时的，失败却是主旋律。但是如何面对失败却把人分成了不同的样子。有的人会被失败击垮，有的人能够不断的爬起来，继续向前。 我想真正的成熟，应该并不是追求完美，而是直面自己的缺憾，这才是生活的本质。 也许他们会明白莫泊桑的一句话：生活可能不像你想象的那么好，但是也不会像你想象的那么糟。人的脆弱和坚强都超乎了自己的想象。有时候，可能脆弱的一句话就泪流满面，有时候，你发现自己咬着牙已经走过了很长的路，\r人的一生中最大的问题不是找不到正确的方法而是在实践中长久的悖逆人性 最淡的墨水也胜过最强的记忆\rAbout Persistence(最淡的墨水也胜过最强的记忆) Nothing in the world can take the place of Persistence. Talent will not; nothing is more common than unsuccessful men with talent. Genius will not; unrewarded genius is almost a proverb. Education will not; the world is full of educated derelicts. Persistence and Determination alone are omnipotent. The slogan “Press On” has solved and will always solve the problems of the human race. ","date":"2019-09-15","objectID":"/about/:1:0","tags":[""],"title":"About","uri":"/about/"},{"categories":[""],"content":"个人介绍 简要介绍：目前居住南京，喜欢云原生并通过了CKA,CKS认证,对监控落地有相关经验， 熟练使用Docker、K8s，有生产k8s集群的构建、运维、优化、排错经验，熟悉各种开源中间件的部署和优化，有运维自动化、监控系统 相关开发经验。 个人信息：lushuan 拿手菜：清蒸鲈鱼、徐州地锅鸡、炒花蛤、香辣小炒肉 兴趣爱好：篮球、音乐、阅读、掼蛋、台球、旅游等 邮箱：lushuan2071@126.com 我的微信 技能说明：我会的东西基本上都写在博客中了，不敢说有多么精通，一直在努力学习中 Kubernetes Prometheus InfluxDB Grafana Ansible Docker Helm Linux Shell MySQL ","date":"2019-09-15","objectID":"/about/:2:0","tags":[""],"title":"About","uri":"/about/"},{"categories":[""],"content":"云计算相关认证 CKA CKS ","date":"2019-09-15","objectID":"/about/:3:0","tags":[""],"title":"About","uri":"/about/"},{"categories":["linux"],"content":"Linux服务器运行久时，系统时间就会存在一定的误差，一般情况下可以使用date命令进行时间设置，但在做数据库集群分片等操作时对多台机器的时间差是有要求的，此时就需要使用ntpdate进行时间同步 ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:0:0","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":["linux"],"content":"手动修改 手动设置时间，需要使用 sudo 权限用户 # 当前时区具体的时间 date -s \"2022-12-31 23:59:59\" # 写入修改后的时间到硬件时钟（RTC）中，将确保设置的时间在下次启动时保持不变 hwclock --systohc 手动修改时间会有误差，建议使用下面的时间服务器进行同步更新 ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:1:0","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":["linux"],"content":"通过时间服务器校正 sudo yum install ntpdate -y ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:2:0","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":["linux"],"content":"同步时间 sudo ntpdate -b time1.aliyun.com ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:2:1","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":["linux"],"content":"配置启动ntpd $ vi /etc/ntp.conf ... # For more information about this file, see the man pages # ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5). driftfile /var/lib/ntp/drift # Permit time synchronization with our time source, but do not # permit the source to query or modify the service on this system. restrict default nomodify notrap nopeer noquery # Permit all access over the loopback interface. This could # be tightened as well, but to do so would effect some of # the administrative functions. restrict 127.0.0.1 restrict ::1 # Hosts on local network are less restricted. #restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap # Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). server time1.aliyun.com #broadcast 192.168.1.255 autokey # broadcast server #broadcastclient # broadcast client #broadcast 224.0.1.1 autokey # multicast server #multicastclient 224.0.1.1 # multicast client #manycastserver 239.255.254.254 # manycast server #manycastclient 239.255.254.254 autokey # manycast client # Enable public key cryptography. #crypto includefile /etc/ntp/crypto/pw # Key file containing the keys and key identifiers used when operating # with symmetric key cryptography. keys /etc/ntp/keys # Specify the key identifiers which are trusted. #trustedkey 4 8 42 # Specify the key identifier to use with the ntpdc utility. #requestkey 8 # Specify the key identifier to use with the ntpq utility. #controlkey 8 # Enable writing of statistics records. #statistics clockstats cryptostats loopstats peerstats # Disable the monitoring facility to prevent amplification attacks using ntpdc # monlist command when default restrict does not include the noquery flag. See # CVE-2013-5211 for more details. # Note: Monitoring will not be disabled with the limited restriction flag. ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:2:2","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":["linux"],"content":"设置开机自启 sudo systemctl status ntpd; sudo systemctl start ntpd; sudo systemctl enable ntpd; ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:2:3","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":["linux"],"content":"ntp 常用服务器 国内 cn.pool.ntp.org 中国开源免费NTP服务器 ntp1.aliyun.com 阿里云NTP服务器 ntp2.aliyun.com 阿里云NTP服务器 time1.aliyun.com 阿里云NTP服务器 time2.aliyun.com 阿里云NTP服务器 国外 time1.apple.com 苹果NTP服务器 time2.apple.com 苹果NTP服务器 time3.apple.com 苹果NTP服务器 time4.apple.com 苹果NTP服务器 time5.apple.com 苹果NTP服务器 time1.google.com 谷歌NTP服务器 time2.google.com 谷歌NTP服务器 time3.google.com 谷歌NTP服务器 time4.google.com 谷歌NTP服务器 pool.ntp.org 开源免费NTP服务器 ","date":"2019-06-29","objectID":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/:2:4","tags":["linux"],"title":"Linux 时间校正","uri":"/linux%E6%97%B6%E9%97%B4%E6%A0%A1%E6%AD%A3/"},{"categories":null,"content":"友链 Hugo LoveIt 官方文档 ","date":"0001-01-01","objectID":"/friends/:1:0","tags":null,"title":"友链墙","uri":"/friends/"},{"categories":null,"content":"资料 云原生资料库 Istio中文文档 ","date":"0001-01-01","objectID":"/friends/:2:0","tags":null,"title":"友链墙","uri":"/friends/"}]